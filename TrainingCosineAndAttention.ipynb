{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "def4f85a-37b5-472e-8782-a5df13c5d601",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec1af4-1d61-46ef-90b9-b972129d09a5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eeb867f-c38e-4e5c-ab89-a7e747ead472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "017d87cd-410b-44ef-ac34-560022867208",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "#torch.backends.cudnn.enabled = False\n",
    "val_size = 5000\n",
    "test_size = 5000\n",
    "batch_size = 16\n",
    "num_workers = 4\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Downloading MNIST again :) Training (60k) and test(5k) + val(5k) split\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('./mnist_data',\n",
    "                                            download=True,\n",
    "                                            train=True,\n",
    "                                            transform=transform),\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=num_workers)\n",
    "\n",
    "test_dataset = datasets.MNIST('./mnist_data',\n",
    "                               download=True,\n",
    "                               train=False,\n",
    "                               transform=transform)\n",
    "\n",
    "val_dataset, test_dataset = random_split(test_dataset, [val_size, test_size])\n",
    "\n",
    "# Test set to compare with DDPM paper\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# Validation set so we can keep track of approximated FID score while training\n",
    "validation_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                            batch_size=16,\n",
    "                                            shuffle=False, num_workers=num_workers)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5630601-e376-4f9d-a13b-cefb9d3dcea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(t, s=torch.tensor([0.008]), T=torch.tensor([1000])):\n",
    "    return min(torch.cos((t / T + s) / (1 + s) * (torch.pi / 2)).pow(2), 0.99999)\n",
    "\n",
    "T = 1000\n",
    "ts = torch.arange(T)\n",
    "alpha_bar = torch.tensor([min(f(t)/f(torch.tensor([0])),0.999) for t in ts]) \n",
    "beta = torch.tensor([1 - alpha_bar[t]/(alpha_bar[t-1]) if t > 0 else torch.tensor([0]) for t in ts])\n",
    "alpha = 1 - beta\n",
    "alpha = alpha.view((1000, 1, 1, 1)).to(device)\n",
    "beta = beta.view((1000, 1, 1, 1)).to(device)\n",
    "alpha_bar = alpha_bar.view((1000, 1, 1, 1)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0ae1fca-cff3-4b53-86d1-65c59740a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up alpha_bar for training and test so alpha_bar_t = alpha_bar[t]\n",
    "T = 1000\n",
    "beta_start, beta_end = [1e-4, 2e-02]\n",
    "beta = torch.linspace(beta_start, beta_end, T)\n",
    "alpha = 1-beta\n",
    "alpha_bar = alpha.clone()\n",
    "for e in range(T-1):\n",
    "    alpha_bar[e+1] *= alpha_bar[e]\n",
    "\n",
    "alpha = alpha.view((1000, 1, 1, 1)).to(device)\n",
    "beta = beta.view((1000, 1, 1, 1)).to(device)\n",
    "alpha_bar = alpha_bar.view((1000, 1, 1, 1)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42fe879-ee13-4434-9a0d-de015ddd4c2f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c671dbab-ffe9-4fe5-841a-80042e7593b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNET, self).__init__()\n",
    "        channels = [32, 64, 128, 256]\n",
    "        self.relu = nn.ReLU()\n",
    "        self.convs_attention = [nn.Conv2d(i, 1, 1) for i in [128, 64, 32]]\n",
    "        self.convs_attention = nn.ModuleList(self.convs_attention)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(2, channels[0], kernel_size=3, padding=1),  # (batchsize, 32, 28, 28)\n",
    "                nn.ReLU(),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2),  # (batchsize, 32, 14, 14)\n",
    "                nn.Conv2d(channels[0], channels[1], kernel_size=3, padding=1),  # (batchsize, 64, 14, 14)\n",
    "                nn.ReLU(),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2),  # (batchsize, 64, 7, 7)\n",
    "                nn.Conv2d(channels[1], channels[2], kernel_size=3, padding=1),  # (batchsize, 128, 7, 7)\n",
    "                nn.ReLU(),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2, padding=1),  # (batchsize, 128, 4, 4)\n",
    "                nn.Conv2d(channels[2], channels[3], kernel_size=3, padding=1),  # (batchsize, 256, 4, 4)\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        self.tconvs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(channels[3], channels[2], kernel_size=3, \n",
    "                                   stride=2, padding=1, output_padding=0),   # (batchsize, 128, 7, 7)\n",
    "                nn.ReLU()\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(channels[2], channels[1], kernel_size=3,\n",
    "                                   stride=2, padding=1, output_padding=1),   # (batchsize, 64, 14, 14)\n",
    "                nn.ReLU()\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(channels[1], channels[0], kernel_size=3, \n",
    "                                   stride=2, padding=1, output_padding=1),   # (batchsize, 32, 28, 28)\n",
    "                nn.ReLU()\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(channels[0],channels[0],kernel_size=3,padding=1),  # (batchsize, 32, 28, 28)\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(channels[0],1,kernel_size=1) # (batchsize, 1, 28, 28)\n",
    "            )      \n",
    "        ])\n",
    "\n",
    "    def attention_block(self, x, g, iter_num):\n",
    "        # x er fra skip-connections, g er nedefra (fra t-convs)\n",
    "        x = (x - torch.mean(x)) / torch.std(x)\n",
    "        #g = (g - torch.mean(g)) / torch.std(g)\n",
    "        res = self.relu(torch.add(x, g))\n",
    "        res = self.sigmoid(self.convs_attention[iter_num](res))\n",
    "        res = torch.mul(x, res)\n",
    "        return res\n",
    "        \n",
    "        \n",
    "    def forward(self, x, t): \n",
    "        x_trans = torch.cat((x, t), dim=-3)\n",
    "        signal = x_trans\n",
    "        signals = []\n",
    "\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            # print(f\"conv {i}\")\n",
    "            signal = conv(signal)\n",
    "            # print(signal.shape)\n",
    "            if i < len(conv):\n",
    "                signals.append(signal)\n",
    "        \n",
    "        for i, tconv in enumerate(self.tconvs):\n",
    "            # print(f\"tconv {i}\")\n",
    "            # print(f\"signal shape: {signal.shape}\")\n",
    "            if i == 0:\n",
    "                signal = tconv(signal)\n",
    "            else:\n",
    "                signal = self.attention_block(signals[-i], signal, i-1)\n",
    "                signal = tconv(signal)\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f04a5df-da69-407e-9285-90de11992cef",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8403ab0-aea6-492a-98b9-31ab55064c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from UNET import UNET\n",
    "epochs = 20\n",
    "model = UNET()\n",
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.MSELoss(reduction=\"sum\")\n",
    "running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dc7e14a-63cd-4432-8f07-43fd00841fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 100), loss: 1255983.892\n",
      "(0, 200), loss: 1248999.861\n",
      "(0, 300), loss: 737596.429\n",
      "(0, 400), loss: 213257.481\n",
      "(0, 500), loss: 162281.977\n",
      "(0, 600), loss: 150298.658\n",
      "(0, 700), loss: 130774.483\n",
      "(0, 800), loss: 124556.714\n",
      "(0, 900), loss: 115610.000\n",
      "(0, 1000), loss: 98599.023\n",
      "(0, 1100), loss: 99232.525\n",
      "(0, 1200), loss: 93927.941\n",
      "(0, 1300), loss: 101457.683\n",
      "(0, 1400), loss: 97409.642\n",
      "(0, 1500), loss: 84902.948\n",
      "(0, 1600), loss: 100722.324\n",
      "(0, 1700), loss: 87278.003\n",
      "(0, 1800), loss: 91724.527\n",
      "(0, 1900), loss: 77018.104\n",
      "(0, 2000), loss: 80400.894\n",
      "(0, 2100), loss: 86284.714\n",
      "(0, 2200), loss: 89190.753\n",
      "(0, 2300), loss: 79691.510\n",
      "(0, 2400), loss: 78424.656\n",
      "(0, 2500), loss: 73924.738\n",
      "(0, 2600), loss: 74731.778\n",
      "(0, 2700), loss: 70937.353\n",
      "(0, 2800), loss: 76137.517\n",
      "(0, 2900), loss: 72241.253\n",
      "(0, 3000), loss: 66463.484\n",
      "(0, 3100), loss: 68828.494\n",
      "(0, 3200), loss: 74108.298\n",
      "(0, 3300), loss: 69042.471\n",
      "(0, 3400), loss: 70040.519\n",
      "(0, 3500), loss: 69315.271\n",
      "(0, 3600), loss: 70015.792\n",
      "(0, 3700), loss: 71444.112\n",
      "(1, 100), loss: 106602.671\n",
      "(1, 200), loss: 69630.705\n",
      "(1, 300), loss: 71494.318\n",
      "(1, 400), loss: 70466.099\n",
      "(1, 500), loss: 75869.878\n",
      "(1, 600), loss: 69602.602\n",
      "(1, 700), loss: 64233.064\n",
      "(1, 800), loss: 65423.504\n",
      "(1, 900), loss: 68153.796\n",
      "(1, 1000), loss: 67542.110\n",
      "(1, 1100), loss: 66067.284\n",
      "(1, 1200), loss: 61632.007\n",
      "(1, 1300), loss: 63687.152\n",
      "(1, 1400), loss: 64321.623\n",
      "(1, 1500), loss: 64596.687\n",
      "(1, 1600), loss: 60173.163\n",
      "(1, 1700), loss: 71562.915\n",
      "(1, 1800), loss: 62636.726\n",
      "(1, 1900), loss: 63433.848\n",
      "(1, 2000), loss: 61241.800\n",
      "(1, 2100), loss: 64236.657\n",
      "(1, 2200), loss: 60667.182\n",
      "(1, 2300), loss: 56542.406\n",
      "(1, 2400), loss: 64378.079\n",
      "(1, 2500), loss: 60695.173\n",
      "(1, 2600), loss: 65793.468\n",
      "(1, 2700), loss: 68454.969\n",
      "(1, 2800), loss: 65028.686\n",
      "(1, 2900), loss: 58998.011\n",
      "(1, 3000), loss: 58099.792\n",
      "(1, 3100), loss: 59080.256\n",
      "(1, 3200), loss: 59455.402\n",
      "(1, 3300), loss: 62399.265\n",
      "(1, 3400), loss: 62893.794\n",
      "(1, 3500), loss: 63509.482\n",
      "(1, 3600), loss: 64661.543\n",
      "(1, 3700), loss: 57387.309\n",
      "(2, 100), loss: 86714.805\n",
      "(2, 200), loss: 54453.447\n",
      "(2, 300), loss: 55137.576\n",
      "(2, 400), loss: 55045.069\n",
      "(2, 500), loss: 51581.323\n",
      "(2, 600), loss: 60761.934\n",
      "(2, 700), loss: 51588.269\n",
      "(2, 800), loss: 58151.387\n",
      "(2, 900), loss: 66161.919\n",
      "(2, 1000), loss: 70103.905\n",
      "(2, 1100), loss: 68690.233\n",
      "(2, 1200), loss: 57795.343\n",
      "(2, 1300), loss: 55677.403\n",
      "(2, 1400), loss: 52916.497\n",
      "(2, 1500), loss: 58012.631\n",
      "(2, 1600), loss: 50944.021\n",
      "(2, 1700), loss: 56915.865\n",
      "(2, 1800), loss: 58784.366\n",
      "(2, 1900), loss: 56558.051\n",
      "(2, 2000), loss: 54381.557\n",
      "(2, 2100), loss: 67386.555\n",
      "(2, 2200), loss: 57378.537\n",
      "(2, 2300), loss: 65987.614\n",
      "(2, 2400), loss: 56634.623\n",
      "(2, 2500), loss: 52304.474\n",
      "(2, 2600), loss: 55419.017\n",
      "(2, 2700), loss: 60331.883\n",
      "(2, 2800), loss: 48775.689\n",
      "(2, 2900), loss: 50756.151\n",
      "(2, 3000), loss: 48500.573\n",
      "(2, 3100), loss: 57335.145\n",
      "(2, 3200), loss: 56652.158\n",
      "(2, 3300), loss: 54200.400\n",
      "(2, 3400), loss: 61176.081\n",
      "(2, 3500), loss: 60810.362\n",
      "(2, 3600), loss: 46765.758\n",
      "(2, 3700), loss: 52871.238\n",
      "(3, 100), loss: 84079.252\n",
      "(3, 200), loss: 73488.622\n",
      "(3, 300), loss: 61692.566\n",
      "(3, 400), loss: 55439.814\n",
      "(3, 500), loss: 55343.778\n",
      "(3, 600), loss: 53445.564\n",
      "(3, 700), loss: 56889.446\n",
      "(3, 800), loss: 52553.546\n",
      "(3, 900), loss: 51636.233\n",
      "(3, 1000), loss: 60443.063\n",
      "(3, 1100), loss: 60159.653\n",
      "(3, 1200), loss: 59716.026\n",
      "(3, 1300), loss: 49003.908\n",
      "(3, 1400), loss: 56848.053\n",
      "(3, 1500), loss: 61224.448\n",
      "(3, 1600), loss: 66084.310\n",
      "(3, 1700), loss: 51028.349\n",
      "(3, 1800), loss: 45163.644\n",
      "(3, 1900), loss: 50051.842\n",
      "(3, 2000), loss: 52185.273\n",
      "(3, 2100), loss: 54632.450\n",
      "(3, 2200), loss: 46778.197\n",
      "(3, 2300), loss: 49977.367\n",
      "(3, 2400), loss: 59612.051\n",
      "(3, 2500), loss: 55692.886\n",
      "(3, 2600), loss: 55829.631\n",
      "(3, 2700), loss: 50577.123\n",
      "(3, 2800), loss: 51838.022\n",
      "(3, 2900), loss: 48870.813\n",
      "(3, 3000), loss: 50741.186\n",
      "(3, 3100), loss: 53373.954\n",
      "(3, 3200), loss: 55383.352\n",
      "(3, 3300), loss: 53823.777\n",
      "(3, 3400), loss: 48895.992\n",
      "(3, 3500), loss: 54812.398\n",
      "(3, 3600), loss: 51218.288\n",
      "(3, 3700), loss: 54882.609\n",
      "(4, 100), loss: 73885.310\n",
      "(4, 200), loss: 50838.288\n",
      "(4, 300), loss: 47411.101\n",
      "(4, 400), loss: 50133.580\n",
      "(4, 500), loss: 50197.208\n",
      "(4, 600), loss: 49123.366\n",
      "(4, 700), loss: 50904.339\n",
      "(4, 800), loss: 54036.622\n",
      "(4, 900), loss: 51193.007\n",
      "(4, 1000), loss: 54723.219\n",
      "(4, 1100), loss: 55860.146\n",
      "(4, 1200), loss: 50874.484\n",
      "(4, 1300), loss: 47681.759\n",
      "(4, 1400), loss: 45529.765\n",
      "(4, 1500), loss: 48321.038\n",
      "(4, 1600), loss: 50223.296\n",
      "(4, 1700), loss: 53830.760\n",
      "(4, 1800), loss: 61286.503\n",
      "(4, 1900), loss: 50470.247\n",
      "(4, 2000), loss: 50035.115\n",
      "(4, 2100), loss: 49605.740\n",
      "(4, 2200), loss: 47399.653\n",
      "(4, 2300), loss: 48962.463\n",
      "(4, 2400), loss: 46683.287\n",
      "(4, 2500), loss: 55974.436\n",
      "(4, 2600), loss: 48062.471\n",
      "(4, 2700), loss: 53345.049\n",
      "(4, 2800), loss: 51579.105\n",
      "(4, 2900), loss: 54236.227\n",
      "(4, 3000), loss: 49244.733\n",
      "(4, 3100), loss: 51488.723\n",
      "(4, 3200), loss: 43270.396\n",
      "(4, 3300), loss: 45746.771\n",
      "(4, 3400), loss: 47354.153\n",
      "(4, 3500), loss: 58928.871\n",
      "(4, 3600), loss: 51119.225\n",
      "(4, 3700), loss: 48967.566\n",
      "(5, 100), loss: 72305.358\n",
      "(5, 200), loss: 50519.982\n",
      "(5, 300), loss: 46218.754\n",
      "(5, 400), loss: 47896.146\n",
      "(5, 500), loss: 49844.087\n",
      "(5, 600), loss: 45199.653\n",
      "(5, 700), loss: 45757.207\n",
      "(5, 800), loss: 51494.262\n",
      "(5, 900), loss: 45137.822\n",
      "(5, 1000), loss: 42305.677\n",
      "(5, 1100), loss: 43250.327\n",
      "(5, 1200), loss: 45864.127\n",
      "(5, 1300), loss: 47405.412\n",
      "(5, 1400), loss: 50326.486\n",
      "(5, 1500), loss: 53930.784\n",
      "(5, 1600), loss: 53231.385\n",
      "(5, 1700), loss: 45036.863\n",
      "(5, 1800), loss: 52734.228\n",
      "(5, 1900), loss: 50560.497\n",
      "(5, 2000), loss: 42715.439\n",
      "(5, 2100), loss: 50405.194\n",
      "(5, 2200), loss: 55099.011\n",
      "(5, 2300), loss: 46116.417\n",
      "(5, 2400), loss: 44861.801\n",
      "(5, 2500), loss: 46924.952\n",
      "(5, 2600), loss: 46316.228\n",
      "(5, 2700), loss: 52742.547\n",
      "(5, 2800), loss: 44964.383\n",
      "(5, 2900), loss: 42553.387\n",
      "(5, 3000), loss: 41866.113\n",
      "(5, 3100), loss: 43697.010\n",
      "(5, 3200), loss: 49790.903\n",
      "(5, 3300), loss: 45612.535\n",
      "(5, 3400), loss: 45332.852\n",
      "(5, 3500), loss: 50272.282\n",
      "(5, 3600), loss: 47861.511\n",
      "(5, 3700), loss: 45378.818\n",
      "(6, 100), loss: 70635.909\n",
      "(6, 200), loss: 46108.592\n",
      "(6, 300), loss: 45067.233\n",
      "(6, 400), loss: 41536.252\n",
      "(6, 500), loss: 43552.817\n",
      "(6, 600), loss: 46266.321\n",
      "(6, 700), loss: 49229.910\n",
      "(6, 800), loss: 38618.762\n",
      "(6, 900), loss: 42240.930\n",
      "(6, 1000), loss: 42687.769\n",
      "(6, 1100), loss: 38132.202\n",
      "(6, 1200), loss: 47351.551\n",
      "(6, 1300), loss: 42778.004\n",
      "(6, 1400), loss: 44881.935\n",
      "(6, 1500), loss: 44604.177\n",
      "(6, 1600), loss: 44543.437\n",
      "(6, 1700), loss: 47482.590\n",
      "(6, 1800), loss: 45668.104\n",
      "(6, 1900), loss: 42175.615\n",
      "(6, 2000), loss: 46797.610\n",
      "(6, 2100), loss: 46679.982\n",
      "(6, 2200), loss: 40421.971\n",
      "(6, 2300), loss: 41963.595\n",
      "(6, 2400), loss: 40184.326\n",
      "(6, 2500), loss: 43660.288\n",
      "(6, 2600), loss: 44697.583\n",
      "(6, 2700), loss: 46051.471\n",
      "(6, 2800), loss: 48842.986\n",
      "(6, 2900), loss: 39339.886\n",
      "(6, 3000), loss: 43524.801\n",
      "(6, 3100), loss: 46401.310\n",
      "(6, 3200), loss: 40419.884\n",
      "(6, 3300), loss: 40628.724\n",
      "(6, 3400), loss: 44634.196\n",
      "(6, 3500), loss: 40324.944\n",
      "(6, 3600), loss: 46594.123\n",
      "(6, 3700), loss: 42432.919\n",
      "(7, 100), loss: 62674.313\n",
      "(7, 200), loss: 37812.191\n",
      "(7, 300), loss: 41540.145\n",
      "(7, 400), loss: 38269.508\n",
      "(7, 500), loss: 47948.420\n",
      "(7, 600), loss: 48124.264\n",
      "(7, 700), loss: 46915.071\n",
      "(7, 800), loss: 39909.145\n",
      "(7, 900), loss: 39100.574\n",
      "(7, 1000), loss: 39174.005\n",
      "(7, 1100), loss: 44660.343\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m eps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# print(eps.shape)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# print(x0.shape)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(eps, model(torch\u001b[38;5;241m.\u001b[39msqrt(alpha_bar[t\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m*\u001b[39m x0 \u001b[38;5;241m+\u001b[39m \n\u001b[0;32m     10\u001b[0m                             torch\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha_bar[t\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m*\u001b[39m eps, t\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(batch_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)))\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     12\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[14], line 71\u001b[0m, in \u001b[0;36mUNET.forward\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     67\u001b[0m signals \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs):\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# print(f\"conv {i}\")\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m     signal \u001b[38;5;241m=\u001b[39m conv(signal)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# print(signal.shape)\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(conv):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:103\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for e, data in enumerate(train_loader):\n",
    "        x0, _ = data\n",
    "        x0 = x0.to(device)\n",
    "        t = torch.randint(1, T+1, (batch_size,)).to(device)\n",
    "        eps = torch.randn(batch_size, 1, 28, 28).to(device)\n",
    "        # print(eps.shape)\n",
    "        # print(x0.shape)\n",
    "        loss = criterion(eps, model(torch.sqrt(alpha_bar[t-1]) * x0 + \n",
    "                                    torch.sqrt(1 - alpha_bar[t-1]) * eps, t.view(batch_size, 1, 1, 1).expand(batch_size, 1, 28, 28)))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if e % 100 == 99:\n",
    "            print(f'{epoch, e+1}, loss: {running_loss:.3f}')\n",
    "            running_loss = 0.0\n",
    "        \n",
    "    if epoch % 5 == 4:\n",
    "        torch.save(model.state_dict(), f\"DDPM_{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54a74a6-6786-43c8-9b7b-569eb01df2f2",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16cd8cdb-77e0-4881-ace7-79d6da4848ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNET(\n",
       "  (relu): ReLU()\n",
       "  (convs_attention): ModuleList(\n",
       "    (0): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (sigmoid): Sigmoid()\n",
       "  (convs): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(2, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (tconvs): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53f69f28-7031-48bc-96d4-85b61ba8d524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f0f2e92810>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 1\n",
    "xt = torch.randn(batch_size, 1, 28, 28).to(device)\n",
    "\n",
    "for t in torch.arange(T, 0, -1):\n",
    "    #print(t)\n",
    "    t = t.to(device)\n",
    "    z = torch.randn(batch_size, 1, 28, 28).to(device) if t > 1 else torch.zeros(batch_size, 1, 28, 28).to(device)\n",
    "    xt_new = 1 / torch.sqrt(alpha[t - 1]) * (xt - (1 - alpha[t - 1])/(torch.sqrt(1 - alpha_bar[t - 1])) * \n",
    "                                                   model(xt, t.view(batch_size, 1, 1, 1).expand(batch_size, 1, 28, 28))) + torch.sqrt(beta[t-1]) * z\n",
    "    xt = xt_new\n",
    "    #if t == 1:\n",
    "        #print((xt - (1 - alpha[t - 1])/(torch.sqrt(1 - alpha_bar[t - 1]))))\n",
    "plt.imshow(xt[0][0].cpu().detach().numpy(), cmap=\"grey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cfbadb66-3f6a-4dd8-b8db-4c0d1662a430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9998999834]]], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(precision=10)\n",
    "alpha_bar[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "76f66843-b77f-4798-a78b-806310d3057b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f371ef1950>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xt[0][0].cpu().detach().numpy(), cmap=\"grey\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
