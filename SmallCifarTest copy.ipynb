{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "def4f85a-37b5-472e-8782-a5df13c5d601",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec1af4-1d61-46ef-90b9-b972129d09a5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eeb867f-c38e-4e5c-ab89-a7e747ead472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "017d87cd-410b-44ef-ac34-560022867208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "#torch.backends.cudnn.enabled = False\n",
    "val_size = 5000\n",
    "test_size = 5000\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
    "transform_valid_test = transforms.Compose([transforms.Resize((299, 299)), \n",
    "                                           transforms.ToTensor()])\n",
    "\n",
    "# Downloading MNIST again :) Training (60k) and test(5k) + val(5k) split\n",
    "train_loader = torch.utils.data.DataLoader(datasets.CIFAR10('./CIFAR_data2',\n",
    "                                            download=True,\n",
    "                                            train=True,\n",
    "                                            transform=transform),\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=num_workers)\n",
    "\n",
    "test_dataset = datasets.CIFAR10('./CIFAR_data2',\n",
    "                               download=True,\n",
    "                               train=False,\n",
    "                               transform=transform_valid_test)\n",
    "\n",
    "val_dataset, test_dataset = random_split(test_dataset, [val_size, test_size])\n",
    "\n",
    "# Test set to compare with DDPM paper\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# Validation set so we can keep track of approximated FID score while training\n",
    "validation_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=num_workers)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb46ce7-ff79-43f0-a03c-412978806bdb",
   "metadata": {},
   "source": [
    "# Ny block: TrÃ¦kker 5000 CIFAR10 billeder til at beregne training FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "301580f7-7363-43aa-8e7b-47b21b8d85f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_FID_dataset = datasets.CIFAR10('./CIFAR_data2',\n",
    "                               download=True,\n",
    "                               train=True,\n",
    "                               transform=transform_valid_test)\n",
    "\n",
    "train_for_FID, _ = random_split(train_FID_dataset, [5000, 45000])\n",
    "\n",
    "train_for_FID_loader = torch.utils.data.DataLoader(train_for_FID, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5630601-e376-4f9d-a13b-cefb9d3dcea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COSINE SCHEDULE\n",
    "\n",
    "def f(t, s=torch.tensor([0.008]), T=torch.tensor([1000])):\n",
    "    return (torch.cos((t / T + s) / (1 + s) * (torch.pi / 2)).pow(2)).clamp(max=0.99999)\n",
    "\n",
    "T = 1000\n",
    "ts = torch.arange(T)\n",
    "alpha_bar = torch.tensor([(f(t)/f(torch.tensor([0]))).clamp(max=0.99999) for t in ts]) \n",
    "beta = torch.tensor([1 - alpha_bar[t]/(alpha_bar[t-1]) if t > 0 else torch.tensor([0]) for t in ts])\n",
    "alpha = 1 - beta\n",
    "alpha = alpha.view((1000, 1, 1, 1)).to(device)\n",
    "beta = beta.view((1000, 1, 1, 1)).to(device)\n",
    "alpha_bar = alpha_bar.view((1000, 1, 1, 1)).to(device)\n",
    "\n",
    "\n",
    "# # Sets up alpha_bar for training and test so alpha_bar_t = alpha_bar[t]\n",
    "# # LINEAR SCHEDULE\n",
    "# T = 1000\n",
    "# beta_start, beta_end = [1e-4, 2e-02]\n",
    "# beta = torch.linspace(beta_start, beta_end, T)\n",
    "# alpha = 1-beta\n",
    "# alpha_bar = alpha.clone()\n",
    "# for e in range(T-1):\n",
    "#     alpha_bar[e+1] *= alpha_bar[e]\n",
    "\n",
    "# alpha = alpha.view((1000, 1, 1, 1)).to(device)\n",
    "# beta = beta.view((1000, 1, 1, 1)).to(device)\n",
    "# alpha_bar = alpha_bar.view((1000, 1, 1, 1)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42fe879-ee13-4434-9a0d-de015ddd4c2f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2624c48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class SinusoidalEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(SinusoidalEmbedding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        half_dim = self.embedding_dim // 2\n",
    "        freqs = torch.exp(\n",
    "            -torch.arange(half_dim, dtype=torch.float32) * math.log(10000) / half_dim\n",
    "        ).to(t.device)\n",
    "        angles = t[:, None] * freqs[None, :]\n",
    "        return torch.cat([angles.sin(), angles.cos()], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c671dbab-ffe9-4fe5-841a-80042e7593b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNET, self).__init__()\n",
    "        self.channels = [3,32, 64, 128, 256, 512]\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(self.channels[0], self.channels[1], kernel_size=3, padding=1),  # (batchsize, 32, 32, 32)\n",
    "                nn.GroupNorm(4, self.channels[1]),\n",
    "                nn.ReLU(),\n",
    "                # nn.Dropout2d(0.1)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2),  # (batchsize, 32, 16, 16)\n",
    "                nn.Conv2d(self.channels[1], self.channels[2], kernel_size=3, padding=1),  # (batchsize, 64, 16, 16)\n",
    "                nn.GroupNorm(4, self.channels[2]),\n",
    "                nn.ReLU(),\n",
    "                # nn.Dropout2d(0.1)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2),  # (batchsize, 64, 7, 7)\n",
    "                nn.Conv2d(self.channels[2], self.channels[3], kernel_size=3, padding=1),  # (batchsize, 128, 8, 8)\n",
    "                nn.GroupNorm(8, self.channels[3]),\n",
    "                nn.ReLU(),\n",
    "                # nn.Dropout2d(0.1)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2),  # (batchsize, 128, 4, 4)\n",
    "                nn.Conv2d(self.channels[3], self.channels[4], kernel_size=3, padding=1),  # (batchsize, 256, 4, 4)\n",
    "                nn.GroupNorm(8, self.channels[4]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2),  # (batchsize, 256, 2, 2)\n",
    "                nn.Conv2d(self.channels[4], self.channels[5], kernel_size=3, padding=1),  # (batchsize, 512, 2, 2)\n",
    "                nn.GroupNorm(8, self.channels[5]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2)\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        self.tconvs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(self.channels[5], self.channels[4], kernel_size=3,\n",
    "                                      stride=2, padding=1, output_padding=1),  # (batchsize, 256, 4, 4)\n",
    "                nn.GroupNorm(8, self.channels[4]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(self.channels[4]*2, self.channels[3], kernel_size=3, \n",
    "                                   stride=2, padding=1, output_padding=1),   # (batchsize, 128, 8, 8)\n",
    "                nn.GroupNorm(8, self.channels[3]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(self.channels[3]*2, self.channels[2], kernel_size=3,\n",
    "                                   stride=2, padding=1, output_padding=1),   # (batchsize, 64, 16, 16)\n",
    "                nn.GroupNorm(8, self.channels[2]),\n",
    "                nn.ReLU(),\n",
    "                # nn.Dropout2d(0.1)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(self.channels[2]*2, self.channels[1], kernel_size=3, \n",
    "                                   stride=2, padding=1, output_padding=1),   # (batchsize, 32, 32, 32)\n",
    "                nn.GroupNorm(4, self.channels[1]),\n",
    "                nn.ReLU(),\n",
    "                # nn.Dropout2d(0.1)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(self.channels[1]*2, self.channels[1], kernel_size=3, padding=1),  # (batchsize, 32, 32, 32)\n",
    "                nn.GroupNorm(4, self.channels[1]),\n",
    "                nn.ReLU(),\n",
    "                # nn.Dropout2d(0.1),\n",
    "                nn.Conv2d(self.channels[1], self.channels[0], kernel_size=1)  # (batchsize, 3, 32, 32)\n",
    "            )      \n",
    "        ])\n",
    "        \n",
    "        self.time_layers = nn.ModuleList([\n",
    "                nn.Linear(128, self.channels[i]) for i in range(len(self.channels))\n",
    "        ])\n",
    "\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        signal = x\n",
    "        signals = []\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            # print(f\"signal shape: {signal.shape}\")\n",
    "            t_emb_processed = self.time_layers[i](t_emb).view(-1, self.channels[i], 1, 1)\n",
    "            signal= t_emb_processed+signal\n",
    "            signal = conv(signal)\n",
    "            if i < len(self.convs)-1:\n",
    "                signals.append(signal)\n",
    "        \n",
    "        for i, tconv in enumerate(self.tconvs):\n",
    "            # print(f\"signal shape: {signal.shape}\")\n",
    "            # print(f\"signals[-{i}] shape: {signals[-i].shape}\")\n",
    "            # print()\n",
    "            t_emb_processed = self.time_layers[-(i + 1)](t_emb).view(-1, self.channels[-(i + 1)], 1, 1)\n",
    "            signal= signal+t_emb_processed\n",
    "            if i == 0:\n",
    "                signal = tconv(signal)\n",
    "            else:\n",
    "                signal = torch.cat((signal, signals[-i]), dim=-3)\n",
    "                signal = tconv(signal)\n",
    "\n",
    "        return signal\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f04a5df-da69-407e-9285-90de11992cef",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8403ab0-aea6-492a-98b9-31ab55064c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from UNET import UNET\n",
    "epochs = 1707\n",
    "model = UNET()\n",
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.MSELoss(reduction=\"sum\")\n",
    "running_loss = 0\n",
    "\n",
    "# Initialize the linear time embedding layer\n",
    "time_embedding_dim = 128  # You can set this to any suitable size\n",
    "time_embedding_layer = SinusoidalEmbedding(time_embedding_dim).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45b5a0b-b53c-439b-8a09-d417af98bc74",
   "metadata": {},
   "source": [
    "# TrÃ¦ner og gemmer model 1, 100, 200, 300, 400, 500, 800, 1100, 1400 og 1707. Gemmer losses i losses.txt. Kan evt. Ã¦ndres til at gemme fÃ¦rre modeller hvis det tager lang tid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dc7e14a-63cd-4432-8f07-43fd00841fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 100), loss: 14333280.953\n",
      "(0, 200), loss: 5707367.441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x10f7b00e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/markus/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Users/markus/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1443, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Users/markus/miniconda3/lib/python3.11/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/markus/miniconda3/lib/python3.11/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/markus/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 947, in wait\n",
      "    ready = selector.select(timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/markus/miniconda3/lib/python3.11/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 16\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m99\u001b[39m:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch,\u001b[38;5;250m \u001b[39me\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs+1):\n",
    "    for e, data in enumerate(train_loader):\n",
    "        x0, _ = data\n",
    "        x0 = x0.to(device)\n",
    "        t = torch.randint(0, T, (batch_size,), device=device)\n",
    "        t_norm = t.float() / T\n",
    "        t_emb = time_embedding_layer(t_norm)  # Shape: [batch_size, embedding_dim]\n",
    "        eps = torch.randn_like(x0).to(device)\n",
    "        x_t = torch.sqrt(alpha_bar[t]) * x0 + torch.sqrt(1 - alpha_bar[t]) * eps\n",
    "        predicted_eps = model(x_t, t_emb)  # Pass the precomputed embedding to the model\n",
    "        loss = criterion(predicted_eps, eps)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if e % 100 == 99:\n",
    "            print(f'{epoch, e+1}, loss: {running_loss:.3f}')\n",
    "            with open(\"losses\", \"a\") as file:\n",
    "                file.write(f\"{running_loss:.3f}\\n\")\n",
    "            \n",
    "            running_loss = 0.0\n",
    "        \n",
    "    if epoch in [1, 100, 200, 300, 400, 500, 800, 1100, 1400]:\n",
    "        torch.save(model.state_dict(), f\"DDPM_final_linear_{epoch}.pth\")\n",
    "torch.save(model.state_dict(), f\"DDPM_final_linear_{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8315c4-019e-40b3-b075-216b2d7bad99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d54a74a6-6786-43c8-9b7b-569eb01df2f2",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a774f9-279c-45dc-9aa0-df511f200960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = UNET()\n",
    "#model.to(device)\n",
    "#model.load_state_dict(torch.load(\"DDPM_final_linear_300.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53f69f28-7031-48bc-96d4-85b61ba8d524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "xt = torch.randn(1, 3, 32, 32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for t in torch.arange(T, 0, -1):\n",
    "        t = t.reshape(1).to(device)\n",
    "        t_norm = t.float() / T\n",
    "        \n",
    "        t_emb = time_embedding_layer(t_norm)\n",
    "\n",
    "        z = torch.randn(1, 3, 32, 32).to(device) if t > 1 else torch.zeros(1, 3, 32, 32).to(device)\n",
    "        \n",
    "        xt_new = 1 / torch.sqrt(alpha[t]) * (xt - (1 - alpha[t]) / torch.sqrt(1 - alpha_bar[t]) * model(xt, t_emb)) + torch.sqrt(beta[t]) * z\n",
    "        xt = xt_new.clamp(-1, 1)\n",
    "        \n",
    "im1 = xt[0]\n",
    "im1 = (im1+1)/2\n",
    "im1 = im1.permute(1, 2, 0)\n",
    "im1 = im1.cpu().detach().numpy()\n",
    "\n",
    "fig,ax = plt.subplots(1, 1, tight_layout=True)\n",
    "ax.imshow(im1)\n",
    "ax.set_title(\"Generated Image\")\n",
    "ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948e4619-6611-4ed0-b4d0-0de38910d97f",
   "metadata": {},
   "source": [
    "# Making test and valid set - commented out, don't touch again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d9c3c6-83b5-436a-b1d9-0bbd38a0d5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Test set: \")\n",
    "#for i, (img, _) in tqdm(enumerate(test_loader)):\n",
    "#    for j in range(img.size(0)):\n",
    "#        save_image(img[j], os.path.join(test_dir, f\"{i * batchsize + j}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d34c29-cfe6-440c-9771-4163bea4ed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Valid set: \")\n",
    "#for i, (img, _) in tqdm(enumerate(validation_loader)):\n",
    "#    for j in range(img.size(0)):\n",
    "#        save_image(img[j], os.path.join(valid_dir, f\"{i * batchsize + j}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e2290a-7bcc-4864-9b7d-c5c31b395774",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Train set: \")\n",
    "#for i, (img, _) in tqdm(enumerate(train_for_FID_loader)):\n",
    "#    for j in range(img.size(0)):\n",
    "#        save_image(img[j], os.path.join(train_dir, f\"{i * batchsize + j}.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7c74f5-fc34-486d-a1d6-b741a423d012",
   "metadata": {},
   "source": [
    "# Make samples from the models we have trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a17985b3-1ce0-4bc0-9540-d355a9b0bd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting generation set: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc41740800094cad9f8f079caf5a521d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::_upsample_bilinear2d_aa.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m im \u001b[38;5;241m=\u001b[39m xt[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(im\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 43\u001b[0m im \u001b[38;5;241m=\u001b[39m \u001b[43mresize_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m im \u001b[38;5;241m=\u001b[39m (im \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     45\u001b[0m im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torchvision/transforms/transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:469\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    466\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39mpil_interpolation)\n\u001b[0;32m--> 469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mantialias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torchvision/transforms/_functional_tensor.py:465\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, antialias)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# Define align_corners to avoid warnings\u001b[39;00m\n\u001b[1;32m    463\u001b[0m align_corners \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m interpolation \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mantialias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m interpolation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m out_dtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39muint8:\n\u001b[1;32m    468\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py:4028\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   4026\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m align_corners \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m antialias:\n\u001b[0;32m-> 4028\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_upsample_bilinear2d_aa\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4029\u001b[0m \u001b[38;5;66;03m# Two levels are necessary to prevent TorchScript from touching\u001b[39;00m\n\u001b[1;32m   4030\u001b[0m \u001b[38;5;66;03m# are_deterministic_algorithms_enabled.\u001b[39;00m\n\u001b[1;32m   4031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::_upsample_bilinear2d_aa.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from pytorch_fid import fid_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "valid_dir = './valid_images'\n",
    "test_dir = './test_images'\n",
    "train_dir = './train_images'\n",
    "gen_dir = './gen_images'\n",
    "os.makedirs(valid_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(gen_dir, exist_ok=True)\n",
    "#for epoch in [800, 1100, 1400, 1707]:\n",
    "#    os.makedirs(f\"./gen_images/{epoch}\", exist_ok=True)\n",
    "\n",
    "resize_transform = transforms.Resize((299, 299))\n",
    "with torch.inference_mode():\n",
    "    for epoch in [1, 100, 200, 300, 400, 500, 800, 1100, 1400, 1707]:\n",
    "        model = UNET()\n",
    "        model.to(device)\n",
    "        # model.load_state_dict(torch.load(f\"DDPM_final_linear_{epoch}.pth\"))\n",
    "        model.eval()\n",
    "        \n",
    "        print(\"Starting generation set: \")\n",
    "        for i in tqdm(range(1000)):\n",
    "            xt = torch.randn(batch_size, 3, 32, 32).to(device)\n",
    "            \n",
    "            for t in torch.arange(T, 0, -1):\n",
    "                t = t.reshape(1).to(device)\n",
    "                t_norm = t.float() / T\n",
    "                \n",
    "                t_emb = time_embedding_layer(t_norm)\n",
    "\n",
    "                z = torch.randn(1, 3, 32, 32).to(device) if t > 0 else torch.zeros(1, 3, 32, 32).to(device)\n",
    "                \n",
    "                xt_new = 1 / torch.sqrt(alpha[t]) * (xt - (1 - alpha[t]) / torch.sqrt(1 - alpha_bar[t]) * model(xt, t_emb)) + torch.sqrt(beta[t]) * z\n",
    "                xt = xt_new.clamp(-1, 1)\n",
    "            im = xt[0]\n",
    "            print(im.shape)\n",
    "            im = resize_transform(im)\n",
    "            im = (im + 1) / 2\n",
    "            im = im.clamp(0, 1)\n",
    "            print(im.shape)\n",
    "            print(\"Saving generated image: \")\n",
    "            save_image(im, os.path.join(f\"./gen_images/{epoch}\", f\"{i}.png\"))\n",
    "            print(epoch, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c189f7c-ed52-48ea-a00e-f76a75938492",
   "metadata": {},
   "source": [
    "# Calculate FID scores. 1 validation score per model until the last, where it calculates test and train FID score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4b6a0e-38e1-492f-a26d-b630fe4b0880",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_scores = []\n",
    "test_score = []\n",
    "train_score = []\n",
    "with torch.inference_mode():\n",
    "    for epoch in [1, 100, 200, 300, 400, 500, 800, 1100, 1400, 1707]:\n",
    "            model = UNET()\n",
    "            model.to(device)\n",
    "            model.load_state_dict(torch.load(f\"DDPM_final_linear_{epoch}.pth\"))\n",
    "            model.eval()\n",
    "            if epoch != 1707:\n",
    "                fid_value = fid_score.calculate_fid_given_paths(\n",
    "                    ['./valid_images', f\"./gen_images/{epoch}\"], \n",
    "                    batch_size=batch_size,\n",
    "                    device=device,\n",
    "                    dims=2048\n",
    "                )\n",
    "                validation_scores.append(fid_value)\n",
    "            else:\n",
    "                fid_value = fid_score.calculate_fid_given_paths(\n",
    "                    ['./test_images', f\"./gen_images/{epoch}\"], \n",
    "                    batch_size=batch_size,\n",
    "                    device=device,\n",
    "                    dims=2048\n",
    "                )\n",
    "                test_score.append(fid_value)\n",
    "                fid_train_value = fid_score.calculate_fid_given_paths(\n",
    "                    ['./train_images', f\"./gen_images/{epoch}\"], \n",
    "                    batch_size=batch_size,\n",
    "                    device=device,\n",
    "                    dims=2048\n",
    "                )\n",
    "                train_score.append(fid_train_value)\n",
    "            print(f\"epoch and valid FID Score: {epoch}, {validation_scores[-1]}\")\n",
    "            if epoch == 1707:\n",
    "                print(f\"epoch and test set FID Score: {epoch}, {test_score[0]}\")\n",
    "                print(f\"epoch and train set FID Score: {epoch}, {train_score[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c35a2db-d4e0-4fee-8559-9f55b0b2d89c",
   "metadata": {},
   "source": [
    "# Get one training loss per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7790569a-6d2a-43e8-a62c-315b8a288512",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"losses.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    lines = [float(line.strip()) / 10 for line in lines]\n",
    "    indices = np.linspace(0, len(lines) - 1, 1707).astype(int)\n",
    "    all_epochs_loss = [lines[i] for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa94f5b-96f6-4119-adf1-135ac77751a8",
   "metadata": {},
   "source": [
    "# Plot everything, remember to change title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d6346c-55e6-4b41-8391-5dd3e274976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid_fid_scores = [349.9, 141.6, 150.1, 137.1, 125.4, 124.3, 121.4, 123.7, 117.3]\n",
    "#test_fid_score = [107.5]\n",
    "#train_fid_score = [107.8]\n",
    "\n",
    "epochs_valid_fid = [1, 100, 200, 300, 400, 500, 800, 1100, 1400]\n",
    "epochs_loss = list(range(1707+1))\n",
    "test_fid_score = test_score\n",
    "train_fid_score = train_score\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.suptitle(\"CIFAR10 DDPM\")\n",
    "plt.title(\"Conc. time-embedding and linear noise schedule\")\n",
    "ax1.plot(epochs_loss, np.log(np.array(all_epochs_loss)), label=\"Log-loss\")\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Log-loss', color='darkblue')\n",
    "ax1.tick_params(axis='y', labelcolor='darkblue')\n",
    "ax1.legend(loc=\"upper left\", bbox_to_anchor=(0.757, 0.85))\n",
    "ax2 = ax1.twinx()\n",
    "ax2.scatter(epochs_valid_fid, validation_scores, color=\"red\", marker=\".\", label=\"Validation FID\")\n",
    "ax2.set_ylabel('FID', color='darkred')\n",
    "ax2.tick_params(axis='y', labelcolor='darkred')\n",
    "ax2.scatter([1707], test_fid_score, label=\"Test and train FID\", marker=\"*\", color=\"orange\")\n",
    "ax2.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2205ba8-08c7-4112-9109-4edde50f3665",
   "metadata": {},
   "source": [
    "# Overfit to one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af12741-2e35-47c5-8a80-7c3ef41bacdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from UNET import UNET\n",
    "\n",
    "model = UNET().to(device)\n",
    "# Initialize the linear time embedding layer\n",
    "time_embedding_dim = 128  # You can set this to any suitable size\n",
    "time_embedding_layer = SinusoidalEmbedding(time_embedding_dim).to(device)\n",
    "\n",
    "# Get a single image from the batch\n",
    "single_batch, _ = next(iter(train_loader))\n",
    "single_batch = single_batch[0].unsqueeze(0).to(device)\n",
    "single_batchs = single_batch.repeat(1, 1, 1, 1)\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "# Training loop to overfit one batch\n",
    "epochs = 5000\n",
    "for epoch in range(epochs):\n",
    "    t = torch.randint(0, T, (1,), device=device)\n",
    "    t_norm = t.float() / T # Normalize the time to [0, 1]\n",
    "    t_emb = time_embedding_layer(t_norm)  # Shape: [1, embedding_dim]\n",
    "    eps = torch.randn_like(single_batchs).to(device)\n",
    "    x_t = torch.sqrt(alpha_bar[t]) * single_batchs + torch.sqrt(1 - alpha_bar[t]) * eps\n",
    "    predicted_eps = model(x_t, t_emb)  # Pass the precomputed embedding to the model\n",
    "    loss = criterion(predicted_eps, eps)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c708ad20-2fa1-4c85-a9a5-b65a4506f61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "xt = torch.randn(1, 3, 32, 32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for t in torch.arange(T, 0, -1):\n",
    "        t = t.reshape(1).to(device)\n",
    "        t_norm = t.float() / T\n",
    "        \n",
    "        t_emb = time_embedding_layer(t_norm)\n",
    "\n",
    "        z = torch.randn(1, 3, 32, 32).to(device) if t > 1 else torch.zeros(1, 3, 32, 32).to(device)\n",
    "        \n",
    "        xt_new = 1 / torch.sqrt(alpha[t]) * (xt - (1 - alpha[t]) / torch.sqrt(1 - alpha_bar[t]) * model(xt, t_emb)) + torch.sqrt(beta[t]) * z\n",
    "        xt_new = xt_new.clamp(-1, 1)\n",
    "        xt = xt_new\n",
    "        \n",
    "im1 = xt[0]\n",
    "im1 = (im1+1)/2\n",
    "im1 = im1.permute(1, 2, 0)\n",
    "im1 = im1.cpu().detach().numpy()\n",
    "\n",
    "im2 = single_batch[0]\n",
    "im2 = (im2+1)/2\n",
    "im2 = im2.permute(1, 2, 0)\n",
    "im2 = im2.cpu().detach().numpy()\n",
    "\n",
    "fig,axs = plt.subplots(1, 2, tight_layout=True)\n",
    "axs[0].imshow(im1)\n",
    "axs[0].set_title(\"Generated Image\")\n",
    "axs[0].axis(\"off\")\n",
    "axs[1].imshow(im2)\n",
    "axs[1].set_title(\"Original Image\")\n",
    "axs[1].axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
