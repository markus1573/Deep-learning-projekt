{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "def4f85a-37b5-472e-8782-a5df13c5d601",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec1af4-1d61-46ef-90b9-b972129d09a5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eeb867f-c38e-4e5c-ab89-a7e747ead472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import random_split \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from torch.autograd import Variable\n",
    "from IPython.display import clear_output\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "017d87cd-410b-44ef-ac34-560022867208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "#torch.backends.cudnn.enabled = False\n",
    "val_size = 5000\n",
    "test_size = 5000\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "pin_memory = False if device == torch.device('cpu') else True\n",
    "\n",
    "# transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "\n",
    "\n",
    "# Normalize input images to [-1, 1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "reverse_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: (x + 1) / 2),  # Reverse the normalization to get values in [0, 1]\n",
    "    transforms.Lambda(lambda x: x.clamp(0, 1)),\n",
    "    transforms.Lambda(lambda x: x.permute(1, 2, 0))\n",
    "])\n",
    "\n",
    "\n",
    "# Downloading MNIST again :) Training (60k) and test(5k) + val(5k) split\n",
    "train_loader = torch.utils.data.DataLoader(datasets.CIFAR10('./CIFAR_data',\n",
    "                                            download=True,\n",
    "                                            train=True,\n",
    "                                            transform=transform),\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=num_workers,\n",
    "                                            pin_memory=pin_memory,\n",
    "                                            drop_last=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10('./CIFAR_data',\n",
    "                               download=True,\n",
    "                               train=False,\n",
    "                               transform=transform)\n",
    "\n",
    "val_dataset, test_dataset = random_split(test_dataset, [val_size, test_size])\n",
    "\n",
    "# Test set to compare with DDPM paper\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=num_workers,\n",
    "                                            pin_memory=pin_memory)\n",
    "\n",
    "# Validation set so we can keep track of approximated FID score while training\n",
    "validation_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=num_workers,\n",
    "                                            pin_memory=pin_memory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfd6f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_beta_schedule(T, s=0.008):\n",
    "    steps = torch.linspace(0, T, T + 1, dtype=torch.float32)\n",
    "    alpha_bar = torch.cos(((steps / T) + s) / (1 + s) * (math.pi / 2)) ** 2\n",
    "    beta = 1 - (alpha_bar[1:] / alpha_bar[:-1])\n",
    "    return beta.clamp(1e-5, 0.999)  # Avoid division by zero\n",
    "\n",
    "T = 1000\n",
    "beta = cosine_beta_schedule(T)\n",
    "alpha = 1.0 - beta\n",
    "alpha_bar = torch.cumprod(alpha, dim=0)  # Cumulative product for alpha_bar\n",
    "\n",
    "# Reshape for broadcasting (if required for your model)\n",
    "alpha = alpha.view((T, 1, 1, 1)).to(device)\n",
    "beta = beta.view((T, 1, 1, 1)).to(device)\n",
    "alpha_bar = alpha_bar.view((T, 1, 1, 1)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0ae1fca-cff3-4b53-86d1-65c59740a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Linear noise schedule\n",
    "# T = 1000\n",
    "# beta_start, beta_end = 1e-4, 2e-2\n",
    "# beta = torch.linspace(beta_start, beta_end, T)  # Linear noise schedule\n",
    "# alpha = 1.0 - beta\n",
    "# alpha_bar = torch.cumprod(alpha, dim=0)  # Cumulative product for alpha_bar\n",
    "\n",
    "# # Reshape for broadcasting (if required for your model)\n",
    "# alpha = alpha.view((T, 1, 1, 1)).to(device)\n",
    "# beta = beta.view((T, 1, 1, 1)).to(device)\n",
    "# alpha_bar = alpha_bar.view((T, 1, 1, 1)).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42fe879-ee13-4434-9a0d-de015ddd4c2f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4681b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class SinusoidalEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(SinusoidalEmbedding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        half_dim = self.embedding_dim // 2\n",
    "        freqs = torch.exp(\n",
    "            -torch.arange(half_dim, dtype=torch.float32) * math.log(10000) / half_dim\n",
    "        ).to(t.device)\n",
    "        angles = t[:, None] * freqs[None, :]\n",
    "        return torch.cat([angles.sin(), angles.cos()], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c671dbab-ffe9-4fe5-841a-80042e7593b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNET, self).__init__()\n",
    "        self.channels = [3,32, 64, 128, 256, 512]\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(self.channels[0], self.channels[1], kernel_size=3, padding=1),  # (batchsize, 32, 32, 32)\n",
    "                nn.GroupNorm(4, self.channels[1]),\n",
    "                nn.ReLU(),\n",
    "                # nn.Dropout2d(0.1)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2),  # (batchsize, 32, 16, 16)\n",
    "                nn.Conv2d(self.channels[1], self.channels[2], kernel_size=3, padding=1),  # (batchsize, 64, 16, 16)\n",
    "                nn.GroupNorm(4, self.channels[2]),\n",
    "                nn.ReLU(),\n",
    "                # nn.Dropout2d(0.1)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2),  # (batchsize, 64, 7, 7)\n",
    "                nn.Conv2d(self.channels[2], self.channels[3], kernel_size=3, padding=1),  # (batchsize, 128, 8, 8)\n",
    "                nn.GroupNorm(8, self.channels[3]),\n",
    "                nn.ReLU(),\n",
    "                # nn.Dropout2d(0.1)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2),  # (batchsize, 128, 4, 4)\n",
    "                nn.Conv2d(self.channels[3], self.channels[4], kernel_size=3, padding=1),  # (batchsize, 256, 4, 4)\n",
    "                nn.GroupNorm(8, self.channels[4]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2),  # (batchsize, 256, 2, 2)\n",
    "                nn.Conv2d(self.channels[4], self.channels[5], kernel_size=3, padding=1),  # (batchsize, 512, 2, 2)\n",
    "                nn.GroupNorm(8, self.channels[5]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2)\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        self.tconvs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(self.channels[5], self.channels[4], kernel_size=3,\n",
    "                                      stride=2, padding=1, output_padding=1),  # (batchsize, 256, 4, 4)\n",
    "                nn.GroupNorm(8, self.channels[4]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(self.channels[4]*2, self.channels[3], kernel_size=3, \n",
    "                                   stride=2, padding=1, output_padding=1),   # (batchsize, 128, 8, 8)\n",
    "                nn.GroupNorm(8, self.channels[3]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(self.channels[3]*2, self.channels[2], kernel_size=3,\n",
    "                                   stride=2, padding=1, output_padding=1),   # (batchsize, 64, 16, 16)\n",
    "                nn.GroupNorm(8, self.channels[2]),\n",
    "                nn.ReLU(),\n",
    "                # nn.Dropout2d(0.1)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(self.channels[2]*2, self.channels[1], kernel_size=3, \n",
    "                                   stride=2, padding=1, output_padding=1),   # (batchsize, 32, 32, 32)\n",
    "                nn.GroupNorm(4, self.channels[1]),\n",
    "                nn.ReLU(),\n",
    "                # nn.Dropout2d(0.1)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(self.channels[1]*2, self.channels[1], kernel_size=3, padding=1),  # (batchsize, 32, 32, 32)\n",
    "                nn.GroupNorm(4, self.channels[1]),\n",
    "                nn.ReLU(),\n",
    "                # nn.Dropout2d(0.1),\n",
    "                nn.Conv2d(self.channels[1], self.channels[0], kernel_size=1)  # (batchsize, 3, 32, 32)\n",
    "            )      \n",
    "        ])\n",
    "        \n",
    "        self.time_layers = nn.ModuleList([\n",
    "                nn.Linear(128, self.channels[i]) for i in range(len(self.channels))\n",
    "        ])\n",
    "\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        signal = x\n",
    "        signals = []\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            # print(f\"signal shape: {signal.shape}\")\n",
    "            t_emb_processed = self.time_layers[i](t_emb).view(-1, self.channels[i], 1, 1)\n",
    "            signal= t_emb_processed+signal\n",
    "            signal = conv(signal)\n",
    "            if i < len(self.convs)-1:\n",
    "                signals.append(signal)\n",
    "        \n",
    "        for i, tconv in enumerate(self.tconvs):\n",
    "            # print(f\"signal shape: {signal.shape}\")\n",
    "            # print(f\"signals[-{i}] shape: {signals[-i].shape}\")\n",
    "            # print()\n",
    "            t_emb_processed = self.time_layers[-(i + 1)](t_emb).view(-1, self.channels[-(i + 1)], 1, 1)\n",
    "            signal= signal+t_emb_processed\n",
    "            if i == 0:\n",
    "                signal = tconv(signal)\n",
    "            else:\n",
    "                signal = torch.cat((signal, signals[-i]), dim=-3)\n",
    "                signal = tconv(signal)\n",
    "\n",
    "        return signal\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f04a5df-da69-407e-9285-90de11992cef",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8403ab0-aea6-492a-98b9-31ab55064c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from UNET import UNET\n",
    "epochs = 200\n",
    "model = UNET()\n",
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4,weight_decay=1e-4)\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Initialize the linear time embedding layer\n",
    "time_embedding_dim = 128  # You can set this to any suitable size\n",
    "time_embedding_layer = SinusoidalEmbedding(time_embedding_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc7e14a-63cd-4432-8f07-43fd00841fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = None\n",
    "if load_model:\n",
    "    model.load_state_dict(torch.load(load_model, map_location=device))\n",
    "\n",
    "losses = []\n",
    "batchlosses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for e, data in enumerate(train_loader):\n",
    "        x0, _ = data\n",
    "        x0 = x0.to(device)\n",
    "        t = torch.randint(0, T, (batch_size,), device=device)\n",
    "        t_norm = t.float() / T\n",
    "        t_emb = time_embedding_layer(t_norm)  # Shape: [batch_size, embedding_dim]\n",
    "        eps = torch.randn_like(x0).to(device)\n",
    "        x_t = torch.sqrt(alpha_bar[t]) * x0 + torch.sqrt(1 - alpha_bar[t]) * eps\n",
    "        predicted_eps = model(x_t, t_emb)  # Pass the precomputed embedding to the model\n",
    "        loss = criterion(predicted_eps, eps)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "\n",
    "        # if e % 100 == 99:\n",
    "        #     print(f\"Epoch {epoch}, Batch {e+1}, Average Loss: {np.mean(losses[-100:]):.4f}\")\n",
    "    \n",
    "    batchlosses.append(np.mean(losses[-100:]))\n",
    "    print(f\"Epoch {epoch}, Average Loss: {batchlosses[-1]:.4f}\")\n",
    "    plt.plot(batchlosses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    if epoch % 2 == 0 and epoch > 0:\n",
    "        torch.save(model.state_dict(), f\"data_CIFAR/DDPM_{epoch}.pth\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54a74a6-6786-43c8-9b7b-569eb01df2f2",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f69f28-7031-48bc-96d4-85b61ba8d524",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = None\n",
    "\n",
    "if load_model:\n",
    "    model.load_state_dict(torch.load(load_model, map_location=device))\n",
    "with torch.inference_mode():\n",
    "    model.eval()\n",
    "    batch_size = 1\n",
    "    xt = torch.randn(batch_size, 3, 32, 32).to(device)\n",
    "\n",
    "    for t in torch.arange(T-1, 0-1, -1):\n",
    "        t = t.reshape(1)\n",
    "        t = t.to(device)\n",
    "        t_emb = time_embedding_layer(t.float())\n",
    "        z = torch.randn(batch_size, 3, 32, 32).to(device) if t > 0 else torch.zeros(batch_size, 3, 32, 32).to(device)\n",
    "        \n",
    "        xt_new = 1 / torch.sqrt(alpha[t]) * (xt - (1 - alpha[t])/(torch.sqrt(1 - alpha_bar[t])) * \n",
    "                                                    model(xt, t_emb)) + torch.sqrt(beta[t]) * z\n",
    "        xt = xt_new\n",
    "\n",
    "image = torch.permute(xt,(2,3,1,0))\n",
    "image = image.reshape(32,32,3)\n",
    "print(image.max())\n",
    "print(image.min())\n",
    "#weird clipping\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7612842",
   "metadata": {},
   "source": [
    "# Overfit one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ac8f961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99, Average Loss: 2361.9143\n",
      "Epoch 199, Average Loss: 965.3192\n",
      "Epoch 299, Average Loss: 715.3903\n",
      "Epoch 399, Average Loss: 570.3433\n",
      "Epoch 499, Average Loss: 543.0333\n",
      "Epoch 599, Average Loss: 376.7072\n",
      "Epoch 699, Average Loss: 435.8806\n",
      "Epoch 799, Average Loss: 402.7203\n",
      "Epoch 899, Average Loss: 299.6741\n",
      "Epoch 999, Average Loss: 242.1120\n",
      "Epoch 1099, Average Loss: 262.6620\n",
      "Epoch 1199, Average Loss: 283.2586\n",
      "Epoch 1299, Average Loss: 239.3669\n",
      "Epoch 1399, Average Loss: 198.6864\n",
      "Epoch 1499, Average Loss: 256.3014\n",
      "Epoch 1599, Average Loss: 303.7292\n",
      "Epoch 1699, Average Loss: 245.0025\n",
      "Epoch 1799, Average Loss: 182.1797\n",
      "Epoch 1899, Average Loss: 174.6387\n",
      "Epoch 1999, Average Loss: 158.4817\n",
      "Epoch 2099, Average Loss: 226.3253\n",
      "Epoch 2199, Average Loss: 188.6242\n",
      "Epoch 2299, Average Loss: 142.9442\n",
      "Epoch 2399, Average Loss: 223.9206\n",
      "Epoch 2499, Average Loss: 170.1531\n",
      "Epoch 2599, Average Loss: 236.6585\n",
      "Epoch 2699, Average Loss: 128.4061\n",
      "Epoch 2799, Average Loss: 198.6085\n",
      "Epoch 2899, Average Loss: 205.0987\n",
      "Epoch 2999, Average Loss: 169.4859\n",
      "Epoch 3099, Average Loss: 174.4585\n",
      "Epoch 3199, Average Loss: 181.9406\n",
      "Epoch 3299, Average Loss: 150.9977\n",
      "Epoch 3399, Average Loss: 105.3551\n",
      "Epoch 3499, Average Loss: 155.5491\n",
      "Epoch 3599, Average Loss: 232.3955\n",
      "Epoch 3699, Average Loss: 145.8431\n",
      "Epoch 3799, Average Loss: 168.8719\n",
      "Epoch 3899, Average Loss: 104.4923\n",
      "Epoch 3999, Average Loss: 112.6311\n",
      "Epoch 4099, Average Loss: 178.0367\n",
      "Epoch 4199, Average Loss: 126.8728\n",
      "Epoch 4299, Average Loss: 131.7652\n",
      "Epoch 4399, Average Loss: 96.3671\n",
      "Epoch 4499, Average Loss: 143.5175\n",
      "Epoch 4599, Average Loss: 125.8500\n",
      "Epoch 4699, Average Loss: 133.0680\n",
      "Epoch 4799, Average Loss: 120.5879\n",
      "Epoch 4899, Average Loss: 196.6130\n",
      "Epoch 4999, Average Loss: 176.1489\n"
     ]
    }
   ],
   "source": [
    "#from UNET import UNET\n",
    "\n",
    "model = UNET().to(device)\n",
    "# Initialize the linear time embedding layer\n",
    "time_embedding_dim = 128  # You can set this to any suitable size\n",
    "time_embedding_layer = SinusoidalEmbedding(time_embedding_dim).to(device)\n",
    "\n",
    "# Get a single image from the batch\n",
    "single_batch, _ = next(iter(train_loader))\n",
    "single_batch = single_batch[0].unsqueeze(0).to(device)\n",
    "single_batchs = single_batch.repeat(1, 1, 1, 1)\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4,weight_decay=1e-4)\n",
    "criterion = torch.nn.MSELoss(reduction=\"sum\")\n",
    "plot_every = 400\n",
    "\n",
    "running_loss = 0\n",
    "# Training loop to overfit one batch\n",
    "epochs = 5000\n",
    "for epoch in range(epochs):\n",
    "    t = torch.randint(0, T, (1,), device=device)\n",
    "    t_norm = t.float() / T # Normalize the time to [0, 1]\n",
    "    t_emb = time_embedding_layer(t_norm)  # Shape: [1, embedding_dim]\n",
    "    eps = torch.randn_like(single_batchs).to(device)\n",
    "    x_t = torch.sqrt(alpha_bar[t]) * single_batchs + torch.sqrt(1 - alpha_bar[t]) * eps\n",
    "    predicted_eps = model(x_t, t_emb)  # Pass the precomputed embedding to the model\n",
    "    loss = criterion(predicted_eps, eps)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "\n",
    "    if epoch % 100 == 99:\n",
    "        print(f\"Epoch {epoch}, Average Loss: {running_loss / 100:.4f}\")\n",
    "        running_loss = 0.0  \n",
    "\n",
    "    # if epoch % plot_every == 0:\n",
    "    #     with torch.inference_mode():\n",
    "    #         model.eval()\n",
    "    #         t = torch.randint(0, T, (1,), device=device)\n",
    "    #         t_emb = time_embedding_layer(t)\n",
    "    #         eps = torch.randn_like(single_batch).to(device)\n",
    "    #         x_t = torch.sqrt(alpha_bar[t]) * single_batch + torch.sqrt(1 - alpha_bar[t]) * eps\n",
    "    #         predicted_eps = model(x_t, t_emb)\n",
    "    #         reconstructed = x_t - predicted_eps\n",
    "    #         fig, axs = plt.subplots(1, 3, tight_layout=True)\n",
    "    #         axs[0].imshow(predicted_eps[0][0].cpu().detach().numpy(), cmap=\"gray\")\n",
    "    #         axs[0].set_title(\"Predicted Noise\")\n",
    "    #         axs[1].imshow(eps[0][0].cpu().detach().numpy(), cmap=\"gray\")\n",
    "    #         axs[1].set_title(\"True Noise\")\n",
    "    #         axs[2].imshow(np.abs(predicted_eps[0][0].cpu().detach().numpy()-eps[0][0].cpu().detach().numpy()), cmap=\"gray\")\n",
    "    #         axs[2].set_title(\"Difference in noise\")\n",
    "    #         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a72823c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize a random noise tensor\n",
    "xt = torch.randn(1, 3, 32, 32).to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Perform the reverse diffusion process\n",
    "with torch.no_grad():\n",
    "    for t in torch.arange(T-1, -1, -1):\n",
    "        t = t.reshape(1).to(device)\n",
    "        t_norm = t.float() / T\n",
    "        \n",
    "        t_emb = time_embedding_layer(t_norm)\n",
    "\n",
    "        z = torch.randn(1, 3, 32, 32).to(device) if t > 0 else torch.zeros(1, 3, 32, 32).to(device)\n",
    "        \n",
    "        xt_new = 1 / torch.sqrt(alpha[t]) * (xt - (1 - alpha[t]) / torch.sqrt(1 - alpha_bar[t]) * model(xt, t_emb)) + torch.sqrt(beta[t]) * z\n",
    "        xt_new = xt_new.clamp(-1, 1)\n",
    "        xt = xt_new\n",
    "        \n",
    "\n",
    "# im1 = xt[0]\n",
    "# im2 = single_batch[0]\n",
    "# im1 = (im1+1)/2\n",
    "# im1 = im1.permute(1, 2, 0)\n",
    "# im1 = im1.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "# im2 = (im2+1)/2\n",
    "# im2 = im2.permute(1, 2, 0)\n",
    "# im2 = im2.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "# print(im1.min(),im1.max())\n",
    "\n",
    "# fig,axs = plt.subplots(1, 2, tight_layout=True)\n",
    "# axs[0].imshow(im1)\n",
    "# axs[0].set_title(\"Generated Image\")\n",
    "# axs[0].axis(\"off\")\n",
    "# axs[1].imshow(im2)\n",
    "# axs[1].set_title(\"Original Image\")\n",
    "# axs[1].axis(\"off\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# Plot the generated image\n",
    "fig,axs = plt.subplots(1, 2, tight_layout=True)\n",
    "axs[0].imshow(reverse_transform(xt[0].cpu().detach()))\n",
    "axs[0].set_title(\"Generated Image\")\n",
    "axs[0].axis(\"off\")\n",
    "axs[1].imshow(reverse_transform(single_batch[0].cpu().detach()))\n",
    "axs[1].set_title(\"Original Image\")\n",
    "axs[1].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1561ce2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
