{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "def4f85a-37b5-472e-8782-a5df13c5d601",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec1af4-1d61-46ef-90b9-b972129d09a5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eeb867f-c38e-4e5c-ab89-a7e747ead472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import random_split \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017d87cd-410b-44ef-ac34-560022867208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "#torch.backends.cudnn.enabled = False\n",
    "val_size = 5000\n",
    "test_size = 5000\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "pin_memory = False if device == torch.device('cpu') else True\n",
    "\n",
    "# transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "\n",
    "\n",
    "# Normalize input images to [-1, 1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "reverse_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: (x + 1) / 2),  # Reverse the normalization to get values in [0, 1]\n",
    "    transforms.Lambda(lambda x: x.clamp(0, 1)),\n",
    "    transforms.Lambda(lambda x: x.permute(1, 2, 0))\n",
    "])\n",
    "\n",
    "\n",
    "# Downloading MNIST again :) Training (60k) and test(5k) + val(5k) split\n",
    "train_loader = torch.utils.data.DataLoader(datasets.CIFAR10('./CIFAR_data',\n",
    "                                            download=True,\n",
    "                                            train=True,\n",
    "                                            transform=transform),\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=num_workers,\n",
    "                                            pin_memory=pin_memory,\n",
    "                                            drop_last=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10('./CIFAR_data',\n",
    "                               download=True,\n",
    "                               train=False,\n",
    "                               transform=transform)\n",
    "\n",
    "val_dataset, test_dataset = random_split(test_dataset, [val_size, test_size])\n",
    "\n",
    "# Test set to compare with DDPM paper\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=num_workers,\n",
    "                                            pin_memory=pin_memory)\n",
    "\n",
    "# Validation set so we can keep track of approximated FID score while training\n",
    "validation_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=num_workers,\n",
    "                                            pin_memory=pin_memory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ae1fca-cff3-4b53-86d1-65c59740a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COSINE SCHEDULE\n",
    "def f(t, s=torch.tensor([0.008]), T=torch.tensor([1000])):\n",
    "    return (torch.cos((t / T + s) / (1 + s) * (torch.pi / 2)).pow(2)).clamp(max=0.99999)\n",
    "\n",
    "T = 1000\n",
    "ts = torch.arange(T)\n",
    "alpha_bar = torch.tensor([(f(t)/f(torch.tensor([0]))).clamp(max=0.99999) for t in ts]) \n",
    "beta = torch.tensor([1 - alpha_bar[t]/(alpha_bar[t-1]) if t > 0 else torch.tensor([0]) for t in ts])\n",
    "alpha = 1 - beta\n",
    "alpha = alpha.view((T, 1, 1, 1)).to(device)\n",
    "beta = beta.view((T, 1, 1, 1)).to(device)\n",
    "alpha_bar = alpha_bar.view((T, 1, 1, 1)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eca76e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up alpha_bar for training and test so alpha_bar_t = alpha_bar[t]\n",
    "# LINEAR SCHEDULE\n",
    "\"\"\"T = 1000\n",
    "beta_start, beta_end = [1e-4, 2e-02]\n",
    "beta = torch.linspace(beta_start, beta_end, T)\n",
    "alpha = 1-beta\n",
    "alpha_bar = alpha.clone()\n",
    "for e in range(T-1):\n",
    "    alpha_bar[e+1] *= alpha_bar[e]\n",
    "\n",
    "alpha = alpha.view((1000, 1, 1, 1)).to(device)\n",
    "beta = beta.view((1000, 1, 1, 1)).to(device)\n",
    "alpha_bar = alpha_bar.view((1000, 1, 1, 1)).to(device)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42fe879-ee13-4434-9a0d-de015ddd4c2f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4681b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class SinusoidalEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(SinusoidalEmbedding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        half_dim = self.embedding_dim // 2\n",
    "        freqs = torch.exp(\n",
    "            -torch.arange(half_dim, dtype=torch.float32) * math.log(10000) / half_dim\n",
    "        ).to(t.device)\n",
    "        angles = t[:, None] * freqs[None, :]\n",
    "        return torch.cat([angles.sin(), angles.cos()], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8eb1e9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearTimeEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(LinearTimeEmbedding, self).__init__()\n",
    "        self.projection = nn.Linear(1, embedding_dim)  # Project the scalar to the embedding dimension\n",
    "\n",
    "    def forward(self, t):\n",
    "        return self.projection(t.unsqueeze(-1))  # Add an extra dimension for the projection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c671dbab-ffe9-4fe5-841a-80042e7593b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (847968117.py, line 52)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[58], line 52\u001b[1;36m\u001b[0m\n\u001b[1;33m    nn.Linear(self.channels[0],self.channels[1])\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "class UNET(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNET, self).__init__()\n",
    "        self.channels = [3,32, 64, 128, 256, 512]\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(self.channels[0], self.channels[1], kernel_size=3, padding=1),  # (batchsize, 32, 32, 32)\n",
    "                nn.GroupNorm(4, self.channels[1]),\n",
    "                nn.ReLU(),\n",
    "                # nn.Dropout2d(0.1)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2),  # (batchsize, 32, 16, 16)\n",
    "                nn.Conv2d(self.channels[1], self.channels[2], kernel_size=3, padding=1),  # (batchsize, 64, 16, 16)\n",
    "                nn.GroupNorm(4, self.channels[2]),\n",
    "                nn.ReLU(),\n",
    "                # nn.Dropout2d(0.1)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2),  # (batchsize, 64, 7, 7)\n",
    "                nn.Conv2d(self.channels[2], self.channels[3], kernel_size=3, padding=1),  # (batchsize, 128, 8, 8)\n",
    "                nn.GroupNorm(8, self.channels[3]),\n",
    "                nn.ReLU(),\n",
    "                # nn.Dropout2d(0.1)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2),  # (batchsize, 128, 4, 4)\n",
    "                nn.Conv2d(self.channels[3], self.channels[4], kernel_size=3, padding=1),  # (batchsize, 256, 4, 4)\n",
    "                nn.GroupNorm(8, self.channels[4]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2),  # (batchsize, 256, 2, 2)\n",
    "                nn.Conv2d(self.channels[4], self.channels[5], kernel_size=3, padding=1),  # (batchsize, 512, 2, 2)\n",
    "                nn.GroupNorm(8, self.channels[5]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2)\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        self.tconvs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(self.channels[5], self.channels[4], kernel_size=3,\n",
    "                                      stride=2, padding=1, output_padding=1),  # (batchsize, 256, 4, 4)\n",
    "                nn.GroupNorm(8, self.channels[4]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(self.channels[4]*2, self.channels[3], kernel_size=3, \n",
    "                                   stride=2, padding=1, output_padding=1),   # (batchsize, 128, 8, 8)\n",
    "                nn.GroupNorm(8, self.channels[3]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(self.channels[3]*2, self.channels[2], kernel_size=3,\n",
    "                                   stride=2, padding=1, output_padding=1),   # (batchsize, 64, 16, 16)\n",
    "                nn.GroupNorm(8, self.channels[2]),\n",
    "                nn.ReLU(),\n",
    "                # nn.Dropout2d(0.1)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(self.channels[2]*2, self.channels[1], kernel_size=3, \n",
    "                                   stride=2, padding=1, output_padding=1),   # (batchsize, 32, 32, 32)\n",
    "                nn.GroupNorm(4, self.channels[1]),\n",
    "                nn.ReLU(),\n",
    "                # nn.Dropout2d(0.1)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(self.channels[1]*2, self.channels[1], kernel_size=3, padding=1),  # (batchsize, 32, 32, 32)\n",
    "                nn.GroupNorm(4, self.channels[1]),\n",
    "                nn.ReLU(),\n",
    "                # nn.Dropout2d(0.1),\n",
    "                nn.Conv2d(self.channels[1], self.channels[0], kernel_size=1)  # (batchsize, 3, 32, 32)\n",
    "            )      \n",
    "        ])\n",
    "        \n",
    "        self.time_layers = nn.ModuleList([\n",
    "                nn.Linear(128, self.channels[i]) for i in range(len(self.channels))\n",
    "        ])\n",
    "\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        signal = x\n",
    "        signals = []\n",
    "\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            # print(f\"signal shape: {signal.shape}\")\n",
    "            t_emb_processed = self.time_layers[i](t_emb).view(-1, self.channels[i], 1, 1)\n",
    "            signal= t_emb_processed+signal\n",
    "            signal = conv(signal)\n",
    "            if i < len(self.convs)-1:\n",
    "                signals.append(signal)\n",
    "        \n",
    "        for i, tconv in enumerate(self.tconvs):\n",
    "            # print(f\"signal shape: {signal.shape}\")\n",
    "            # print(f\"signals[-{i}] shape: {signals[-i].shape}\")\n",
    "            # print()\n",
    "            t_emb_processed = self.time_layers[-(i + 1)](t_emb).view(-1, self.channels[-(i + 1)], 1, 1)\n",
    "            signal= signal+t_emb_processed\n",
    "            if i == 0:\n",
    "                signal = tconv(signal)\n",
    "            else:\n",
    "                signal = torch.cat((signal, signals[-i]), dim=-3)\n",
    "                signal = tconv(signal)\n",
    "\n",
    "        return signal\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f04a5df-da69-407e-9285-90de11992cef",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8403ab0-aea6-492a-98b9-31ab55064c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from UNET import UNET\n",
    "epochs = 200\n",
    "model = UNET()\n",
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.MSELoss(reduction=\"sum\")\n",
    "running_loss = 0\n",
    "\n",
    "# Initialize the linear time embedding layer\n",
    "time_embedding_dim = 128  # You can set this to any suitable size\n",
    "time_embedding_layer = SinusoidalEmbedding(time_embedding_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc7e14a-63cd-4432-8f07-43fd00841fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([655, 359, 730, 135, 136, 914, 998, 949, 901, 588,  75, 244, 236, 268,\n",
      "        630, 702, 519, 847, 639, 557, 431, 937, 642, 672, 692, 660, 185, 451,\n",
      "        612, 305, 167, 214, 273, 628, 835, 852, 712, 812, 377, 127, 783, 600,\n",
      "        265, 968, 789, 775, 422, 812, 235, 397, 556, 792, 411, 162, 949, 103,\n",
      "        782, 868, 792, 909,  93,  66, 575, 557, 132,  77, 136, 325, 511, 488,\n",
      "        475, 912, 873, 379, 200,   2, 175, 286, 176, 861, 784, 660, 194, 727,\n",
      "        707, 458, 985, 439, 136, 526, 800, 977,  10,  13, 431, 812, 241, 928,\n",
      "        443, 878, 726, 162, 286, 860, 885, 977, 786, 678, 593,  15,  10, 790,\n",
      "        133, 842, 629,  80, 935, 562, 239, 379, 262, 732, 214, 274, 809, 902,\n",
      "         10, 336])\n",
      "tensor([136, 490, 485, 497, 646, 574, 140, 131, 606, 255,  35, 337, 333, 806,\n",
      "        803, 851, 544, 635, 862, 474, 845, 838, 222, 294, 197, 122, 663, 433,\n",
      "        276, 317, 381, 355, 759, 376, 153,   9, 267, 295,   0, 527, 350, 726,\n",
      "        935, 555, 788, 580, 829, 840, 727, 125, 488, 974, 194, 155, 237, 791,\n",
      "        233, 280, 197, 117, 698, 588, 985, 885, 837, 724, 418, 767,  25, 648,\n",
      "         89, 679, 237, 323, 381, 571, 287, 218, 178, 269, 764, 443, 223, 362,\n",
      "        518, 738, 674, 167, 928, 335, 746, 864, 813, 846, 998, 589, 885, 752,\n",
      "        557, 301, 415, 943, 550, 198, 909, 192, 968, 537, 942, 444, 127, 470,\n",
      "        711, 920, 736, 230, 157, 428, 524,  17, 759, 330, 194, 885, 798, 741,\n",
      "        585, 155])\n",
      "tensor([642, 682, 359, 587, 798,  42, 502, 399, 731, 801, 186, 242, 480,  89,\n",
      "        623, 279, 651,  44, 879, 209, 399, 545, 834, 755, 592, 992, 122, 537,\n",
      "        360, 479, 888, 754, 195, 976,  45, 319, 906, 720, 807, 882, 865, 669,\n",
      "         78, 883, 467, 267, 476, 192, 482, 712, 654,  53, 928, 386, 303, 796,\n",
      "        784, 117, 349, 949, 740, 316, 605,  84, 794, 222,  98,  21, 792, 182,\n",
      "        785, 122,  39, 399, 286, 546, 884, 727, 209,   0, 380, 960, 946, 590,\n",
      "        259, 482, 285, 275, 785, 962,  96, 493,  47, 741,  92, 830, 385, 219,\n",
      "        866, 985, 653, 918, 290, 488, 779, 579, 612,  48, 803, 502, 223, 551,\n",
      "        129, 345, 678, 704, 153, 419, 560,  71, 698, 600, 912, 988, 678, 534,\n",
      "        934, 799])\n",
      "tensor([273, 998, 154,  44, 409, 660, 740,  58, 903,  86, 668,  11, 968, 372,\n",
      "        400, 356, 711, 777, 250, 495, 386, 833, 342, 238, 255, 352, 836, 298,\n",
      "        998,  99, 283, 759, 920, 355, 336, 758, 328, 505, 665, 487, 845, 571,\n",
      "        903, 417, 286, 152, 361, 513, 215,  95, 365, 259,  64, 301, 780,   4,\n",
      "         79, 741, 470, 407, 325, 158, 459, 514, 869, 207, 583, 563, 927, 953,\n",
      "        111, 317, 790, 268, 239, 797, 137, 625, 488, 924, 205, 307, 507, 859,\n",
      "        863, 852, 529, 750, 271, 970, 196,   7, 443,   6, 710, 455, 398, 585,\n",
      "        915, 609,  88, 776, 370, 919,  77, 148, 245, 241, 303, 758, 782,   0,\n",
      "        654,  68, 840,  66, 999,  49, 201, 384, 132, 982, 258, 664, 717, 463,\n",
      "         19, 934])\n",
      "tensor([921, 230,   2, 327,  14, 791, 777, 784, 852, 407, 963, 884, 401, 993,\n",
      "        746, 915, 111, 329, 897, 954, 249,  73, 615, 606, 871, 677, 474, 850,\n",
      "        529, 529,  90, 672, 900, 129, 815, 885, 403, 986, 856,  34, 410, 874,\n",
      "        930, 541, 220, 405, 659, 971, 794, 946,  20, 164, 122,  39, 233,  13,\n",
      "        965, 259, 219, 473, 998,   0, 604,  51, 331, 762, 908, 177, 916, 224,\n",
      "        828,  77,  96, 343, 534,  44, 486, 690, 891,  85, 345, 809, 668,  64,\n",
      "        145, 841, 472,  63, 987, 272, 309, 830, 386, 535, 582,  82, 234, 531,\n",
      "        820, 956,  62,   6, 834, 723, 842, 463, 579, 116, 923, 337, 248, 318,\n",
      "        729, 193,  78, 515,  51, 413, 793, 133, 164, 394, 318, 368, 833, 249,\n",
      "        812, 594])\n",
      "tensor([574, 117, 103, 664,  29, 405, 261, 577,  67, 470, 149, 975, 848, 678,\n",
      "        144, 804, 517, 498,  73, 369, 387, 868, 460, 770, 585, 848, 378, 670,\n",
      "        864, 664, 410, 249, 982, 331, 952,  52, 831, 743, 536, 956, 113, 404,\n",
      "        175, 825, 748,  72, 200, 611, 317, 717, 159,  54, 865, 706, 196, 217,\n",
      "        271,  49, 745, 611, 783, 450, 184, 701, 585, 565, 468, 752, 995, 601,\n",
      "        188, 499, 797, 648, 518, 348, 181, 220,  28, 370, 535, 247,  90, 437,\n",
      "          0, 572, 821, 121, 984,  20,  30, 313, 395, 336, 771, 544,  43, 646,\n",
      "        486, 929, 280, 348, 130, 767, 234, 705, 535, 184, 228, 771, 169, 888,\n",
      "        829, 328, 451,  46, 191, 315, 512, 911, 822, 423, 485,  45, 394, 924,\n",
      "        559, 111])\n",
      "tensor([ 33, 268,  89, 661, 935, 852, 371, 523, 633, 552, 979,  12, 216, 187,\n",
      "        484, 270, 371, 416, 329, 114, 198, 266, 441, 390, 665, 348, 511, 882,\n",
      "        215, 369, 932,  65, 895, 534, 401,  17, 852, 782, 807, 696, 330,  16,\n",
      "        749, 163, 750, 997, 274,  36, 783, 815, 457, 203, 356, 575, 571, 707,\n",
      "         30, 321, 680, 239, 524,  87, 339,   4, 239, 621, 358, 489,  68, 573,\n",
      "        922, 912, 593, 346, 713, 848, 781, 563, 340, 140, 410, 177, 842, 888,\n",
      "        746, 704, 362, 996, 176, 305, 686, 224, 924, 612, 134, 145, 451, 373,\n",
      "        763,  21, 985, 578, 621, 134, 911, 595, 827, 924, 726, 807, 224,  65,\n",
      "        800, 671, 332, 351, 100,  14, 572, 534, 366, 724,  26, 354, 300, 507,\n",
      "        174,  75])\n",
      "tensor([551, 695, 670, 645, 916, 897, 525, 995, 944, 865, 382, 535, 415,  32,\n",
      "        982, 684, 581, 866,  37, 142, 434, 302, 983, 142, 804, 308, 817, 408,\n",
      "        853, 785, 439, 188, 807, 593, 672, 604, 852, 648, 431, 878, 994, 398,\n",
      "        563, 471,  63, 470, 684, 520,  29, 244, 789, 777, 431, 414, 441, 465,\n",
      "        681, 125, 536, 672, 574, 156, 828, 866, 452, 149, 805, 530, 307, 332,\n",
      "        700, 948, 982, 513, 415, 937,  69, 824, 505,  44, 537,  55, 509, 656,\n",
      "         50, 465, 347,  86,  38, 762, 821, 108, 593, 417, 571, 915, 657, 406,\n",
      "         93, 410, 671, 391, 715, 995, 541, 285, 859, 676, 754, 843, 354, 924,\n",
      "        623, 550, 489, 202, 921, 637, 656, 518, 965, 250, 475, 545, 499, 388,\n",
      "        961, 101])\n",
      "tensor([983, 863, 620, 896, 570, 691, 906, 416, 390, 371, 302, 798, 618, 579,\n",
      "        708, 509, 632, 535, 966, 688, 775, 274, 996, 247, 356,  49, 983, 990,\n",
      "        640, 979,  55, 988, 618, 393, 402, 296, 439, 583, 810, 302, 323, 722,\n",
      "        132, 685, 721, 187, 299, 809, 508, 324, 298, 438, 912, 713, 706, 444,\n",
      "        628, 654,  62, 951, 739, 460, 971,  57,  60, 245, 974, 852, 939, 101,\n",
      "        628, 975, 756, 128, 940, 752, 675, 991, 699, 390,   8, 176, 834, 241,\n",
      "        579, 143, 295,  61, 352, 860, 171, 656, 294, 920, 507, 916, 391, 996,\n",
      "        692, 521, 806, 197, 129,  24, 567, 958, 802,  55, 970, 414, 361, 137,\n",
      "        831, 944, 253, 329, 852, 118,  62,  64, 600, 825, 986,  17,  39, 123,\n",
      "        576, 244])\n",
      "tensor([ 12, 865, 706, 849,  19, 668, 927, 989, 981, 544, 611, 422, 347, 271,\n",
      "        875, 472, 490, 239, 940, 399, 142, 210, 298,  21, 741,  81, 878,  73,\n",
      "        184, 698, 413, 763, 293, 202, 606, 726,  59, 252, 555, 440, 912, 579,\n",
      "        295, 368, 872, 265, 950, 985, 397, 792, 379, 251, 248, 920, 517, 518,\n",
      "         91, 406, 163, 816, 991, 259, 663, 215, 617, 214, 583, 699, 211, 600,\n",
      "        926, 409, 627, 101, 304, 578, 986, 312, 619, 986, 787, 986, 670, 685,\n",
      "        608, 762, 591, 384, 347, 751, 880,  64, 325, 912, 457, 730, 762, 264,\n",
      "        544, 221, 727, 741, 466, 876, 948, 248, 306, 182, 801, 412, 794, 232,\n",
      "        209, 715, 706,  33, 977, 574, 417, 139, 376, 633, 913, 602, 683, 544,\n",
      "         61, 694])\n",
      "tensor([ 74,  19, 713, 952, 336, 790, 421, 578, 165, 119, 336, 819, 387, 628,\n",
      "        340,  43, 982, 100, 826, 332, 881, 745,  70, 672, 343, 518, 921, 449,\n",
      "        695, 365, 781, 410, 178, 921, 388, 807, 703, 225, 702,  72, 409, 748,\n",
      "          1,  69,  56, 187, 993, 837, 696, 476, 582, 344, 489, 198, 773, 963,\n",
      "        109, 704, 346, 519,  66, 284, 976, 340, 854, 240, 313, 921, 501, 238,\n",
      "        987, 631, 150, 947, 374, 392, 291, 901, 486, 903, 680, 205,  18, 345,\n",
      "        851, 881, 667, 531, 126, 565, 567, 625, 991, 374, 447, 860, 146,   2,\n",
      "        814, 501, 683, 304, 476, 161, 374, 349, 220, 504, 392, 558,  18, 884,\n",
      "        717, 399, 708, 620, 832, 675, 713, 216, 399, 644, 880, 603, 732, 169,\n",
      "        805, 100])\n",
      "tensor([842, 957, 304, 725, 950, 583, 552, 741,  25, 112, 951, 491, 271, 683,\n",
      "        447, 995, 806, 525, 170, 358, 290, 956, 354, 789, 520, 183, 885, 515,\n",
      "        504, 896, 978, 317, 112, 543, 619, 697, 716, 538, 573, 114, 367, 481,\n",
      "        133, 798, 881,  24, 947, 124, 362, 159, 778, 200, 263, 138, 985, 412,\n",
      "        890, 456, 207, 807, 634, 511, 956, 223, 691, 594, 592, 503, 298, 567,\n",
      "        843,  29, 791, 963, 982, 123, 344, 509, 414, 962, 600, 808,  67, 456,\n",
      "        195, 858,   8,  38, 499, 665, 815,  61, 636, 663, 845, 683, 887, 864,\n",
      "        808, 874, 614, 135, 355, 498,   2,  72, 146, 921, 569, 647, 982, 862,\n",
      "        933, 496, 517, 757, 490, 216, 946, 563, 361, 808, 987, 661, 346, 161,\n",
      "        640, 520])\n",
      "tensor([732, 297, 994, 795, 281, 970, 401, 939, 590, 781, 957, 657, 854, 415,\n",
      "        902, 946, 379, 599, 490, 913, 866, 685, 681, 251, 145, 230, 724, 631,\n",
      "        328, 930, 672, 180, 791, 869,  67, 309, 524, 769, 448,  34, 711, 249,\n",
      "        658, 494, 366, 708, 670, 521, 337, 494, 510, 581,  41, 256, 516, 979,\n",
      "        480, 737,  46, 316, 950, 899, 923, 202, 579, 943,  24, 494, 511, 852,\n",
      "        633, 458,  60, 677, 864,  98, 389, 675, 306,  34, 430, 167, 441, 171,\n",
      "        390, 517, 780, 945, 131,  13, 587, 866, 933, 859, 216, 451, 314, 518,\n",
      "        261, 417, 420, 390, 610, 846, 105, 474, 131, 519, 659, 410, 184, 120,\n",
      "        127, 101, 217, 128, 956, 422, 855, 906, 105,  76, 740, 221, 802,  84,\n",
      "         63, 964])\n",
      "tensor([582, 586,   6, 972, 782, 802, 895, 109, 842, 299,  18, 658, 169, 458,\n",
      "        268,  52, 532,  99, 413, 822, 161, 268, 966, 879,  18, 369,  36, 949,\n",
      "        320, 342, 568, 639, 276, 983, 625, 136, 556, 513, 583, 897, 721,  23,\n",
      "        126,  30, 301, 593, 646, 445, 782, 210, 915, 599, 929, 206, 270, 646,\n",
      "        634, 616, 297, 824, 787, 975,  33, 924, 374, 807, 839, 841, 355, 761,\n",
      "        618, 872, 365, 685, 676, 101, 805, 546, 480, 171, 167, 271, 240, 693,\n",
      "        319, 984,  48, 766, 334, 146, 177, 590, 413, 316, 830, 153, 975, 624,\n",
      "        299, 214, 448, 772, 166, 238, 829, 151, 690, 489, 147, 245, 543, 303,\n",
      "        719, 436, 639, 477, 752, 707, 610, 612, 514, 106, 224, 498, 835,   1,\n",
      "         55, 881])\n",
      "tensor([226, 955, 456, 567, 947, 616, 669, 134, 128,  38,  55, 718, 658, 547,\n",
      "        372, 976, 558, 749, 599, 139,  82, 431, 187, 621, 125, 193,  10, 303,\n",
      "        777, 830,  45, 694, 772, 807, 207, 112, 663, 872, 693, 850, 858,   2,\n",
      "        654, 541, 782, 190, 101, 953, 875, 852, 529, 808, 525, 761,   5, 774,\n",
      "          1,  66, 227, 537, 470, 303, 621, 473, 485, 211, 140, 922, 644, 645,\n",
      "         93, 946, 970, 370, 985, 581, 171, 787, 325, 537, 113, 504, 411, 618,\n",
      "        486, 408,  32, 537,  31, 849, 438, 757, 417, 932,  20, 268,  52, 981,\n",
      "        766, 728, 285, 857, 458, 471, 209, 143, 419, 222, 421, 285, 557,  85,\n",
      "        225, 994, 588, 838, 864, 293, 681, 140, 659, 779, 564,  60, 479, 812,\n",
      "        379, 276])\n",
      "tensor([329, 783, 591, 424, 617, 431,   1,  62, 802, 443, 751,  47, 880, 313,\n",
      "        913, 588, 862, 301,   0, 430, 428, 455, 499, 138, 965, 579, 533, 667,\n",
      "        938, 783, 201, 964,  67, 126, 228, 156, 999, 913, 258, 881, 748, 434,\n",
      "        826, 309, 807, 502, 785, 444, 105,  40,  30, 170, 859, 856,  64, 790,\n",
      "        901,  86, 695, 923, 215, 929, 946,  73, 493, 180, 639, 524, 834, 447,\n",
      "        308, 367, 374, 235, 986, 151, 821, 285, 593,  73, 276, 788, 167, 193,\n",
      "        279, 719, 301, 203, 559, 284, 313, 328, 752, 577, 677, 830, 813, 142,\n",
      "        559, 118, 577, 189, 330, 919,  46,  96, 281, 578, 762,  75, 598, 638,\n",
      "        469,   8, 971, 728, 769,  29, 716, 155, 387, 436, 978, 313, 662, 315,\n",
      "        187, 993])\n",
      "tensor([891, 591, 283,  64,  42, 765, 437,  12, 827, 186,  30, 885, 304,  66,\n",
      "        725, 789, 237, 892, 495, 825, 242, 824, 636, 899, 296, 411, 519, 372,\n",
      "        120, 847, 337,  61, 508, 258, 572, 398, 631, 617, 676, 505, 669, 663,\n",
      "        531, 923, 422, 724, 886, 247, 953, 560,  79,  38,  47, 208, 944, 315,\n",
      "        779, 786, 763,  57, 714, 467, 759, 256, 723, 970, 806, 361, 166, 228,\n",
      "        914, 571, 657, 674, 982, 643, 266, 223, 661, 580,  11, 231, 712, 490,\n",
      "        564, 397, 182, 542, 748, 773, 279, 240, 896, 883, 375, 657, 822, 305,\n",
      "        209, 250, 767, 395, 477, 169, 374, 586, 454, 119, 802, 976, 901, 906,\n",
      "        540, 138, 593, 424, 612, 942, 501, 100, 323, 381, 843, 614,  96, 173,\n",
      "        454, 663])\n",
      "tensor([233, 714, 639, 301, 792, 472,  28, 760, 575, 475, 720, 452, 666, 878,\n",
      "        278, 394, 937, 298, 510, 499, 926, 852, 719, 333, 284, 850, 213, 885,\n",
      "         35, 949, 882, 832, 745, 679,  56, 752, 510,  42, 295, 565, 621, 282,\n",
      "        513, 429, 183, 787, 137,  48, 205, 934, 836, 722, 249, 912, 530, 786,\n",
      "        429, 236, 345,  55, 689, 550, 340, 912, 499, 707, 918,  51, 544, 168,\n",
      "        645, 352, 143,  74, 286, 903, 619, 202, 967, 850, 302, 204, 922, 814,\n",
      "        749, 526, 110, 410, 393, 933, 491,   3, 493, 528, 531,  38, 842,  85,\n",
      "        877, 287, 918,  24,  43, 739, 575, 413, 707, 697, 770, 903, 915, 471,\n",
      "        668, 466, 125, 605, 959, 868, 281, 881, 628, 266, 100, 946, 326,  85,\n",
      "         89, 366])\n",
      "tensor([375, 135, 724, 200, 806, 458, 105, 962, 324, 806,  52, 918, 941, 249,\n",
      "        752, 640, 241, 992,  17, 319, 877, 163, 784, 999, 591, 739, 822, 572,\n",
      "         71,  12, 949, 460, 390, 628, 712, 741, 877, 374, 734, 395, 691, 900,\n",
      "        954, 180, 998, 609, 406, 166, 930, 279, 122, 753, 679,  33, 517, 991,\n",
      "        424, 577,  87, 939, 256,  22, 311, 142, 941, 676, 266, 770, 562, 229,\n",
      "        775, 518, 123, 957,  50, 879,  92, 788, 412, 268, 246, 791,  42, 675,\n",
      "        325, 523, 923, 403, 552, 495, 282, 132, 341, 564, 983, 598, 306,  16,\n",
      "        884, 362, 607, 942, 223, 379, 793, 812, 890, 943, 869, 581, 616, 290,\n",
      "          0, 288, 674, 259,  59, 812,   0, 463, 246, 674, 976, 834, 723, 761,\n",
      "        446, 702])\n",
      "tensor([332, 426, 767,  18, 299, 516, 563, 181,  61, 423, 290, 535, 599, 482,\n",
      "        508, 171, 899, 249,  16, 777, 230, 239, 678, 355, 300, 826, 945, 446,\n",
      "        448, 934, 108, 469, 607, 384, 960, 533, 974, 692, 282, 392, 907, 465,\n",
      "        177, 290, 620, 679, 464, 538, 203, 625, 213,  91, 773,  90, 529, 433,\n",
      "        752, 825, 178, 371,  53, 846, 318, 116, 833, 992, 176, 217, 723, 518,\n",
      "        947, 247, 550, 463, 367,  18, 903, 196, 673, 786, 320, 791, 377, 977,\n",
      "        236, 508, 972, 681, 537, 695, 747, 284, 158, 990, 145, 475, 284, 174,\n",
      "        301, 211, 381, 720, 377, 154, 945, 854, 233, 393, 461, 694, 305, 146,\n",
      "        407,  50, 725, 416, 774, 338, 730, 980, 795, 285, 196, 509, 939,   5,\n",
      "        604, 205])\n",
      "tensor([747, 561, 321, 610, 838, 782, 401, 656, 418, 764, 310, 934, 117, 792,\n",
      "        395, 602,  97, 686, 827, 245, 628, 495, 124, 664, 878, 976, 758, 936,\n",
      "        546, 160, 562, 160, 897, 360,  31, 598, 559, 182, 506,  85, 805, 743,\n",
      "        496,  53, 965, 639, 861, 779, 893, 472, 266, 873, 907, 125,  37, 709,\n",
      "        992, 461, 352, 413, 820, 812, 482, 992, 880, 113, 141, 890, 434, 829,\n",
      "        466, 526, 641, 611, 717, 332, 915,  45, 346, 227,  19, 279, 410, 181,\n",
      "         59, 831, 925, 684, 934, 752, 462,  22, 692, 977, 310, 950, 466, 959,\n",
      "        695,  56, 574, 833, 672,  44, 788, 971, 503, 392, 172, 346, 384, 286,\n",
      "        565, 791, 716, 883, 196, 866, 511,   9,  72, 397, 342, 491, 391,  55,\n",
      "        954, 550])\n",
      "tensor([658, 711, 276, 825, 564, 181,  45, 429, 257, 970, 992, 517, 704, 844,\n",
      "        630, 810, 337, 676,  74, 553,  93, 557, 262, 759, 189, 386, 987, 401,\n",
      "        709, 218, 943, 835, 359, 373, 170, 922,  92, 181, 281, 156, 842, 759,\n",
      "        347, 795, 203, 596,  60, 328, 883,  58, 320, 514, 197, 971, 569, 808,\n",
      "        979, 642, 387, 216, 752, 654, 945, 870, 452, 214,  57,  32, 824, 508,\n",
      "        249, 544, 640, 558, 453, 498, 920, 297, 389, 234, 973, 160,   7, 854,\n",
      "         97, 281, 895, 701, 453, 453, 629, 660, 211, 633, 967, 813, 183,  39,\n",
      "        864, 113, 404, 774, 216, 636, 905, 223, 484, 971, 654, 722, 340, 386,\n",
      "        803, 822, 117, 810, 365,  94, 380,  15,  21, 221, 183, 264, 160, 237,\n",
      "        330, 690])\n",
      "tensor([572, 949, 887, 910, 300, 878, 459, 179, 139, 181, 744,  99, 833, 106,\n",
      "        109, 672, 819, 196, 130,  20, 875, 436, 764, 498, 806, 275,  52, 782,\n",
      "        870, 452, 402, 330, 624, 918,  77, 476, 600,  29, 373, 130, 400, 286,\n",
      "        331, 430, 511, 491, 756, 471, 808, 899, 471, 335, 639, 834,  93, 177,\n",
      "        357, 840, 910, 422, 753, 162, 261, 482, 543, 283, 201, 185, 438,  48,\n",
      "        748, 839, 171, 590, 244, 261, 995, 188, 648, 467, 988, 649,  10, 272,\n",
      "        486, 372, 483, 900, 999, 490, 198, 349, 478, 628, 301, 111, 122,  83,\n",
      "        167,  75, 485, 131, 161, 319, 414, 137, 793, 576, 633,   0, 973, 885,\n",
      "        256,  17, 549, 641,  21, 820, 793, 166, 374, 879, 710, 910, 875, 421,\n",
      "        289, 854])\n",
      "tensor([ 36, 793, 356, 828, 864, 410, 174, 118, 543, 613, 604,  69, 239, 343,\n",
      "        741,   7, 382, 398, 436, 311, 192, 312,  25, 101, 887, 420, 723,  24,\n",
      "        148, 987,  30, 734, 547, 611, 585,  47, 810,  40, 311, 425, 349, 863,\n",
      "        445, 847, 695, 824,  30, 906, 561, 591, 434, 343, 804, 657,  52, 843,\n",
      "        513, 690, 375, 315, 736,  71, 598, 573,  66, 904, 153, 738, 178, 962,\n",
      "        265, 541, 733, 159, 490,   6, 918, 196,  89, 845, 727,  98, 118, 400,\n",
      "        620, 208, 867, 378, 800, 933,  81, 752, 249, 714, 736, 794, 708, 129,\n",
      "        840, 533, 814, 487, 252, 264, 617, 837, 623, 730, 124, 655, 349, 895,\n",
      "         82, 223, 823, 863,  15, 691, 313, 848, 161, 983, 342, 940, 209, 824,\n",
      "        828, 969])\n",
      "tensor([540, 202, 378, 536, 772, 356, 245, 738, 996, 711, 996, 212, 886, 329,\n",
      "        394, 562, 691,  76, 518, 479, 145, 170, 967, 444, 689, 100, 807, 110,\n",
      "        720, 746, 733, 444, 266, 468, 781, 114, 952, 657, 469, 327, 648,  30,\n",
      "        890, 249, 310, 730, 555,   0, 401,  83, 996, 258, 591, 179, 314, 653,\n",
      "        928, 821,  12, 900, 481, 206, 916, 575, 206,  26, 832, 493, 252, 837,\n",
      "        754, 419, 547, 857, 229, 321, 402, 639, 562, 534, 850, 244, 797, 137,\n",
      "         68, 244, 447, 257, 147, 822, 488,  21, 506, 388,  95, 235, 835, 153,\n",
      "        980, 636, 506, 847, 384,  59, 834, 821, 242,  69, 863, 751, 552, 113,\n",
      "        328, 457, 502, 345, 333, 256, 408, 611, 985, 907, 142, 120, 733, 560,\n",
      "        461, 976])\n",
      "tensor([606, 429, 633, 521, 201, 582, 865, 438, 801, 572, 520,   6, 724, 530,\n",
      "         74, 239,   3, 348, 233, 213, 445, 593, 274, 761, 906, 946, 237, 309,\n",
      "        951, 753, 453, 891, 295, 827, 795, 149, 270, 463, 816, 578, 383, 585,\n",
      "        993,  28, 827, 709, 428, 874, 191, 705, 389, 434, 407, 646, 230,  12,\n",
      "        880, 436, 797,  44, 499, 868, 641, 476,   7, 134, 285,  16, 153,  55,\n",
      "        491, 607,  47, 341, 566, 595, 286, 631, 531, 159, 820, 338, 864, 107,\n",
      "        407, 579, 165, 843, 190, 831, 192, 719, 419, 390, 802, 639, 938, 994,\n",
      "        207, 262, 101,  80, 477, 943, 314, 471, 572, 445, 372, 298, 795, 612,\n",
      "        435, 190, 859, 641, 692, 707, 485, 348, 911, 732, 117, 186, 177, 206,\n",
      "        555, 539])\n",
      "tensor([635, 216, 731, 999, 739, 783, 270, 626, 320, 646, 729, 322, 390, 335,\n",
      "        843, 671, 334, 891, 315, 709, 671, 490,  83, 159, 401, 438, 188, 692,\n",
      "        653,  67, 952, 138, 940, 921, 788, 918, 989, 487, 263,  68, 947, 171,\n",
      "        498, 805,  56, 323, 514, 692, 976, 569, 360, 238, 707, 687, 780, 177,\n",
      "        286, 764, 891, 901, 484, 371, 408, 550, 663, 883, 728,   0, 733, 218,\n",
      "        989, 630, 848, 724,  85, 675, 721, 171, 399, 881, 590, 328, 231, 795,\n",
      "        909, 639, 158, 668, 383, 738, 277,  82, 613, 797, 743,  95, 615, 758,\n",
      "        551, 781, 527, 427, 915, 888, 912, 220, 590, 573, 477, 768, 734, 311,\n",
      "        348, 793, 350, 192, 804, 832, 469, 170, 448,  93, 317, 748, 163, 663,\n",
      "        126, 674])\n",
      "tensor([342, 722, 559, 285, 884, 959, 623, 428, 657, 763, 706, 503, 302, 992,\n",
      "        550, 469, 395, 463, 654,  49, 300, 902, 256, 969, 850,  60, 588, 406,\n",
      "        565, 689, 157, 770,  97, 188, 537, 574, 721, 907, 715, 563, 227, 620,\n",
      "        954, 832, 667, 171, 654, 319, 695, 571, 336, 781,  44,  76, 810, 224,\n",
      "        145, 313, 379, 309, 759, 680, 144, 184, 943, 770, 113, 135, 566, 996,\n",
      "        371, 402, 910, 736, 487,  38, 275, 393, 881, 218,  72, 837, 276, 176,\n",
      "        210, 267, 807, 552, 782, 351, 938,  85, 102, 758, 189, 424, 557,  98,\n",
      "        630, 704, 789, 951,  21, 810, 832, 538, 765, 539, 919, 391, 321, 134,\n",
      "        245, 517, 908, 347, 207, 780, 676, 709, 164, 318, 630, 682, 228, 591,\n",
      "        206, 111])\n",
      "tensor([111, 421, 547, 173, 714, 833, 863, 672, 169, 432, 536,  67, 748, 127,\n",
      "        258, 340, 262, 340, 234, 893,  48, 756, 228, 650, 261, 491, 673, 739,\n",
      "          0, 815, 428, 176, 387,  71,  43, 647, 973, 692, 826, 239,  55, 803,\n",
      "        283, 600, 745, 564, 100, 256, 399, 146, 242, 426, 215, 806, 369, 893,\n",
      "        202, 244, 235,  82, 709, 237, 697, 748, 599, 717, 126, 290, 322, 203,\n",
      "        338, 856, 899, 725, 579, 387, 320, 900, 804, 893, 891, 336,  63, 518,\n",
      "        302, 590, 192,  68, 539, 131, 437, 182, 229, 452, 636, 841, 152, 198,\n",
      "        189,  53, 692, 417, 962,  26, 989, 534,  13, 434, 373, 920,  33, 578,\n",
      "        714,  75, 884, 408, 743, 525,  80, 397, 809, 792, 525, 827, 289, 366,\n",
      "        293, 256])\n",
      "tensor([ 49, 483, 902, 337, 134, 639, 956,  53, 172, 907, 423, 352, 101, 816,\n",
      "        860, 894, 540, 317, 941,  85, 588, 704, 244, 181,   2, 471, 177, 413,\n",
      "        446, 945, 602, 789, 746, 840, 571, 581,  13, 374, 249, 815, 673, 176,\n",
      "        625, 728, 116, 201, 558, 260, 750, 328, 487, 167, 642, 622, 849, 400,\n",
      "        782, 558, 602, 652, 710, 276, 221, 632, 734, 687, 617, 653, 682, 656,\n",
      "        830, 291, 469, 956, 404, 672, 839, 597, 639, 132,  89, 364, 705, 207,\n",
      "        793, 465, 104, 608, 502, 738, 410, 230, 288, 241, 524, 785, 204, 363,\n",
      "        921, 949, 171, 616, 282, 944, 184, 434, 440, 165, 802, 490, 848, 772,\n",
      "        856, 892, 647,  97, 450, 519, 108, 729, 169, 103, 957, 408, 708, 230,\n",
      "        831, 203])\n",
      "tensor([220, 455, 986, 688, 917, 410, 806, 105, 944, 591,  55, 669, 804, 659,\n",
      "         26, 496, 250, 384, 529, 404, 915, 150, 653,  31, 953, 842, 591, 276,\n",
      "         87,   7, 980,  22, 593, 539, 407, 680, 216, 580, 702, 336, 783, 730,\n",
      "        311,  58, 199, 971, 651, 654, 432,  94, 883, 173,  90, 809, 635, 150,\n",
      "        917, 121, 114, 354, 954, 939, 549,  94, 872,  24, 634, 521, 636, 559,\n",
      "        540, 291, 451, 374, 960, 130, 712, 277, 198, 404,  87, 305,  84, 948,\n",
      "        577, 510, 417,  35, 190, 477, 535, 519, 728, 147, 891, 645, 947, 657,\n",
      "         72, 524, 509, 508, 735, 215, 804,  54,  96, 285, 679, 411, 536, 748,\n",
      "        297, 804, 337, 354, 865, 184, 739, 388, 766,  21, 714, 108, 431, 860,\n",
      "        599, 815])\n",
      "tensor([ 51, 660, 451, 322, 794, 767, 946, 501, 747, 127, 254, 948, 939, 188,\n",
      "          4, 907, 818, 614, 921, 159, 615, 417, 776, 832, 817, 264, 181, 973,\n",
      "        198, 384, 492, 281, 306,  61, 737, 266, 309, 869, 224, 734, 799, 113,\n",
      "        397,  42, 428, 123, 434, 888, 505, 208, 880, 486, 472, 882, 619, 246,\n",
      "        756, 682, 983, 776, 628, 532,  59, 316, 404,  20, 143, 302, 893, 920,\n",
      "        231,  30, 375, 799, 140, 466, 835,  43, 716, 439, 212, 254, 717, 739,\n",
      "        565, 646, 272, 847, 354, 880, 394, 689, 362, 392, 674, 386, 234, 190,\n",
      "        327, 435,  48,  69, 574, 526, 557, 589, 997, 496, 869, 234, 278, 817,\n",
      "         83, 322,  84, 712, 953, 471, 379, 825, 665, 381, 674, 908, 106, 895,\n",
      "         85, 805])\n",
      "tensor([627, 354, 227, 525, 348, 788, 806, 937, 885, 660, 196,   2, 955, 544,\n",
      "         56, 215, 578, 976, 394, 805, 839,  57, 370, 753,   3, 802, 578, 407,\n",
      "        421, 947, 754, 664, 758, 989, 165, 541, 240, 711, 316, 731, 391, 589,\n",
      "        500, 813, 132, 995, 994, 299, 712, 885, 831, 869,  43, 715, 311, 785,\n",
      "        708,  95, 650, 957, 678, 901, 235, 970, 939, 826, 564, 234, 192, 490,\n",
      "        351, 214, 379, 176, 404, 100, 385, 634, 160, 432, 361, 173, 296, 477,\n",
      "        900, 980, 773, 393, 181, 395, 666, 378, 814, 953, 787, 784, 298, 216,\n",
      "        978, 674, 867,  90, 847, 478, 324, 613, 122, 981, 560, 933, 777, 146,\n",
      "        850, 109,  27, 540, 480, 708, 483, 678,   8, 727, 201, 463, 133, 296,\n",
      "        266, 590])\n",
      "tensor([823, 369, 843, 317, 317, 196, 457, 380, 663, 911, 733, 330, 926, 768,\n",
      "        351, 985, 861, 139, 610, 809, 570, 469, 172, 390, 734, 127, 112, 303,\n",
      "        137, 195, 715, 356, 199, 831, 431, 632,  95, 127, 938, 588, 115, 620,\n",
      "         34, 253, 354, 496, 893, 587,  55, 343, 996, 717, 138, 486,  95, 103,\n",
      "        857, 690,  16, 349,  92, 769, 925, 813,  41, 350, 655, 271, 197, 521,\n",
      "        570, 488, 461, 115, 654,  20, 975, 430, 132, 744, 808,  42, 190, 660,\n",
      "        857, 650, 290, 632, 346, 625, 992, 498, 785, 898, 133, 873, 630, 518,\n",
      "        235, 439, 584, 232, 536, 936, 209, 157, 460, 940, 604, 123, 977, 384,\n",
      "        358, 345, 462, 795, 414, 214, 370, 742, 929, 451, 479, 890, 763, 478,\n",
      "        858, 232])\n",
      "tensor([627, 818, 211, 837, 401, 164,  56, 849, 681, 335,  35, 953, 909, 638,\n",
      "        619, 848, 746, 924,  45, 718, 771,  59, 955, 379, 329, 178, 704, 268,\n",
      "        133, 486, 981, 931, 886, 328, 293, 734, 173, 116, 695, 952, 844, 266,\n",
      "        584, 273, 546, 448, 109, 623, 847, 675, 324, 847, 114, 775, 360, 639,\n",
      "        245, 600, 330, 347, 413,  70, 383, 165, 717, 502, 589, 832, 182, 121,\n",
      "        794,  53, 866, 660, 589, 277, 134, 147, 509, 588, 208,  38, 634, 323,\n",
      "        912, 579, 908, 566, 215, 206, 659,  93, 399, 124, 306, 648, 417, 959,\n",
      "        408, 811, 602, 737, 411,   5, 301, 670, 950, 950, 539, 921, 715, 833,\n",
      "        693, 659,   0, 503, 971, 256, 631, 104, 156, 121, 328, 698, 659, 908,\n",
      "        522, 471])\n",
      "tensor([ 43, 400, 598, 748, 132, 148, 338, 905, 815, 845, 638, 276, 307,  65,\n",
      "        710, 379, 175, 603, 690, 324, 678, 933, 654, 189, 910, 565, 865, 712,\n",
      "        901, 376, 294, 333, 720, 643, 116, 493, 840,   0, 299, 104, 981, 415,\n",
      "        105, 206, 915, 151, 760, 215, 266, 916, 393, 229, 335, 415, 617, 624,\n",
      "         15, 660, 264, 130, 745, 793, 854, 610, 742, 507, 351, 237, 527,   6,\n",
      "        376, 477, 119, 775,  55, 648, 161, 346, 841, 834, 548, 367, 980, 981,\n",
      "         90, 149, 384,  94, 303, 774,  97, 936, 581, 698, 650, 827, 939, 717,\n",
      "        262, 524, 446, 866, 639,  85, 548, 704, 191, 817, 467, 446, 434, 425,\n",
      "        446, 775, 146,  92, 715, 102, 508, 276, 300, 732, 630, 335, 149, 114,\n",
      "        346, 457])\n",
      "tensor([961,  62,  95, 668, 416, 279, 760, 381, 236, 121, 207, 475, 117,  75,\n",
      "        155, 767, 185,  37, 552, 490, 858, 368, 445, 801, 831, 151, 457, 939,\n",
      "        934, 609, 706, 763, 301, 789, 654, 423, 527, 854,  24, 473, 954, 732,\n",
      "        898, 114, 362,  55, 109, 282, 772, 147, 255, 219, 670, 265, 734,  25,\n",
      "        873, 559, 302, 538, 909, 332, 655,  75, 584, 938, 278, 616, 671, 145,\n",
      "        691, 265, 653, 639, 511,  16, 386, 467, 278, 725, 464, 985, 522, 242,\n",
      "        188, 977, 563, 519, 399, 161, 719, 625, 229, 651, 565, 266, 998, 667,\n",
      "        840, 679, 705, 767, 497, 544, 720,  24, 375, 930, 465, 672, 672, 167,\n",
      "        113, 105, 848, 128, 361, 512, 565, 947, 424, 817, 263, 283, 691, 613,\n",
      "        880, 687])\n",
      "tensor([199, 416, 256, 195, 799, 335, 867, 798, 662, 724, 723, 676, 265, 658,\n",
      "        999, 170,  18, 136, 843,  75, 447, 211, 536, 195, 656, 252, 742, 598,\n",
      "        679, 616, 490,  64, 436, 539, 918, 820, 610, 666, 272, 807, 805, 856,\n",
      "        450, 404, 827, 803, 391, 336,   2, 336, 577, 985, 366, 150,  82,  36,\n",
      "        224, 111,  16, 430,  61, 325, 570, 756, 643, 497, 959, 443, 293, 739,\n",
      "        511, 280, 268, 304, 162,  74, 599, 969, 554, 163, 645, 413, 816, 939,\n",
      "        540, 682, 332, 450, 703, 352, 428, 715, 407,  54, 803, 874, 672, 738,\n",
      "        709, 462, 473, 387, 576, 620,  52, 506, 230, 468, 445,  29,  80, 875,\n",
      "        297, 921, 635, 390, 235,  81, 530, 337,  29, 872, 293, 619, 312, 314,\n",
      "        920, 935])\n",
      "tensor([284, 471, 745, 805, 688, 863, 236, 480, 755, 663, 371, 421, 439,  29,\n",
      "        137, 595, 356, 730, 849, 540, 380,  59, 403, 117,   3, 420, 406, 365,\n",
      "        254, 118, 827, 518, 612, 187, 309, 935, 689, 998, 480, 238, 894, 737,\n",
      "        473, 810, 398, 650, 412, 811, 975, 187, 582, 465, 239, 588, 995, 330,\n",
      "        976, 398,  93, 746, 567, 242, 969, 561,  52, 897, 857, 204, 318, 595,\n",
      "        257, 906, 845, 704,  97, 617,  42, 757, 177, 473, 117, 944, 811, 345,\n",
      "         77, 861,  61, 488, 129, 289, 837,  22, 639, 175, 152, 160, 725, 993,\n",
      "        229,  35, 103, 702, 768,  14, 837, 955, 751, 990, 467, 552, 687, 136,\n",
      "        282,  91, 579, 472, 600, 409, 232, 545, 428, 838, 696, 512, 757, 767,\n",
      "        216, 744])\n",
      "tensor([124, 246,   7, 426,  11, 411, 121, 841, 893, 630, 193, 997, 390, 741,\n",
      "        271, 587, 144, 340, 311, 939, 767,  70, 673, 311, 276, 481, 154, 327,\n",
      "        403, 944, 398, 653, 472, 523, 112, 378, 231, 321, 720, 161,  30, 423,\n",
      "        820, 329, 942, 922, 628, 921, 303, 558, 896, 864, 599, 319, 387, 222,\n",
      "        377, 359, 160, 373,  64, 667, 490,  79, 292, 579, 299, 414, 686, 598,\n",
      "        435, 618, 147, 247, 404, 876, 173, 899, 748, 238, 891, 921, 970, 440,\n",
      "        951, 730, 199, 431, 401, 430, 696, 632, 592, 206, 134,  25, 839,  49,\n",
      "        591, 277, 148, 208, 576, 126, 900, 426, 687, 287,  47, 754, 448, 283,\n",
      "        510, 657, 899, 774, 326,  42, 329, 245, 396, 536, 142, 959, 983, 901,\n",
      "        147, 536])\n",
      "tensor([344, 996, 779, 645, 984, 527, 818, 505, 905, 880, 303, 583, 305, 583,\n",
      "        794, 300, 826, 377,  13, 604, 249, 556, 517, 567, 468, 153, 876, 161,\n",
      "        663, 851, 937,   2, 463, 256, 439, 502, 410, 646, 495,  72,  95, 223,\n",
      "        197, 375, 870, 727, 591, 425, 606, 346, 355, 231, 115, 343, 403, 153,\n",
      "        279,  91, 955,  68, 442, 513, 773, 494, 190, 625, 177,  89, 448, 229,\n",
      "        608, 882, 287, 892, 358, 769, 315, 822, 564, 584, 430,  40,  57, 274,\n",
      "        826, 126, 170, 155, 228, 557, 897, 808, 614, 908, 846, 180, 539, 369,\n",
      "        837, 637, 443,  89, 751, 426, 606, 401, 297, 537, 453, 633, 819, 104,\n",
      "        157, 138, 346, 617, 114, 480, 176, 632, 687, 862, 248, 966, 924, 973,\n",
      "        275, 218])\n",
      "tensor([400, 825, 305, 274, 214, 558, 314, 281, 826,  66, 863, 180, 557, 276,\n",
      "         25, 235, 411,  17, 927, 620, 486, 611, 254, 755, 303, 610,  42,  84,\n",
      "        787,  41, 957, 819, 743, 966, 457, 263, 527, 495, 129,  74, 535, 379,\n",
      "        511, 145, 258, 969,  95, 759, 648, 517, 673, 802, 895, 447, 867, 491,\n",
      "         74, 191, 491, 479, 736, 982,  57, 941, 222, 983, 847, 637, 406, 766,\n",
      "        664, 889, 250, 740, 406, 739, 318, 450, 147, 352, 856, 275, 187, 250,\n",
      "        973, 349, 613, 658, 524, 493, 187, 720, 613, 996, 667, 253, 631, 210,\n",
      "        403, 247, 146, 532, 117, 903, 906, 985, 642, 348, 580, 661, 359, 628,\n",
      "        625, 585, 432, 259, 424,  79, 142, 639, 303, 895,  44, 514, 309, 220,\n",
      "        292, 825])\n",
      "tensor([515, 179, 187, 147, 165, 942, 680, 374, 520, 698, 253, 676, 625, 627,\n",
      "        109, 340, 746, 977, 941, 638, 301, 937, 997, 434, 515, 602, 523, 134,\n",
      "        412, 807, 357, 245, 142, 364, 554, 675, 230, 567, 517, 367, 272, 576,\n",
      "        459, 986, 652, 865, 126, 631, 989, 135, 747, 998, 323, 167, 834, 333,\n",
      "        576, 913, 602, 892, 986, 338, 569, 744, 694, 619, 635, 847, 913, 208,\n",
      "        143, 723, 352,  61, 333, 240, 845, 820, 769, 330, 569, 196, 305, 330,\n",
      "        281, 135, 405, 762, 632, 135, 809, 251, 536, 290, 456, 515, 269, 436,\n",
      "        355, 108, 751, 150, 124, 504, 712, 753, 908, 520,  54, 678, 769, 352,\n",
      "        108, 997, 937,  72, 133, 580, 782, 618, 246, 436,  52, 941, 188, 213,\n",
      "         75, 424])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predicted_eps, eps)\n\u001b[0;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     17\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\shan\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\shan\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "load_model = None\n",
    "if load_model:\n",
    "    model.load_state_dict(torch.load(load_model, map_location=device))\n",
    "\n",
    "losses = []\n",
    "batchlosses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for e, data in enumerate(train_loader):\n",
    "        x0, _ = data\n",
    "        x0 = x0.to(device)\n",
    "        t = torch.randint(0, T, (batch_size,), device=device)\n",
    "        t_norm = t.float() / T\n",
    "        t_emb = time_embedding_layer(t_norm)  # Shape: [batch_size, embedding_dim]\n",
    "        eps = torch.randn_like(x0).to(device)\n",
    "        x_t = torch.sqrt(alpha_bar[t]) * x0 + torch.sqrt(1 - alpha_bar[t]) * eps\n",
    "        predicted_eps = model(x_t, t_emb)  # Pass the precomputed embedding to the model\n",
    "        loss = criterion(predicted_eps, eps)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "\n",
    "        # if e % 100 == 99:\n",
    "        #     print(f\"Epoch {epoch}, Batch {e+1}, Average Loss: {np.mean(losses[-100:]):.4f}\")\n",
    "    \n",
    "    batchlosses.append(np.mean(losses[-100:]))\n",
    "    print(f\"Epoch {epoch}, Average Loss: {batchlosses[-1]:.4f}\")\n",
    "    plt.plot(batchlosses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    if epoch % 2 == 0 and epoch > 0:\n",
    "        torch.save(model.state_dict(), f\"data_SHAN_CIFAR/DDPM_{epoch}.pth\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54a74a6-6786-43c8-9b7b-569eb01df2f2",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f69f28-7031-48bc-96d4-85b61ba8d524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1410cbf71d0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_model = \"DDPM_CIFAR_200.pth\"\n",
    "\n",
    "if load_model:\n",
    "    model.load_state_dict(torch.load(load_model, map_location=device))\n",
    "with torch.inference_mode():\n",
    "    model.eval()\n",
    "    batch_size = 1\n",
    "    xt = torch.randn(batch_size, 3, 32, 32).to(device)\n",
    "    \n",
    "\n",
    "    for t in torch.arange(T-1, -1, -1):\n",
    "        t = t.reshape(1)\n",
    "        t = t.to(device)\n",
    "        t_emb = time_embedding_layer(t.float())\n",
    "        z = torch.randn(batch_size, 3, 32, 32).to(device) if t > 1 else torch.zeros(batch_size, 3, 32, 32).to(device)\n",
    "        \n",
    "        xt_new = 1 / torch.sqrt(alpha[t]) * (xt - (1 - alpha[t])/(torch.sqrt(1 - alpha_bar[t])) * \n",
    "                                                    model(xt, t_emb)) + torch.sqrt(beta[t]) * z\n",
    "        xt = xt_new\n",
    "\n",
    "\n",
    "plt.imshow(reverse_transform(xt[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7612842",
   "metadata": {},
   "source": [
    "# Overfit one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac8f961",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from UNET import UNET\n",
    "\n",
    "model = UNET().to(device)\n",
    "# Initialize the linear time embedding layer\n",
    "time_embedding_dim = 128  # You can set this to any suitable size\n",
    "time_embedding_layer = LinearTimeEmbedding(time_embedding_dim).to(device)\n",
    "\n",
    "# Get a single image from the batch\n",
    "single_batch, _ = next(iter(train_loader))\n",
    "single_batch = single_batch[0].unsqueeze(0).to(device)\n",
    "single_batchs = single_batch.repeat(1, 1, 1, 1)\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.MSELoss(reduction=\"sum\")\n",
    "plot_every = 400\n",
    "\n",
    "running_loss = 0\n",
    "# Training loop to overfit one batch\n",
    "epochs = 5000\n",
    "for epoch in range(epochs):\n",
    "    t = torch.randint(0, T, (1,), device=device)\n",
    "    t_norm = t.float() / T # Normalize the time to [0, 1]\n",
    "    t_emb = time_embedding_layer(t_norm)  # Shape: [1, embedding_dim]\n",
    "    eps = torch.randn_like(single_batchs).to(device)\n",
    "    x_t = torch.sqrt(alpha_bar[t]) * single_batchs + torch.sqrt(1 - alpha_bar[t]) * eps\n",
    "    predicted_eps = model(x_t, t_emb)  # Pass the precomputed embedding to the model\n",
    "    loss = criterion(predicted_eps, eps)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "\n",
    "    if epoch % 100 == 99:\n",
    "        print(f\"Epoch {epoch}, Average Loss: {running_loss / 100:.4f}\")\n",
    "        running_loss = 0.0  \n",
    "\n",
    "    # if epoch % plot_every == 0:\n",
    "    #     with torch.inference_mode():\n",
    "    #         model.eval()\n",
    "    #         t = torch.randint(0, T, (1,), device=device)\n",
    "    #         t_emb = time_embedding_layer(t)\n",
    "    #         eps = torch.randn_like(single_batch).to(device)\n",
    "    #         x_t = torch.sqrt(alpha_bar[t]) * single_batch + torch.sqrt(1 - alpha_bar[t]) * eps\n",
    "    #         predicted_eps = model(x_t, t_emb)\n",
    "    #         reconstructed = x_t - predicted_eps\n",
    "    #         fig, axs = plt.subplots(1, 3, tight_layout=True)\n",
    "    #         axs[0].imshow(predicted_eps[0][0].cpu().detach().numpy(), cmap=\"gray\")\n",
    "    #         axs[0].set_title(\"Predicted Noise\")\n",
    "    #         axs[1].imshow(eps[0][0].cpu().detach().numpy(), cmap=\"gray\")\n",
    "    #         axs[1].set_title(\"True Noise\")\n",
    "    #         axs[2].imshow(np.abs(predicted_eps[0][0].cpu().detach().numpy()-eps[0][0].cpu().detach().numpy()), cmap=\"gray\")\n",
    "    #         axs[2].set_title(\"Difference in noise\")\n",
    "    #         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a72823c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.826073 0.8980392\n",
      "-29.50021 0.011764705\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize a random noise tensor\n",
    "xt = torch.randn(1, 3, 32, 32).to(device)\n",
    "\n",
    "# Perform the reverse diffusion process\n",
    "with torch.no_grad():\n",
    "    for t in torch.arange(T-1, -1, -1):\n",
    "        t = t.reshape(1).to(device)\n",
    "        t_emb = time_embedding_layer(t.float())\n",
    "        z = torch.randn(1, 3, 32, 32).to(device) if t > 1 else torch.zeros(1, 3, 32, 32).to(device)\n",
    "        \n",
    "        xt_new = 1 / torch.sqrt(alpha[t]) * (xt - (1 - alpha[t]) / torch.sqrt(1 - alpha_bar[t]) * model(xt, t_emb)) + torch.sqrt(beta[t]) * z\n",
    "        xt = xt_new\n",
    "\n",
    "# Plot the generated image\n",
    "fig,axs = plt.subplots(1, 2, tight_layout=True)\n",
    "im_1 = reverse_transform(xt[0].permute(1, 2, 0).cpu().detach().numpy())\n",
    "axs[0].imshow(im_1)\n",
    "axs[0].set_title(\"Generated Image\")\n",
    "axs[0].axis(\"off\")\n",
    "im_2 = reverse_transform(single_batch[0].permute(1, 2, 0).cpu().detach().numpy())\n",
    "axs[1].imshow(im_2)\n",
    "axs[1].set_title(\"Original Image\")\n",
    "axs[1].axis(\"off\")\n",
    "print(im_1.max(),im_2.max())\n",
    "print(im_1.min(),im_2.min())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fb7c99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
