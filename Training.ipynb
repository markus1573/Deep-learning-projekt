{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "def4f85a-37b5-472e-8782-a5df13c5d601",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec1af4-1d61-46ef-90b9-b972129d09a5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3eeb867f-c38e-4e5c-ab89-a7e747ead472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "017d87cd-410b-44ef-ac34-560022867208",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "#torch.backends.cudnn.enabled = False\n",
    "val_size = 5000\n",
    "test_size = 5000\n",
    "batch_size = 16\n",
    "num_workers = 4\n",
    "pin_memory = False if device == torch.device('cpu') else True\n",
    "\n",
    "# transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "# Downloading MNIST again :) Training (60k) and test(5k) + val(5k) split\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('./mnist_data',\n",
    "                                            download=True,\n",
    "                                            train=True,\n",
    "                                            transform=transform),\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=num_workers,\n",
    "                                            pin_memory=pin_memory,\n",
    "                                            drop_last=True)\n",
    "\n",
    "test_dataset = datasets.MNIST('./mnist_data',\n",
    "                               download=True,\n",
    "                               train=False,\n",
    "                               transform=transform)\n",
    "\n",
    "val_dataset, test_dataset = random_split(test_dataset, [val_size, test_size])\n",
    "\n",
    "# Test set to compare with DDPM paper\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=num_workers,\n",
    "                                            pin_memory=pin_memory)\n",
    "\n",
    "# Validation set so we can keep track of approximated FID score while training\n",
    "validation_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=num_workers,\n",
    "                                            pin_memory=pin_memory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0ae1fca-cff3-4b53-86d1-65c59740a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cosine noise schedule\n",
    "def f(t, s=torch.tensor([0.008]), T=torch.tensor([1000])):\n",
    "    return min(torch.cos((t / T + s) / (1 + s) * (torch.pi / 2)).pow(2), 0.999)\n",
    "\n",
    "T = 1000\n",
    "ts = torch.arange(T)\n",
    "alpha_bar = torch.tensor([min(f(t)/f(torch.tensor([0])),0.999) for t in ts]) \n",
    "beta = torch.tensor([1 - alpha_bar[t]/(alpha_bar[t-1]) if t > 0 else torch.tensor([0]) for t in ts])\n",
    "alpha = 1 - beta\n",
    "alpha = alpha.view((1000, 1, 1, 1)).to(device)\n",
    "beta = beta.view((1000, 1, 1, 1)).to(device)\n",
    "alpha_bar = alpha_bar.view((1000, 1, 1, 1)).to(device)\n",
    "\n",
    "\n",
    "# # Linear noise schedule\n",
    "# T = 1000\n",
    "# beta_start, beta_end = 1e-4, 2e-2\n",
    "# beta = torch.linspace(beta_start, beta_end, T)  # Linear noise schedule\n",
    "# alpha = 1.0 - beta\n",
    "# alpha_bar = torch.cumprod(alpha, dim=0)  # Cumulative product for alpha_bar\n",
    "\n",
    "# # Reshape for broadcasting (if required for your model)\n",
    "# alpha = alpha.view((T, 1, 1, 1)).to(device)\n",
    "# beta = beta.view((T, 1, 1, 1)).to(device)\n",
    "# alpha_bar = alpha_bar.view((T, 1, 1, 1)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93cbbcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c42fe879-ee13-4434-9a0d-de015ddd4c2f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c671dbab-ffe9-4fe5-841a-80042e7593b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNET, self).__init__()\n",
    "        channels = [32, 64, 128, 256]\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(2, channels[0], kernel_size=3, padding=1),  # (batchsize, 32, 28, 28)\n",
    "                nn.GroupNorm(4, channels[0]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2),  # (batchsize, 32, 14, 14)\n",
    "                nn.Conv2d(channels[0], channels[1], kernel_size=3, padding=1),  # (batchsize, 64, 14, 14)\n",
    "                nn.GroupNorm(4, channels[1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2),  # (batchsize, 64, 7, 7)\n",
    "                nn.Conv2d(channels[1], channels[2], kernel_size=3, padding=1),  # (batchsize, 128, 7, 7)\n",
    "                nn.GroupNorm(8, channels[2]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.3)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2, padding=1),  # (batchsize, 128, 4, 4)\n",
    "                nn.Conv2d(channels[2], channels[3], kernel_size=3, padding=1),  # (batchsize, 256, 4, 4)\n",
    "                nn.GroupNorm(8, channels[3]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.3)\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        self.tconvs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(channels[3], channels[2], kernel_size=3, \n",
    "                                   stride=2, padding=1, output_padding=0),   # (batchsize, 128, 7, 7)\n",
    "                nn.GroupNorm(8, channels[2]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.3)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(channels[2]*2, channels[1], kernel_size=3,\n",
    "                                   stride=2, padding=1, output_padding=1),   # (batchsize, 64, 14, 14)\n",
    "                nn.GroupNorm(8, channels[1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(channels[1]*2, channels[0], kernel_size=3, \n",
    "                                   stride=2, padding=1, output_padding=1),   # (batchsize, 32, 28, 28)\n",
    "                nn.GroupNorm(4, channels[0]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(channels[0]*2,channels[0],kernel_size=3,padding=1),  # (batchsize, 32, 28, 28)\n",
    "                nn.GroupNorm(4, channels[0]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2),\n",
    "                nn.Conv2d(channels[0],1,kernel_size=1) # (batchsize, 1, 28, 28)\n",
    "            )      \n",
    "        ])\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x_trans = torch.cat((x, t), dim=-3)\n",
    "        signal = x_trans\n",
    "        signals = []\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "            # print(f\"conv {i}\")\n",
    "            signal = conv(signal)\n",
    "            # print(signal.shape)\n",
    "            if i < len(self.convs)-1:\n",
    "                signals.append(signal)\n",
    "                \n",
    "        for i, tconv in enumerate(self.tconvs):\n",
    "            # print(f\"tconv {i}\")\n",
    "            # print(f\"signal shape: {signal.shape}\")\n",
    "            if i == 0:\n",
    "                signal = tconv(signal)\n",
    "            else:\n",
    "                signal = torch.cat((signal, signals[-i]), dim=-3)\n",
    "                signal = tconv(signal)\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f04a5df-da69-407e-9285-90de11992cef",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8403ab0-aea6-492a-98b9-31ab55064c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from UNET import UNET\n",
    "epochs = 20\n",
    "model = UNET()\n",
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.MSELoss()\n",
    "running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8dc7e14a-63cd-4432-8f07-43fd00841fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 100), Average_loss: 1.021\n",
      "(0, 200), Average_loss: 1.002\n",
      "(0, 300), Average_loss: 1.002\n",
      "(0, 400), Average_loss: 1.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(eps, model(xt, t\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(batch_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)))\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 12\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     15\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for e, data in enumerate(train_loader):\n",
    "        x0, _ = data\n",
    "        x0 = x0.to(device)\n",
    "        t = torch.randint(0, T, (batch_size,)).to(device)\n",
    "        eps = torch.randn(batch_size, 1, 28, 28).to(device)\n",
    "        # print(eps.shape)\n",
    "        # print(x0.shape)\n",
    "        xt = torch.sqrt(alpha_bar[t]) * x0 + torch.sqrt(1 - alpha_bar[t]) * eps\n",
    "        loss = criterion(eps, model(xt, t.view(batch_size, 1, 1, 1).expand(batch_size, 1, 28, 28)))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if e % 100 == 99:\n",
    "            print(f'{epoch, e+1}, Average_loss: {running_loss/100:.3f}')\n",
    "            running_loss = 0.0\n",
    "    if epoch % 10 == 9:\n",
    "        torch.save(model.state_dict(), f\"DDPM_{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54a74a6-6786-43c8-9b7b-569eb01df2f2",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "999697a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"DDPM_20.pth\",map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f69f28-7031-48bc-96d4-85b61ba8d524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2632e7c7890>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    model.eval()\n",
    "    batch_size_sample = 1\n",
    "    xt = torch.randn(batch_size_sample, 1, 28, 28).to(device)\n",
    "\n",
    "    for t in torch.arange(T-1, -1, -1):\n",
    "        # print(t)\n",
    "        t = t.to(device)\n",
    "        z = torch.randn(batch_size_sample, 1, 28, 28).to(device) if t > 1 else torch.zeros(batch_size_sample, 1, 28, 28).to(device)\n",
    "        xt_new = 1 / torch.sqrt(alpha[t]) * (xt - (1 - alpha[t])/(torch.sqrt(1 - alpha_bar[t])) * \n",
    "                                                    model(xt, t.view(batch_size_sample, 1, 1, 1).expand(batch_size_sample, 1, 28, 28))) + torch.sqrt(beta[t]) * z\n",
    "        xt = xt_new\n",
    "plt.imshow(xt[0][0].cpu().detach().numpy(), cmap=\"grey\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6bf278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from all the timesteps\n",
    "fig, axs = plt.subplots(1, 5, tight_layout=True, figsize=(40, 20))\n",
    "tens = np.arange(10,51,10)\n",
    "seed = 3546\n",
    "for ten in tens:\n",
    "    with torch.inference_mode():\n",
    "        # torch.manual_seed(seed)\n",
    "        model.load_state_dict(torch.load(f\"Data_cosnoise_normalized/DDPM_{ten}.pth\", map_location=device))\n",
    "        model.eval()\n",
    "        batch_size_sample = 1\n",
    "        xt = torch.randn(batch_size_sample, 1, 28, 28).to(device)\n",
    "\n",
    "        for t in torch.arange(T, 0, -1):\n",
    "            t = t.to(device)\n",
    "            # print(t)\n",
    "            z = torch.randn(batch_size_sample, 1, 28, 28).to(device) if t > 1 else torch.zeros(batch_size_sample, 1, 28, 28).to(device)\n",
    "            xt_new = 1 / torch.sqrt(alpha[t - 1]) * (xt - (1 - alpha[t - 1])/(torch.sqrt(1 - alpha_bar[t - 1])) * \n",
    "                                                        model(xt, t.view(batch_size_sample, 1, 1, 1).expand(batch_size_sample, 1, 28, 28))) + torch.sqrt(beta[t-1]) * z\n",
    "            xt = xt_new\n",
    "            axs[(ten//10)-1].imshow(xt[0][0].cpu().detach().numpy(), cmap=\"gray\")\n",
    "            axs[(ten//10)-1].axis(\"off\")\n",
    "            axs[(ten//10)-1].set_title(f\"Epoch={ten}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0293c70c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
