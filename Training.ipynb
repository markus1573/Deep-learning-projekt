{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "def4f85a-37b5-472e-8782-a5df13c5d601",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec1af4-1d61-46ef-90b9-b972129d09a5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eeb867f-c38e-4e5c-ab89-a7e747ead472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "017d87cd-410b-44ef-ac34-560022867208",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "#torch.backends.cudnn.enabled = False\n",
    "val_size = 5000\n",
    "test_size = 5000\n",
    "batch_size = 16\n",
    "num_workers = 4\n",
    "pin_memory = False if device == torch.device('cpu') else True\n",
    "\n",
    "# transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "# Downloading MNIST again :) Training (60k) and test(5k) + val(5k) split\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('./mnist_data',\n",
    "                                            download=True,\n",
    "                                            train=True,\n",
    "                                            transform=transform),\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=num_workers,\n",
    "                                            pin_memory=pin_memory,\n",
    "                                            drop_last=True)\n",
    "\n",
    "test_dataset = datasets.MNIST('./mnist_data',\n",
    "                               download=True,\n",
    "                               train=False,\n",
    "                               transform=transform)\n",
    "\n",
    "val_dataset, test_dataset = random_split(test_dataset, [val_size, test_size])\n",
    "\n",
    "# Test set to compare with DDPM paper\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=num_workers,\n",
    "                                            pin_memory=pin_memory)\n",
    "\n",
    "# Validation set so we can keep track of approximated FID score while training\n",
    "validation_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=num_workers,\n",
    "                                            pin_memory=pin_memory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0ae1fca-cff3-4b53-86d1-65c59740a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cosine noise schedule\n",
    "def f(t, s=torch.tensor([0.008]), T=torch.tensor([1000])):\n",
    "    return min(torch.cos((t / T + s) / (1 + s) * (torch.pi / 2)).pow(2), 0.999)\n",
    "\n",
    "T = 1000\n",
    "ts = torch.arange(T)\n",
    "alpha_bar = torch.tensor([min(f(t)/f(torch.tensor([0])),0.999) for t in ts]) \n",
    "beta = torch.tensor([1 - alpha_bar[t]/(alpha_bar[t-1]) if t > 0 else torch.tensor([0]) for t in ts])\n",
    "alpha = 1 - beta\n",
    "alpha = alpha.view((1000, 1, 1, 1)).to(device)\n",
    "beta = beta.view((1000, 1, 1, 1)).to(device)\n",
    "alpha_bar = alpha_bar.view((1000, 1, 1, 1)).to(device)\n",
    "\n",
    "\n",
    "# # Linear noise schedule\n",
    "# T = 1000\n",
    "# beta_start, beta_end = 1e-4, 2e-2\n",
    "# beta = torch.linspace(beta_start, beta_end, T)  # Linear noise schedule\n",
    "# alpha = 1.0 - beta\n",
    "# alpha_bar = torch.cumprod(alpha, dim=0)  # Cumulative product for alpha_bar\n",
    "\n",
    "# # Reshape for broadcasting (if required for your model)\n",
    "# alpha = alpha.view((T, 1, 1, 1)).to(device)\n",
    "# beta = beta.view((T, 1, 1, 1)).to(device)\n",
    "# alpha_bar = alpha_bar.view((T, 1, 1, 1)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93cbbcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c42fe879-ee13-4434-9a0d-de015ddd4c2f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c671dbab-ffe9-4fe5-841a-80042e7593b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNET, self).__init__()\n",
    "        channels = [32, 64, 128, 256]\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(2, channels[0], kernel_size=3, padding=1),  # (batchsize, 32, 28, 28)\n",
    "                nn.GroupNorm(4, channels[0]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2),  # (batchsize, 32, 14, 14)\n",
    "                nn.Conv2d(channels[0], channels[1], kernel_size=3, padding=1),  # (batchsize, 64, 14, 14)\n",
    "                nn.GroupNorm(4, channels[1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2),  # (batchsize, 64, 7, 7)\n",
    "                nn.Conv2d(channels[1], channels[2], kernel_size=3, padding=1),  # (batchsize, 128, 7, 7)\n",
    "                nn.GroupNorm(8, channels[2]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.3)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2, padding=1),  # (batchsize, 128, 4, 4)\n",
    "                nn.Conv2d(channels[2], channels[3], kernel_size=3, padding=1),  # (batchsize, 256, 4, 4)\n",
    "                nn.GroupNorm(8, channels[3]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.3)\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        self.tconvs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(channels[3], channels[2], kernel_size=3, \n",
    "                                   stride=2, padding=1, output_padding=0),   # (batchsize, 128, 7, 7)\n",
    "                nn.GroupNorm(8, channels[2]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.3)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(channels[2]*2, channels[1], kernel_size=3,\n",
    "                                   stride=2, padding=1, output_padding=1),   # (batchsize, 64, 14, 14)\n",
    "                nn.GroupNorm(8, channels[1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(channels[1]*2, channels[0], kernel_size=3, \n",
    "                                   stride=2, padding=1, output_padding=1),   # (batchsize, 32, 28, 28)\n",
    "                nn.GroupNorm(4, channels[0]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(channels[0]*2,channels[0],kernel_size=3,padding=1),  # (batchsize, 32, 28, 28)\n",
    "                nn.GroupNorm(4, channels[0]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2),\n",
    "                nn.Conv2d(channels[0],1,kernel_size=1) # (batchsize, 1, 28, 28)\n",
    "            )      \n",
    "        ])\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x_trans = torch.cat((x, t), dim=-3)\n",
    "        signal = x_trans\n",
    "        signals = []\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "            # print(f\"conv {i}\")\n",
    "            signal = conv(signal)\n",
    "            # print(signal.shape)\n",
    "            if i < len(self.convs)-1:\n",
    "                signals.append(signal)\n",
    "                \n",
    "        for i, tconv in enumerate(self.tconvs):\n",
    "            # print(f\"tconv {i}\")\n",
    "            # print(f\"signal shape: {signal.shape}\")\n",
    "            if i == 0:\n",
    "                signal = tconv(signal)\n",
    "            else:\n",
    "                signal = torch.cat((signal, signals[-i]), dim=-3)\n",
    "                signal = tconv(signal)\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f04a5df-da69-407e-9285-90de11992cef",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8403ab0-aea6-492a-98b9-31ab55064c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from UNET import UNET\n",
    "epochs = 20\n",
    "model = UNET()\n",
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dc7e14a-63cd-4432-8f07-43fd00841fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 100), Average_loss: 1.035\n",
      "(0, 200), Average_loss: 1.013\n",
      "(0, 300), Average_loss: 1.009\n",
      "(0, 400), Average_loss: 1.006\n",
      "(0, 500), Average_loss: 1.003\n",
      "(0, 600), Average_loss: 1.004\n",
      "(0, 700), Average_loss: 1.003\n",
      "(0, 800), Average_loss: 1.002\n",
      "(0, 900), Average_loss: 1.001\n",
      "(0, 1000), Average_loss: 1.000\n",
      "(0, 1100), Average_loss: 1.001\n",
      "(0, 1200), Average_loss: 1.002\n",
      "(0, 1300), Average_loss: 0.999\n",
      "(0, 1400), Average_loss: 0.998\n",
      "(0, 1500), Average_loss: 0.999\n",
      "(0, 1600), Average_loss: 0.997\n",
      "(0, 1700), Average_loss: 0.995\n",
      "(0, 1800), Average_loss: 0.961\n",
      "(0, 1900), Average_loss: 0.772\n",
      "(0, 2000), Average_loss: 0.567\n",
      "(0, 2100), Average_loss: 0.477\n",
      "(0, 2200), Average_loss: 0.434\n",
      "(0, 2300), Average_loss: 0.393\n",
      "(0, 2400), Average_loss: 0.377\n",
      "(0, 2500), Average_loss: 0.366\n",
      "(0, 2600), Average_loss: 0.349\n",
      "(0, 2700), Average_loss: 0.328\n",
      "(0, 2800), Average_loss: 0.321\n",
      "(0, 2900), Average_loss: 0.312\n",
      "(0, 3000), Average_loss: 0.303\n",
      "(0, 3100), Average_loss: 0.291\n",
      "(0, 3200), Average_loss: 0.295\n",
      "(0, 3300), Average_loss: 0.276\n",
      "(0, 3400), Average_loss: 0.278\n",
      "(0, 3500), Average_loss: 0.275\n",
      "(0, 3600), Average_loss: 0.265\n",
      "(0, 3700), Average_loss: 0.262\n",
      "(1, 100), Average_loss: 0.384\n",
      "(1, 200), Average_loss: 0.251\n",
      "(1, 300), Average_loss: 0.245\n",
      "(1, 400), Average_loss: 0.238\n",
      "(1, 500), Average_loss: 0.240\n",
      "(1, 600), Average_loss: 0.230\n",
      "(1, 700), Average_loss: 0.239\n",
      "(1, 800), Average_loss: 0.229\n",
      "(1, 900), Average_loss: 0.227\n",
      "(1, 1000), Average_loss: 0.231\n",
      "(1, 1100), Average_loss: 0.224\n",
      "(1, 1200), Average_loss: 0.217\n",
      "(1, 1300), Average_loss: 0.216\n",
      "(1, 1400), Average_loss: 0.212\n",
      "(1, 1500), Average_loss: 0.213\n",
      "(1, 1600), Average_loss: 0.207\n",
      "(1, 1700), Average_loss: 0.212\n",
      "(1, 1800), Average_loss: 0.212\n",
      "(1, 1900), Average_loss: 0.205\n",
      "(1, 2000), Average_loss: 0.198\n",
      "(1, 2100), Average_loss: 0.201\n",
      "(1, 2200), Average_loss: 0.204\n",
      "(1, 2300), Average_loss: 0.203\n",
      "(1, 2400), Average_loss: 0.202\n",
      "(1, 2500), Average_loss: 0.190\n",
      "(1, 2600), Average_loss: 0.191\n",
      "(1, 2700), Average_loss: 0.187\n",
      "(1, 2800), Average_loss: 0.184\n",
      "(1, 2900), Average_loss: 0.186\n",
      "(1, 3000), Average_loss: 0.190\n",
      "(1, 3100), Average_loss: 0.189\n",
      "(1, 3200), Average_loss: 0.193\n",
      "(1, 3300), Average_loss: 0.180\n",
      "(1, 3400), Average_loss: 0.187\n",
      "(1, 3500), Average_loss: 0.178\n",
      "(1, 3600), Average_loss: 0.184\n",
      "(1, 3700), Average_loss: 0.179\n",
      "(2, 100), Average_loss: 0.271\n",
      "(2, 200), Average_loss: 0.179\n",
      "(2, 300), Average_loss: 0.174\n",
      "(2, 400), Average_loss: 0.177\n",
      "(2, 500), Average_loss: 0.174\n",
      "(2, 600), Average_loss: 0.178\n",
      "(2, 700), Average_loss: 0.175\n",
      "(2, 800), Average_loss: 0.176\n",
      "(2, 900), Average_loss: 0.170\n",
      "(2, 1000), Average_loss: 0.172\n",
      "(2, 1100), Average_loss: 0.169\n",
      "(2, 1200), Average_loss: 0.170\n",
      "(2, 1300), Average_loss: 0.170\n",
      "(2, 1400), Average_loss: 0.167\n",
      "(2, 1500), Average_loss: 0.170\n",
      "(2, 1600), Average_loss: 0.165\n",
      "(2, 1700), Average_loss: 0.163\n",
      "(2, 1800), Average_loss: 0.163\n",
      "(2, 1900), Average_loss: 0.165\n",
      "(2, 2000), Average_loss: 0.164\n",
      "(2, 2100), Average_loss: 0.166\n",
      "(2, 2200), Average_loss: 0.166\n",
      "(2, 2300), Average_loss: 0.166\n",
      "(2, 2400), Average_loss: 0.166\n",
      "(2, 2500), Average_loss: 0.164\n",
      "(2, 2600), Average_loss: 0.161\n",
      "(2, 2700), Average_loss: 0.162\n",
      "(2, 2800), Average_loss: 0.159\n",
      "(2, 2900), Average_loss: 0.156\n",
      "(2, 3000), Average_loss: 0.157\n",
      "(2, 3100), Average_loss: 0.161\n",
      "(2, 3200), Average_loss: 0.161\n",
      "(2, 3300), Average_loss: 0.157\n",
      "(2, 3400), Average_loss: 0.161\n",
      "(2, 3500), Average_loss: 0.155\n",
      "(2, 3600), Average_loss: 0.156\n",
      "(2, 3700), Average_loss: 0.152\n",
      "(3, 100), Average_loss: 0.238\n",
      "(3, 200), Average_loss: 0.155\n",
      "(3, 300), Average_loss: 0.157\n",
      "(3, 400), Average_loss: 0.149\n",
      "(3, 500), Average_loss: 0.151\n",
      "(3, 600), Average_loss: 0.153\n",
      "(3, 700), Average_loss: 0.155\n",
      "(3, 800), Average_loss: 0.145\n",
      "(3, 900), Average_loss: 0.145\n",
      "(3, 1000), Average_loss: 0.147\n",
      "(3, 1100), Average_loss: 0.154\n",
      "(3, 1200), Average_loss: 0.152\n",
      "(3, 1300), Average_loss: 0.148\n",
      "(3, 1400), Average_loss: 0.152\n",
      "(3, 1500), Average_loss: 0.148\n",
      "(3, 1600), Average_loss: 0.145\n",
      "(3, 1700), Average_loss: 0.147\n",
      "(3, 1800), Average_loss: 0.151\n",
      "(3, 1900), Average_loss: 0.151\n",
      "(3, 2000), Average_loss: 0.150\n",
      "(3, 2100), Average_loss: 0.145\n",
      "(3, 2200), Average_loss: 0.147\n",
      "(3, 2300), Average_loss: 0.146\n",
      "(3, 2400), Average_loss: 0.147\n",
      "(3, 2500), Average_loss: 0.148\n",
      "(3, 2600), Average_loss: 0.141\n",
      "(3, 2700), Average_loss: 0.140\n",
      "(3, 2800), Average_loss: 0.144\n",
      "(3, 2900), Average_loss: 0.144\n",
      "(3, 3000), Average_loss: 0.147\n",
      "(3, 3100), Average_loss: 0.144\n",
      "(3, 3200), Average_loss: 0.143\n",
      "(3, 3300), Average_loss: 0.140\n",
      "(3, 3400), Average_loss: 0.140\n",
      "(3, 3500), Average_loss: 0.143\n",
      "(3, 3600), Average_loss: 0.143\n",
      "(3, 3700), Average_loss: 0.140\n",
      "(4, 100), Average_loss: 0.208\n",
      "(4, 200), Average_loss: 0.140\n",
      "(4, 300), Average_loss: 0.141\n",
      "(4, 400), Average_loss: 0.142\n",
      "(4, 500), Average_loss: 0.134\n",
      "(4, 600), Average_loss: 0.141\n",
      "(4, 700), Average_loss: 0.137\n",
      "(4, 800), Average_loss: 0.135\n",
      "(4, 900), Average_loss: 0.139\n",
      "(4, 1000), Average_loss: 0.141\n",
      "(4, 1100), Average_loss: 0.137\n",
      "(4, 1200), Average_loss: 0.140\n",
      "(4, 1300), Average_loss: 0.135\n",
      "(4, 1400), Average_loss: 0.136\n",
      "(4, 1500), Average_loss: 0.138\n",
      "(4, 1600), Average_loss: 0.132\n",
      "(4, 1700), Average_loss: 0.139\n",
      "(4, 1800), Average_loss: 0.136\n",
      "(4, 1900), Average_loss: 0.137\n",
      "(4, 2000), Average_loss: 0.132\n",
      "(4, 2100), Average_loss: 0.133\n",
      "(4, 2200), Average_loss: 0.132\n",
      "(4, 2300), Average_loss: 0.136\n",
      "(4, 2400), Average_loss: 0.134\n",
      "(4, 2500), Average_loss: 0.133\n",
      "(4, 2600), Average_loss: 0.134\n",
      "(4, 2700), Average_loss: 0.133\n",
      "(4, 2800), Average_loss: 0.132\n",
      "(4, 2900), Average_loss: 0.133\n",
      "(4, 3000), Average_loss: 0.131\n",
      "(4, 3100), Average_loss: 0.131\n",
      "(4, 3200), Average_loss: 0.129\n",
      "(4, 3300), Average_loss: 0.132\n",
      "(4, 3400), Average_loss: 0.132\n",
      "(4, 3500), Average_loss: 0.129\n",
      "(4, 3600), Average_loss: 0.132\n",
      "(4, 3700), Average_loss: 0.130\n",
      "(5, 100), Average_loss: 0.198\n",
      "(5, 200), Average_loss: 0.133\n",
      "(5, 300), Average_loss: 0.129\n",
      "(5, 400), Average_loss: 0.129\n",
      "(5, 500), Average_loss: 0.134\n",
      "(5, 600), Average_loss: 0.134\n",
      "(5, 700), Average_loss: 0.127\n",
      "(5, 800), Average_loss: 0.132\n",
      "(5, 900), Average_loss: 0.130\n",
      "(5, 1000), Average_loss: 0.129\n",
      "(5, 1100), Average_loss: 0.126\n",
      "(5, 1200), Average_loss: 0.124\n",
      "(5, 1300), Average_loss: 0.128\n",
      "(5, 1400), Average_loss: 0.131\n",
      "(5, 1500), Average_loss: 0.126\n",
      "(5, 1600), Average_loss: 0.132\n",
      "(5, 1700), Average_loss: 0.129\n",
      "(5, 1800), Average_loss: 0.129\n",
      "(5, 1900), Average_loss: 0.126\n",
      "(5, 2000), Average_loss: 0.128\n",
      "(5, 2100), Average_loss: 0.125\n",
      "(5, 2200), Average_loss: 0.127\n",
      "(5, 2300), Average_loss: 0.128\n",
      "(5, 2400), Average_loss: 0.124\n",
      "(5, 2500), Average_loss: 0.126\n",
      "(5, 2600), Average_loss: 0.125\n",
      "(5, 2700), Average_loss: 0.129\n",
      "(5, 2800), Average_loss: 0.126\n",
      "(5, 2900), Average_loss: 0.126\n",
      "(5, 3000), Average_loss: 0.125\n",
      "(5, 3100), Average_loss: 0.128\n",
      "(5, 3200), Average_loss: 0.124\n",
      "(5, 3300), Average_loss: 0.122\n",
      "(5, 3400), Average_loss: 0.121\n",
      "(5, 3500), Average_loss: 0.128\n",
      "(5, 3600), Average_loss: 0.119\n",
      "(5, 3700), Average_loss: 0.121\n",
      "(6, 100), Average_loss: 0.189\n",
      "(6, 200), Average_loss: 0.125\n",
      "(6, 300), Average_loss: 0.124\n",
      "(6, 400), Average_loss: 0.123\n",
      "(6, 500), Average_loss: 0.126\n",
      "(6, 600), Average_loss: 0.119\n",
      "(6, 700), Average_loss: 0.121\n",
      "(6, 800), Average_loss: 0.122\n",
      "(6, 900), Average_loss: 0.127\n",
      "(6, 1000), Average_loss: 0.123\n",
      "(6, 1100), Average_loss: 0.120\n",
      "(6, 1200), Average_loss: 0.122\n",
      "(6, 1300), Average_loss: 0.123\n",
      "(6, 1400), Average_loss: 0.125\n",
      "(6, 1500), Average_loss: 0.126\n",
      "(6, 1600), Average_loss: 0.123\n",
      "(6, 1700), Average_loss: 0.121\n",
      "(6, 1800), Average_loss: 0.125\n",
      "(6, 1900), Average_loss: 0.122\n",
      "(6, 2000), Average_loss: 0.121\n",
      "(6, 2100), Average_loss: 0.125\n",
      "(6, 2200), Average_loss: 0.124\n",
      "(6, 2300), Average_loss: 0.124\n",
      "(6, 2400), Average_loss: 0.121\n",
      "(6, 2500), Average_loss: 0.117\n",
      "(6, 2600), Average_loss: 0.122\n",
      "(6, 2700), Average_loss: 0.124\n",
      "(6, 2800), Average_loss: 0.124\n",
      "(6, 2900), Average_loss: 0.121\n",
      "(6, 3000), Average_loss: 0.118\n",
      "(6, 3100), Average_loss: 0.118\n",
      "(6, 3200), Average_loss: 0.118\n",
      "(6, 3300), Average_loss: 0.118\n",
      "(6, 3400), Average_loss: 0.119\n",
      "(6, 3500), Average_loss: 0.118\n",
      "(6, 3600), Average_loss: 0.121\n",
      "(6, 3700), Average_loss: 0.122\n",
      "(7, 100), Average_loss: 0.176\n",
      "(7, 200), Average_loss: 0.117\n",
      "(7, 300), Average_loss: 0.116\n",
      "(7, 400), Average_loss: 0.119\n",
      "(7, 500), Average_loss: 0.122\n",
      "(7, 600), Average_loss: 0.118\n",
      "(7, 700), Average_loss: 0.115\n",
      "(7, 800), Average_loss: 0.121\n",
      "(7, 900), Average_loss: 0.118\n",
      "(7, 1000), Average_loss: 0.121\n",
      "(7, 1100), Average_loss: 0.115\n",
      "(7, 1200), Average_loss: 0.117\n",
      "(7, 1300), Average_loss: 0.121\n",
      "(7, 1400), Average_loss: 0.118\n",
      "(7, 1500), Average_loss: 0.118\n",
      "(7, 1600), Average_loss: 0.118\n",
      "(7, 1700), Average_loss: 0.119\n",
      "(7, 1800), Average_loss: 0.122\n",
      "(7, 1900), Average_loss: 0.119\n",
      "(7, 2000), Average_loss: 0.116\n",
      "(7, 2100), Average_loss: 0.115\n",
      "(7, 2200), Average_loss: 0.118\n",
      "(7, 2300), Average_loss: 0.113\n",
      "(7, 2400), Average_loss: 0.120\n",
      "(7, 2500), Average_loss: 0.118\n",
      "(7, 2600), Average_loss: 0.116\n",
      "(7, 2700), Average_loss: 0.116\n",
      "(7, 2800), Average_loss: 0.118\n",
      "(7, 2900), Average_loss: 0.115\n",
      "(7, 3000), Average_loss: 0.115\n",
      "(7, 3100), Average_loss: 0.114\n",
      "(7, 3200), Average_loss: 0.119\n",
      "(7, 3300), Average_loss: 0.117\n",
      "(7, 3400), Average_loss: 0.117\n",
      "(7, 3500), Average_loss: 0.116\n",
      "(7, 3600), Average_loss: 0.116\n",
      "(7, 3700), Average_loss: 0.116\n",
      "(8, 100), Average_loss: 0.176\n",
      "(8, 200), Average_loss: 0.114\n",
      "(8, 300), Average_loss: 0.116\n",
      "(8, 400), Average_loss: 0.117\n",
      "(8, 500), Average_loss: 0.117\n",
      "(8, 600), Average_loss: 0.115\n",
      "(8, 700), Average_loss: 0.115\n",
      "(8, 800), Average_loss: 0.117\n",
      "(8, 900), Average_loss: 0.116\n",
      "(8, 1000), Average_loss: 0.113\n",
      "(8, 1100), Average_loss: 0.116\n",
      "(8, 1200), Average_loss: 0.113\n",
      "(8, 1300), Average_loss: 0.115\n",
      "(8, 1400), Average_loss: 0.110\n",
      "(8, 1500), Average_loss: 0.115\n",
      "(8, 1600), Average_loss: 0.112\n",
      "(8, 1700), Average_loss: 0.111\n",
      "(8, 1800), Average_loss: 0.113\n",
      "(8, 1900), Average_loss: 0.112\n",
      "(8, 2000), Average_loss: 0.117\n",
      "(8, 2100), Average_loss: 0.112\n",
      "(8, 2200), Average_loss: 0.115\n",
      "(8, 2300), Average_loss: 0.117\n",
      "(8, 2400), Average_loss: 0.110\n",
      "(8, 2500), Average_loss: 0.117\n",
      "(8, 2600), Average_loss: 0.114\n",
      "(8, 2700), Average_loss: 0.112\n",
      "(8, 2800), Average_loss: 0.113\n",
      "(8, 2900), Average_loss: 0.113\n",
      "(8, 3000), Average_loss: 0.110\n",
      "(8, 3100), Average_loss: 0.113\n",
      "(8, 3200), Average_loss: 0.111\n",
      "(8, 3300), Average_loss: 0.113\n",
      "(8, 3400), Average_loss: 0.114\n",
      "(8, 3500), Average_loss: 0.111\n",
      "(8, 3600), Average_loss: 0.113\n",
      "(8, 3700), Average_loss: 0.112\n",
      "(9, 100), Average_loss: 0.172\n",
      "(9, 200), Average_loss: 0.115\n",
      "(9, 300), Average_loss: 0.111\n",
      "(9, 400), Average_loss: 0.112\n",
      "(9, 500), Average_loss: 0.113\n",
      "(9, 600), Average_loss: 0.112\n",
      "(9, 700), Average_loss: 0.111\n",
      "(9, 800), Average_loss: 0.112\n",
      "(9, 900), Average_loss: 0.113\n",
      "(9, 1000), Average_loss: 0.111\n",
      "(9, 1100), Average_loss: 0.109\n",
      "(9, 1200), Average_loss: 0.111\n",
      "(9, 1300), Average_loss: 0.107\n",
      "(9, 1400), Average_loss: 0.114\n",
      "(9, 1500), Average_loss: 0.108\n",
      "(9, 1600), Average_loss: 0.113\n",
      "(9, 1700), Average_loss: 0.115\n",
      "(9, 1800), Average_loss: 0.112\n",
      "(9, 1900), Average_loss: 0.111\n",
      "(9, 2000), Average_loss: 0.112\n",
      "(9, 2100), Average_loss: 0.112\n",
      "(9, 2200), Average_loss: 0.114\n",
      "(9, 2300), Average_loss: 0.110\n",
      "(9, 2400), Average_loss: 0.111\n",
      "(9, 2500), Average_loss: 0.108\n",
      "(9, 2600), Average_loss: 0.108\n",
      "(9, 2700), Average_loss: 0.112\n",
      "(9, 2800), Average_loss: 0.112\n",
      "(9, 2900), Average_loss: 0.111\n",
      "(9, 3000), Average_loss: 0.109\n",
      "(9, 3100), Average_loss: 0.108\n",
      "(9, 3200), Average_loss: 0.107\n",
      "(9, 3300), Average_loss: 0.108\n",
      "(9, 3400), Average_loss: 0.108\n",
      "(9, 3500), Average_loss: 0.109\n",
      "(9, 3600), Average_loss: 0.111\n",
      "(9, 3700), Average_loss: 0.113\n",
      "(10, 100), Average_loss: 0.163\n",
      "(10, 200), Average_loss: 0.110\n",
      "(10, 300), Average_loss: 0.108\n",
      "(10, 400), Average_loss: 0.110\n",
      "(10, 500), Average_loss: 0.111\n",
      "(10, 600), Average_loss: 0.110\n",
      "(10, 700), Average_loss: 0.109\n",
      "(10, 800), Average_loss: 0.108\n",
      "(10, 900), Average_loss: 0.109\n",
      "(10, 1000), Average_loss: 0.108\n",
      "(10, 1100), Average_loss: 0.105\n",
      "(10, 1200), Average_loss: 0.107\n",
      "(10, 1300), Average_loss: 0.107\n",
      "(10, 1400), Average_loss: 0.107\n",
      "(10, 1500), Average_loss: 0.106\n",
      "(10, 1600), Average_loss: 0.108\n",
      "(10, 1700), Average_loss: 0.109\n",
      "(10, 1800), Average_loss: 0.107\n",
      "(10, 1900), Average_loss: 0.108\n",
      "(10, 2000), Average_loss: 0.106\n",
      "(10, 2100), Average_loss: 0.107\n",
      "(10, 2200), Average_loss: 0.109\n",
      "(10, 2300), Average_loss: 0.109\n",
      "(10, 2400), Average_loss: 0.109\n",
      "(10, 2500), Average_loss: 0.107\n",
      "(10, 2600), Average_loss: 0.112\n",
      "(10, 2700), Average_loss: 0.106\n",
      "(10, 2800), Average_loss: 0.106\n",
      "(10, 2900), Average_loss: 0.105\n",
      "(10, 3000), Average_loss: 0.108\n",
      "(10, 3100), Average_loss: 0.104\n",
      "(10, 3200), Average_loss: 0.105\n",
      "(10, 3300), Average_loss: 0.106\n",
      "(10, 3400), Average_loss: 0.108\n",
      "(10, 3500), Average_loss: 0.103\n",
      "(10, 3600), Average_loss: 0.106\n",
      "(10, 3700), Average_loss: 0.107\n",
      "(11, 100), Average_loss: 0.159\n",
      "(11, 200), Average_loss: 0.105\n",
      "(11, 300), Average_loss: 0.104\n",
      "(11, 400), Average_loss: 0.107\n",
      "(11, 500), Average_loss: 0.108\n",
      "(11, 600), Average_loss: 0.107\n",
      "(11, 700), Average_loss: 0.107\n",
      "(11, 800), Average_loss: 0.107\n",
      "(11, 900), Average_loss: 0.105\n",
      "(11, 1000), Average_loss: 0.106\n",
      "(11, 1100), Average_loss: 0.106\n",
      "(11, 1200), Average_loss: 0.107\n",
      "(11, 1300), Average_loss: 0.106\n",
      "(11, 1400), Average_loss: 0.109\n",
      "(11, 1500), Average_loss: 0.107\n",
      "(11, 1600), Average_loss: 0.106\n",
      "(11, 1700), Average_loss: 0.104\n",
      "(11, 1800), Average_loss: 0.104\n",
      "(11, 1900), Average_loss: 0.105\n",
      "(11, 2000), Average_loss: 0.104\n",
      "(11, 2100), Average_loss: 0.107\n",
      "(11, 2200), Average_loss: 0.108\n",
      "(11, 2300), Average_loss: 0.104\n",
      "(11, 2400), Average_loss: 0.107\n",
      "(11, 2500), Average_loss: 0.104\n",
      "(11, 2600), Average_loss: 0.107\n",
      "(11, 2700), Average_loss: 0.104\n",
      "(11, 2800), Average_loss: 0.101\n",
      "(11, 2900), Average_loss: 0.105\n",
      "(11, 3000), Average_loss: 0.104\n",
      "(11, 3100), Average_loss: 0.105\n",
      "(11, 3200), Average_loss: 0.104\n",
      "(11, 3300), Average_loss: 0.108\n",
      "(11, 3400), Average_loss: 0.108\n",
      "(11, 3500), Average_loss: 0.102\n",
      "(11, 3600), Average_loss: 0.105\n",
      "(11, 3700), Average_loss: 0.106\n",
      "(12, 100), Average_loss: 0.154\n",
      "(12, 200), Average_loss: 0.103\n",
      "(12, 300), Average_loss: 0.104\n",
      "(12, 400), Average_loss: 0.105\n",
      "(12, 500), Average_loss: 0.102\n",
      "(12, 600), Average_loss: 0.106\n",
      "(12, 700), Average_loss: 0.104\n",
      "(12, 800), Average_loss: 0.102\n",
      "(12, 900), Average_loss: 0.105\n",
      "(12, 1000), Average_loss: 0.103\n",
      "(12, 1100), Average_loss: 0.104\n",
      "(12, 1200), Average_loss: 0.104\n",
      "(12, 1300), Average_loss: 0.106\n",
      "(12, 1400), Average_loss: 0.106\n",
      "(12, 1500), Average_loss: 0.102\n",
      "(12, 1600), Average_loss: 0.105\n",
      "(12, 1700), Average_loss: 0.105\n",
      "(12, 1800), Average_loss: 0.104\n",
      "(12, 1900), Average_loss: 0.102\n",
      "(12, 2000), Average_loss: 0.101\n",
      "(12, 2100), Average_loss: 0.104\n",
      "(12, 2200), Average_loss: 0.103\n",
      "(12, 2300), Average_loss: 0.101\n",
      "(12, 2400), Average_loss: 0.102\n",
      "(12, 2500), Average_loss: 0.105\n",
      "(12, 2600), Average_loss: 0.103\n",
      "(12, 2700), Average_loss: 0.103\n",
      "(12, 2800), Average_loss: 0.102\n",
      "(12, 2900), Average_loss: 0.103\n",
      "(12, 3000), Average_loss: 0.105\n",
      "(12, 3100), Average_loss: 0.103\n",
      "(12, 3200), Average_loss: 0.102\n",
      "(12, 3300), Average_loss: 0.102\n",
      "(12, 3400), Average_loss: 0.103\n",
      "(12, 3500), Average_loss: 0.102\n",
      "(12, 3600), Average_loss: 0.103\n",
      "(12, 3700), Average_loss: 0.106\n",
      "(13, 100), Average_loss: 0.154\n",
      "(13, 200), Average_loss: 0.100\n",
      "(13, 300), Average_loss: 0.104\n",
      "(13, 400), Average_loss: 0.101\n",
      "(13, 500), Average_loss: 0.100\n",
      "(13, 600), Average_loss: 0.099\n",
      "(13, 700), Average_loss: 0.102\n",
      "(13, 800), Average_loss: 0.103\n",
      "(13, 900), Average_loss: 0.101\n",
      "(13, 1000), Average_loss: 0.101\n",
      "(13, 1100), Average_loss: 0.102\n",
      "(13, 1200), Average_loss: 0.102\n",
      "(13, 1300), Average_loss: 0.104\n",
      "(13, 1400), Average_loss: 0.100\n",
      "(13, 1500), Average_loss: 0.100\n",
      "(13, 1600), Average_loss: 0.100\n",
      "(13, 1700), Average_loss: 0.104\n",
      "(13, 1800), Average_loss: 0.101\n",
      "(13, 1900), Average_loss: 0.103\n",
      "(13, 2000), Average_loss: 0.100\n",
      "(13, 2100), Average_loss: 0.103\n",
      "(13, 2200), Average_loss: 0.101\n",
      "(13, 2300), Average_loss: 0.103\n",
      "(13, 2400), Average_loss: 0.100\n",
      "(13, 2500), Average_loss: 0.100\n",
      "(13, 2600), Average_loss: 0.100\n",
      "(13, 2700), Average_loss: 0.103\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     12\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     15\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m99\u001b[39m:\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\optim\\adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    158\u001b[0m         group,\n\u001b[0;32m    159\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    164\u001b[0m         state_steps)\n\u001b[1;32m--> 166\u001b[0m     adam(\n\u001b[0;32m    167\u001b[0m         params_with_grad,\n\u001b[0;32m    168\u001b[0m         grads,\n\u001b[0;32m    169\u001b[0m         exp_avgs,\n\u001b[0;32m    170\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    171\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    172\u001b[0m         state_steps,\n\u001b[0;32m    173\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    174\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    175\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    176\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    177\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    178\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    179\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    180\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    181\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    182\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    183\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    184\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    185\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    186\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    187\u001b[0m     )\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\optim\\adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 316\u001b[0m func(params,\n\u001b[0;32m    317\u001b[0m      grads,\n\u001b[0;32m    318\u001b[0m      exp_avgs,\n\u001b[0;32m    319\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    320\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    321\u001b[0m      state_steps,\n\u001b[0;32m    322\u001b[0m      amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m    323\u001b[0m      has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    324\u001b[0m      beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    325\u001b[0m      beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    326\u001b[0m      lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m    327\u001b[0m      weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[0;32m    328\u001b[0m      eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m    329\u001b[0m      maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[0;32m    330\u001b[0m      capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m    331\u001b[0m      differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[0;32m    332\u001b[0m      grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[0;32m    333\u001b[0m      found_inf\u001b[38;5;241m=\u001b[39mfound_inf)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\optim\\adam.py:508\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;66;03m# Update steps\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# If steps are on CPU, foreach will fall back to the slow path, which is a for-loop calling t.add(1) over\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# and over. 1 will then be wrapped into a Tensor over and over again, which is slower than if we just\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# wrapped it once now. The alpha is required to assure we go to the right overload.\u001b[39;00m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_state_steps[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mis_cpu:\n\u001b[1;32m--> 508\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_add_(device_state_steps, torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1.0\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m), alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_add_(device_state_steps, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for e, data in enumerate(train_loader):\n",
    "        x0, _ = data\n",
    "        x0 = x0.to(device)\n",
    "        t = torch.randint(1, T+1, (batch_size,)).to(device)\n",
    "        eps = torch.randn(batch_size, 1, 28, 28).to(device)\n",
    "        # print(eps.shape)\n",
    "        # print(x0.shape)\n",
    "        xt = torch.sqrt(alpha_bar[t-1]) * x0 + torch.sqrt(1 - alpha_bar[t-1]) * eps\n",
    "        loss = criterion(eps, model(xt, t.view(batch_size, 1, 1, 1).expand(batch_size, 1, 28, 28)))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if e % 100 == 99:\n",
    "            print(f'{epoch, e+1}, Average_loss: {running_loss/100:.3f}')\n",
    "            running_loss = 0.0\n",
    "    if epoch % 10 == 9:\n",
    "        torch.save(model.state_dict(), f\"DDPM_{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54a74a6-6786-43c8-9b7b-569eb01df2f2",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "999697a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"DDPM_10.pth\",map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53f69f28-7031-48bc-96d4-85b61ba8d524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2636742c850>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    model.eval()\n",
    "    batch_size_sample = 1\n",
    "    xt = torch.randn(batch_size_sample, 1, 28, 28).to(device)\n",
    "\n",
    "    for t in torch.arange(T, 0, -1):\n",
    "        # print(t)\n",
    "        t = t.to(device)\n",
    "        z = torch.randn(batch_size_sample, 1, 28, 28).to(device) if t > 1 else torch.zeros(batch_size_sample, 1, 28, 28).to(device)\n",
    "        xt_new = 1 / torch.sqrt(alpha[t - 1]) * (xt - (1 - alpha[t - 1])/(torch.sqrt(1 - alpha_bar[t - 1])) * \n",
    "                                                    model(xt, t.view(batch_size_sample, 1, 1, 1).expand(batch_size_sample, 1, 28, 28))) + torch.sqrt(beta[t-1]) * z\n",
    "        xt = xt_new\n",
    "plt.imshow(xt[0][0].cpu().detach().numpy(), cmap=\"grey\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6bf278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from all the timesteps\n",
    "fig, axs = plt.subplots(1, 5, tight_layout=True, figsize=(40, 20))\n",
    "tens = np.arange(10,51,10)\n",
    "seed = 3546\n",
    "for ten in tens:\n",
    "    with torch.inference_mode():\n",
    "        # torch.manual_seed(seed)\n",
    "        model.load_state_dict(torch.load(f\"Data_cosnoise_normalized/DDPM_{ten}.pth\", map_location=device))\n",
    "        model.eval()\n",
    "        batch_size_sample = 1\n",
    "        xt = torch.randn(batch_size_sample, 1, 28, 28).to(device)\n",
    "\n",
    "        for t in torch.arange(T, 0, -1):\n",
    "            t = t.to(device)\n",
    "            # print(t)\n",
    "            z = torch.randn(batch_size_sample, 1, 28, 28).to(device) if t > 1 else torch.zeros(batch_size_sample, 1, 28, 28).to(device)\n",
    "            xt_new = 1 / torch.sqrt(alpha[t - 1]) * (xt - (1 - alpha[t - 1])/(torch.sqrt(1 - alpha_bar[t - 1])) * \n",
    "                                                        model(xt, t.view(batch_size_sample, 1, 1, 1).expand(batch_size_sample, 1, 28, 28))) + torch.sqrt(beta[t-1]) * z\n",
    "            xt = xt_new\n",
    "            axs[(ten//10)-1].imshow(xt[0][0].cpu().detach().numpy(), cmap=\"gray\")\n",
    "            axs[(ten//10)-1].axis(\"off\")\n",
    "            axs[(ten//10)-1].set_title(f\"Epoch={ten}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0293c70c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
