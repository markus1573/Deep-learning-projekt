{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "def4f85a-37b5-472e-8782-a5df13c5d601",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec1af4-1d61-46ef-90b9-b972129d09a5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eeb867f-c38e-4e5c-ab89-a7e747ead472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "017d87cd-410b-44ef-ac34-560022867208",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "#torch.backends.cudnn.enabled = False\n",
    "val_size = 5000\n",
    "test_size = 5000\n",
    "batch_size = 16\n",
    "num_workers = 4\n",
    "pin_memory = False if device == torch.device('cpu') else True\n",
    "\n",
    "# transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "# Downloading MNIST again :) Training (60k) and test(5k) + val(5k) split\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('./mnist_data',\n",
    "                                            download=True,\n",
    "                                            train=True,\n",
    "                                            transform=transform),\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=num_workers,\n",
    "                                            pin_memory=pin_memory,\n",
    "                                            drop_last=True)\n",
    "\n",
    "test_dataset = datasets.MNIST('./mnist_data',\n",
    "                               download=True,\n",
    "                               train=False,\n",
    "                               transform=transform)\n",
    "\n",
    "val_dataset, test_dataset = random_split(test_dataset, [val_size, test_size])\n",
    "\n",
    "# Test set to compare with DDPM paper\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=num_workers,\n",
    "                                            pin_memory=pin_memory)\n",
    "\n",
    "# Validation set so we can keep track of approximated FID score while training\n",
    "validation_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=num_workers,\n",
    "                                            pin_memory=pin_memory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0ae1fca-cff3-4b53-86d1-65c59740a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cosine noise schedule\n",
    "def f(t, s=torch.tensor([0.008]), T=torch.tensor([1000])):\n",
    "    return min(torch.cos((t / T + s) / (1 + s) * (torch.pi / 2)).pow(2), 0.999)\n",
    "\n",
    "T = 1000\n",
    "ts = torch.arange(T)\n",
    "alpha_bar = torch.tensor([min(f(t)/f(torch.tensor([0])),0.999) for t in ts]) \n",
    "beta = torch.tensor([1 - alpha_bar[t]/(alpha_bar[t-1]) if t > 0 else torch.tensor([0]) for t in ts])\n",
    "alpha = 1 - beta\n",
    "alpha = alpha.view((1000, 1, 1, 1)).to(device)\n",
    "beta = beta.view((1000, 1, 1, 1)).to(device)\n",
    "alpha_bar = alpha_bar.view((1000, 1, 1, 1)).to(device)\n",
    "\n",
    "\n",
    "# # Linear noise schedule\n",
    "# T = 1000\n",
    "# beta_start, beta_end = 1e-4, 2e-2\n",
    "# beta = torch.linspace(beta_start, beta_end, T)  # Linear noise schedule\n",
    "# alpha = 1.0 - beta\n",
    "# alpha_bar = torch.cumprod(alpha, dim=0)  # Cumulative product for alpha_bar\n",
    "\n",
    "# # Reshape for broadcasting (if required for your model)\n",
    "# alpha = alpha.view((T, 1, 1, 1)).to(device)\n",
    "# beta = beta.view((T, 1, 1, 1)).to(device)\n",
    "# alpha_bar = alpha_bar.view((T, 1, 1, 1)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93cbbcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c42fe879-ee13-4434-9a0d-de015ddd4c2f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c671dbab-ffe9-4fe5-841a-80042e7593b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNET, self).__init__()\n",
    "        channels = [32, 64, 128, 256]\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(2, channels[0], kernel_size=3, padding=1),  # (batchsize, 32, 28, 28)\n",
    "                nn.GroupNorm(4, channels[0]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2),  # (batchsize, 32, 14, 14)\n",
    "                nn.Conv2d(channels[0], channels[1], kernel_size=3, padding=1),  # (batchsize, 64, 14, 14)\n",
    "                nn.GroupNorm(4, channels[1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2),  # (batchsize, 64, 7, 7)\n",
    "                nn.Conv2d(channels[1], channels[2], kernel_size=3, padding=1),  # (batchsize, 128, 7, 7)\n",
    "                nn.GroupNorm(8, channels[2]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.3)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2, padding=1),  # (batchsize, 128, 4, 4)\n",
    "                nn.Conv2d(channels[2], channels[3], kernel_size=3, padding=1),  # (batchsize, 256, 4, 4)\n",
    "                nn.GroupNorm(8, channels[3]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.3)\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        self.tconvs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(channels[3], channels[2], kernel_size=3, \n",
    "                                   stride=2, padding=1, output_padding=0),   # (batchsize, 128, 7, 7)\n",
    "                nn.GroupNorm(8, channels[2]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.3)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(channels[2]*2, channels[1], kernel_size=3,\n",
    "                                   stride=2, padding=1, output_padding=1),   # (batchsize, 64, 14, 14)\n",
    "                nn.GroupNorm(8, channels[1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(channels[1]*2, channels[0], kernel_size=3, \n",
    "                                   stride=2, padding=1, output_padding=1),   # (batchsize, 32, 28, 28)\n",
    "                nn.GroupNorm(4, channels[0]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(channels[0]*2,channels[0],kernel_size=3,padding=1),  # (batchsize, 32, 28, 28)\n",
    "                nn.GroupNorm(4, channels[0]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2),\n",
    "                nn.Conv2d(channels[0],1,kernel_size=1) # (batchsize, 1, 28, 28)\n",
    "            )      \n",
    "        ])\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x_trans = torch.cat((x, t), dim=-3)\n",
    "        signal = x_trans\n",
    "        signals = []\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "            # print(f\"conv {i}\")\n",
    "            signal = conv(signal)\n",
    "            # print(signal.shape)\n",
    "            if i < len(self.convs)-1:\n",
    "                signals.append(signal)\n",
    "                \n",
    "        for i, tconv in enumerate(self.tconvs):\n",
    "            # print(f\"tconv {i}\")\n",
    "            # print(f\"signal shape: {signal.shape}\")\n",
    "            if i == 0:\n",
    "                signal = tconv(signal)\n",
    "            else:\n",
    "                signal = torch.cat((signal, signals[-i]), dim=-3)\n",
    "                signal = tconv(signal)\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f04a5df-da69-407e-9285-90de11992cef",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8403ab0-aea6-492a-98b9-31ab55064c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from UNET import UNET\n",
    "epochs = 20\n",
    "model = UNET()\n",
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dc7e14a-63cd-4432-8f07-43fd00841fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 100), Average_loss: 1.003\n",
      "(0, 200), Average_loss: 0.999\n",
      "(0, 300), Average_loss: 0.998\n",
      "(0, 400), Average_loss: 1.001\n",
      "(0, 500), Average_loss: 0.918\n",
      "(0, 600), Average_loss: 0.509\n",
      "(0, 700), Average_loss: 0.346\n",
      "(0, 800), Average_loss: 0.296\n",
      "(0, 900), Average_loss: 0.269\n",
      "(0, 1000), Average_loss: 0.237\n",
      "(0, 1100), Average_loss: 0.211\n",
      "(0, 1200), Average_loss: 0.201\n",
      "(0, 1300), Average_loss: 0.191\n",
      "(0, 1400), Average_loss: 0.184\n",
      "(0, 1500), Average_loss: 0.183\n",
      "(0, 1600), Average_loss: 0.168\n",
      "(0, 1700), Average_loss: 0.167\n",
      "(0, 1800), Average_loss: 0.163\n",
      "(0, 1900), Average_loss: 0.161\n",
      "(0, 2000), Average_loss: 0.160\n",
      "(0, 2100), Average_loss: 0.159\n",
      "(0, 2200), Average_loss: 0.157\n",
      "(0, 2300), Average_loss: 0.151\n",
      "(0, 2400), Average_loss: 0.154\n",
      "(0, 2500), Average_loss: 0.152\n",
      "(0, 2600), Average_loss: 0.146\n",
      "(0, 2700), Average_loss: 0.147\n",
      "(0, 2800), Average_loss: 0.149\n",
      "(0, 2900), Average_loss: 0.145\n",
      "(0, 3000), Average_loss: 0.141\n",
      "(0, 3100), Average_loss: 0.141\n",
      "(0, 3200), Average_loss: 0.139\n",
      "(0, 3300), Average_loss: 0.134\n",
      "(0, 3400), Average_loss: 0.138\n",
      "(0, 3500), Average_loss: 0.137\n",
      "(0, 3600), Average_loss: 0.133\n",
      "(0, 3700), Average_loss: 0.132\n",
      "(1, 100), Average_loss: 0.198\n",
      "(1, 200), Average_loss: 0.127\n",
      "(1, 300), Average_loss: 0.130\n",
      "(1, 400), Average_loss: 0.128\n",
      "(1, 500), Average_loss: 0.129\n",
      "(1, 600), Average_loss: 0.126\n",
      "(1, 700), Average_loss: 0.126\n",
      "(1, 800), Average_loss: 0.123\n",
      "(1, 900), Average_loss: 0.125\n",
      "(1, 1000), Average_loss: 0.126\n",
      "(1, 1100), Average_loss: 0.122\n",
      "(1, 1200), Average_loss: 0.122\n",
      "(1, 1300), Average_loss: 0.121\n",
      "(1, 1400), Average_loss: 0.124\n",
      "(1, 1500), Average_loss: 0.119\n",
      "(1, 1600), Average_loss: 0.125\n",
      "(1, 1700), Average_loss: 0.120\n",
      "(1, 1800), Average_loss: 0.119\n",
      "(1, 1900), Average_loss: 0.118\n",
      "(1, 2000), Average_loss: 0.120\n",
      "(1, 2100), Average_loss: 0.121\n",
      "(1, 2200), Average_loss: 0.117\n",
      "(1, 2300), Average_loss: 0.119\n",
      "(1, 2400), Average_loss: 0.114\n",
      "(1, 2500), Average_loss: 0.115\n",
      "(1, 2600), Average_loss: 0.115\n",
      "(1, 2700), Average_loss: 0.120\n",
      "(1, 2800), Average_loss: 0.115\n",
      "(1, 2900), Average_loss: 0.115\n",
      "(1, 3000), Average_loss: 0.114\n",
      "(1, 3100), Average_loss: 0.111\n",
      "(1, 3200), Average_loss: 0.113\n",
      "(1, 3300), Average_loss: 0.115\n",
      "(1, 3400), Average_loss: 0.115\n",
      "(1, 3500), Average_loss: 0.116\n",
      "(1, 3600), Average_loss: 0.111\n",
      "(1, 3700), Average_loss: 0.113\n",
      "(2, 100), Average_loss: 0.165\n",
      "(2, 200), Average_loss: 0.114\n",
      "(2, 300), Average_loss: 0.112\n",
      "(2, 400), Average_loss: 0.110\n",
      "(2, 500), Average_loss: 0.113\n",
      "(2, 600), Average_loss: 0.112\n",
      "(2, 700), Average_loss: 0.112\n",
      "(2, 800), Average_loss: 0.108\n",
      "(2, 900), Average_loss: 0.110\n",
      "(2, 1000), Average_loss: 0.110\n",
      "(2, 1100), Average_loss: 0.111\n",
      "(2, 1200), Average_loss: 0.106\n",
      "(2, 1300), Average_loss: 0.110\n",
      "(2, 1400), Average_loss: 0.109\n",
      "(2, 1500), Average_loss: 0.107\n",
      "(2, 1600), Average_loss: 0.112\n",
      "(2, 1700), Average_loss: 0.109\n",
      "(2, 1800), Average_loss: 0.104\n",
      "(2, 1900), Average_loss: 0.108\n",
      "(2, 2000), Average_loss: 0.107\n",
      "(2, 2100), Average_loss: 0.103\n",
      "(2, 2200), Average_loss: 0.106\n",
      "(2, 2300), Average_loss: 0.106\n",
      "(2, 2400), Average_loss: 0.105\n",
      "(2, 2500), Average_loss: 0.105\n",
      "(2, 2600), Average_loss: 0.105\n",
      "(2, 2700), Average_loss: 0.105\n",
      "(2, 2800), Average_loss: 0.106\n",
      "(2, 2900), Average_loss: 0.102\n",
      "(2, 3000), Average_loss: 0.104\n",
      "(2, 3100), Average_loss: 0.106\n",
      "(2, 3200), Average_loss: 0.103\n",
      "(2, 3300), Average_loss: 0.103\n",
      "(2, 3400), Average_loss: 0.103\n",
      "(2, 3500), Average_loss: 0.106\n",
      "(2, 3600), Average_loss: 0.106\n",
      "(2, 3700), Average_loss: 0.104\n",
      "(3, 100), Average_loss: 0.155\n",
      "(3, 200), Average_loss: 0.104\n",
      "(3, 300), Average_loss: 0.103\n",
      "(3, 400), Average_loss: 0.101\n",
      "(3, 500), Average_loss: 0.104\n",
      "(3, 600), Average_loss: 0.102\n",
      "(3, 700), Average_loss: 0.100\n",
      "(3, 800), Average_loss: 0.103\n",
      "(3, 900), Average_loss: 0.101\n",
      "(3, 1000), Average_loss: 0.102\n",
      "(3, 1100), Average_loss: 0.101\n",
      "(3, 1200), Average_loss: 0.100\n",
      "(3, 1300), Average_loss: 0.101\n",
      "(3, 1400), Average_loss: 0.103\n",
      "(3, 1500), Average_loss: 0.099\n",
      "(3, 1600), Average_loss: 0.101\n",
      "(3, 1700), Average_loss: 0.101\n",
      "(3, 1800), Average_loss: 0.101\n",
      "(3, 1900), Average_loss: 0.103\n",
      "(3, 2000), Average_loss: 0.100\n",
      "(3, 2100), Average_loss: 0.103\n",
      "(3, 2200), Average_loss: 0.100\n",
      "(3, 2300), Average_loss: 0.098\n",
      "(3, 2400), Average_loss: 0.098\n",
      "(3, 2500), Average_loss: 0.100\n",
      "(3, 2600), Average_loss: 0.098\n",
      "(3, 2700), Average_loss: 0.098\n",
      "(3, 2800), Average_loss: 0.099\n",
      "(3, 2900), Average_loss: 0.099\n",
      "(3, 3000), Average_loss: 0.098\n",
      "(3, 3100), Average_loss: 0.099\n",
      "(3, 3200), Average_loss: 0.098\n",
      "(3, 3300), Average_loss: 0.098\n",
      "(3, 3400), Average_loss: 0.102\n",
      "(3, 3500), Average_loss: 0.098\n",
      "(3, 3600), Average_loss: 0.102\n",
      "(3, 3700), Average_loss: 0.095\n",
      "(4, 100), Average_loss: 0.149\n",
      "(4, 200), Average_loss: 0.101\n",
      "(4, 300), Average_loss: 0.096\n",
      "(4, 400), Average_loss: 0.097\n",
      "(4, 500), Average_loss: 0.097\n",
      "(4, 600), Average_loss: 0.098\n",
      "(4, 700), Average_loss: 0.100\n",
      "(4, 800), Average_loss: 0.099\n",
      "(4, 900), Average_loss: 0.098\n",
      "(4, 1000), Average_loss: 0.099\n",
      "(4, 1100), Average_loss: 0.096\n",
      "(4, 1200), Average_loss: 0.100\n",
      "(4, 1300), Average_loss: 0.097\n",
      "(4, 1400), Average_loss: 0.097\n",
      "(4, 1500), Average_loss: 0.101\n",
      "(4, 1600), Average_loss: 0.095\n",
      "(4, 1700), Average_loss: 0.100\n",
      "(4, 1800), Average_loss: 0.097\n",
      "(4, 1900), Average_loss: 0.098\n",
      "(4, 2000), Average_loss: 0.099\n",
      "(4, 2100), Average_loss: 0.095\n",
      "(4, 2200), Average_loss: 0.097\n",
      "(4, 2300), Average_loss: 0.097\n",
      "(4, 2400), Average_loss: 0.096\n",
      "(4, 2500), Average_loss: 0.092\n",
      "(4, 2600), Average_loss: 0.095\n",
      "(4, 2700), Average_loss: 0.094\n",
      "(4, 2800), Average_loss: 0.095\n",
      "(4, 2900), Average_loss: 0.098\n",
      "(4, 3000), Average_loss: 0.094\n",
      "(4, 3100), Average_loss: 0.096\n",
      "(4, 3200), Average_loss: 0.098\n",
      "(4, 3300), Average_loss: 0.097\n",
      "(4, 3400), Average_loss: 0.096\n",
      "(4, 3500), Average_loss: 0.095\n",
      "(4, 3600), Average_loss: 0.094\n",
      "(4, 3700), Average_loss: 0.095\n",
      "(5, 100), Average_loss: 0.143\n",
      "(5, 200), Average_loss: 0.097\n",
      "(5, 300), Average_loss: 0.097\n",
      "(5, 400), Average_loss: 0.093\n",
      "(5, 500), Average_loss: 0.098\n",
      "(5, 600), Average_loss: 0.095\n",
      "(5, 700), Average_loss: 0.094\n",
      "(5, 800), Average_loss: 0.096\n",
      "(5, 900), Average_loss: 0.093\n",
      "(5, 1000), Average_loss: 0.093\n",
      "(5, 1100), Average_loss: 0.094\n",
      "(5, 1200), Average_loss: 0.092\n",
      "(5, 1300), Average_loss: 0.096\n",
      "(5, 1400), Average_loss: 0.094\n",
      "(5, 1500), Average_loss: 0.095\n",
      "(5, 1600), Average_loss: 0.095\n",
      "(5, 1700), Average_loss: 0.096\n",
      "(5, 1800), Average_loss: 0.092\n",
      "(5, 1900), Average_loss: 0.092\n",
      "(5, 2000), Average_loss: 0.092\n",
      "(5, 2100), Average_loss: 0.092\n",
      "(5, 2200), Average_loss: 0.094\n",
      "(5, 2300), Average_loss: 0.092\n",
      "(5, 2400), Average_loss: 0.094\n",
      "(5, 2500), Average_loss: 0.094\n",
      "(5, 2600), Average_loss: 0.092\n",
      "(5, 2700), Average_loss: 0.095\n",
      "(5, 2800), Average_loss: 0.093\n",
      "(5, 2900), Average_loss: 0.095\n",
      "(5, 3000), Average_loss: 0.095\n",
      "(5, 3100), Average_loss: 0.092\n",
      "(5, 3200), Average_loss: 0.096\n",
      "(5, 3300), Average_loss: 0.095\n",
      "(5, 3400), Average_loss: 0.097\n",
      "(5, 3500), Average_loss: 0.092\n",
      "(5, 3600), Average_loss: 0.092\n",
      "(5, 3700), Average_loss: 0.093\n",
      "(6, 100), Average_loss: 0.141\n",
      "(6, 200), Average_loss: 0.092\n",
      "(6, 300), Average_loss: 0.092\n",
      "(6, 400), Average_loss: 0.090\n",
      "(6, 500), Average_loss: 0.092\n",
      "(6, 600), Average_loss: 0.097\n",
      "(6, 700), Average_loss: 0.095\n",
      "(6, 800), Average_loss: 0.089\n",
      "(6, 900), Average_loss: 0.092\n",
      "(6, 1000), Average_loss: 0.090\n",
      "(6, 1100), Average_loss: 0.094\n",
      "(6, 1200), Average_loss: 0.091\n",
      "(6, 1300), Average_loss: 0.092\n",
      "(6, 1400), Average_loss: 0.091\n",
      "(6, 1500), Average_loss: 0.092\n",
      "(6, 1600), Average_loss: 0.094\n",
      "(6, 1700), Average_loss: 0.093\n",
      "(6, 1800), Average_loss: 0.092\n",
      "(6, 1900), Average_loss: 0.093\n",
      "(6, 2000), Average_loss: 0.092\n",
      "(6, 2100), Average_loss: 0.090\n",
      "(6, 2200), Average_loss: 0.093\n",
      "(6, 2300), Average_loss: 0.092\n",
      "(6, 2400), Average_loss: 0.092\n",
      "(6, 2500), Average_loss: 0.091\n",
      "(6, 2600), Average_loss: 0.090\n",
      "(6, 2700), Average_loss: 0.092\n",
      "(6, 2800), Average_loss: 0.092\n",
      "(6, 2900), Average_loss: 0.092\n",
      "(6, 3000), Average_loss: 0.092\n",
      "(6, 3100), Average_loss: 0.092\n",
      "(6, 3200), Average_loss: 0.091\n",
      "(6, 3300), Average_loss: 0.092\n",
      "(6, 3400), Average_loss: 0.093\n",
      "(6, 3500), Average_loss: 0.092\n",
      "(6, 3600), Average_loss: 0.089\n",
      "(6, 3700), Average_loss: 0.091\n",
      "(7, 100), Average_loss: 0.139\n",
      "(7, 200), Average_loss: 0.093\n",
      "(7, 300), Average_loss: 0.090\n",
      "(7, 400), Average_loss: 0.090\n",
      "(7, 500), Average_loss: 0.091\n",
      "(7, 600), Average_loss: 0.091\n",
      "(7, 700), Average_loss: 0.089\n",
      "(7, 800), Average_loss: 0.092\n",
      "(7, 900), Average_loss: 0.090\n",
      "(7, 1000), Average_loss: 0.091\n",
      "(7, 1100), Average_loss: 0.087\n",
      "(7, 1200), Average_loss: 0.091\n",
      "(7, 1300), Average_loss: 0.092\n",
      "(7, 1400), Average_loss: 0.091\n",
      "(7, 1500), Average_loss: 0.093\n",
      "(7, 1600), Average_loss: 0.091\n",
      "(7, 1700), Average_loss: 0.091\n",
      "(7, 1800), Average_loss: 0.089\n",
      "(7, 1900), Average_loss: 0.092\n",
      "(7, 2000), Average_loss: 0.091\n",
      "(7, 2100), Average_loss: 0.090\n",
      "(7, 2200), Average_loss: 0.089\n",
      "(7, 2300), Average_loss: 0.090\n",
      "(7, 2400), Average_loss: 0.093\n",
      "(7, 2500), Average_loss: 0.089\n",
      "(7, 2600), Average_loss: 0.092\n",
      "(7, 2700), Average_loss: 0.092\n",
      "(7, 2800), Average_loss: 0.092\n",
      "(7, 2900), Average_loss: 0.089\n",
      "(7, 3000), Average_loss: 0.088\n",
      "(7, 3100), Average_loss: 0.089\n",
      "(7, 3200), Average_loss: 0.091\n",
      "(7, 3300), Average_loss: 0.091\n",
      "(7, 3400), Average_loss: 0.091\n",
      "(7, 3500), Average_loss: 0.093\n",
      "(7, 3600), Average_loss: 0.091\n",
      "(7, 3700), Average_loss: 0.089\n",
      "(8, 100), Average_loss: 0.135\n",
      "(8, 200), Average_loss: 0.090\n",
      "(8, 300), Average_loss: 0.089\n",
      "(8, 400), Average_loss: 0.089\n",
      "(8, 500), Average_loss: 0.089\n",
      "(8, 600), Average_loss: 0.091\n",
      "(8, 700), Average_loss: 0.090\n",
      "(8, 800), Average_loss: 0.089\n",
      "(8, 900), Average_loss: 0.090\n",
      "(8, 1000), Average_loss: 0.089\n",
      "(8, 1100), Average_loss: 0.091\n",
      "(8, 1200), Average_loss: 0.088\n",
      "(8, 1300), Average_loss: 0.091\n",
      "(8, 1400), Average_loss: 0.088\n",
      "(8, 1500), Average_loss: 0.092\n",
      "(8, 1600), Average_loss: 0.089\n",
      "(8, 1700), Average_loss: 0.090\n",
      "(8, 1800), Average_loss: 0.089\n",
      "(8, 1900), Average_loss: 0.091\n",
      "(8, 2000), Average_loss: 0.093\n",
      "(8, 2100), Average_loss: 0.087\n",
      "(8, 2200), Average_loss: 0.090\n",
      "(8, 2300), Average_loss: 0.090\n",
      "(8, 2400), Average_loss: 0.093\n",
      "(8, 2500), Average_loss: 0.090\n",
      "(8, 2600), Average_loss: 0.090\n",
      "(8, 2700), Average_loss: 0.090\n",
      "(8, 2800), Average_loss: 0.088\n",
      "(8, 2900), Average_loss: 0.089\n",
      "(8, 3000), Average_loss: 0.088\n",
      "(8, 3100), Average_loss: 0.089\n",
      "(8, 3200), Average_loss: 0.086\n",
      "(8, 3300), Average_loss: 0.088\n",
      "(8, 3400), Average_loss: 0.090\n",
      "(8, 3500), Average_loss: 0.088\n",
      "(8, 3600), Average_loss: 0.090\n",
      "(8, 3700), Average_loss: 0.089\n",
      "(9, 100), Average_loss: 0.129\n",
      "(9, 200), Average_loss: 0.087\n",
      "(9, 300), Average_loss: 0.086\n",
      "(9, 400), Average_loss: 0.089\n",
      "(9, 500), Average_loss: 0.089\n",
      "(9, 600), Average_loss: 0.091\n",
      "(9, 700), Average_loss: 0.090\n",
      "(9, 800), Average_loss: 0.089\n",
      "(9, 900), Average_loss: 0.090\n",
      "(9, 1000), Average_loss: 0.090\n",
      "(9, 1100), Average_loss: 0.088\n",
      "(9, 1200), Average_loss: 0.089\n",
      "(9, 1300), Average_loss: 0.087\n",
      "(9, 1400), Average_loss: 0.091\n",
      "(9, 1500), Average_loss: 0.089\n",
      "(9, 1600), Average_loss: 0.089\n",
      "(9, 1700), Average_loss: 0.088\n",
      "(9, 1800), Average_loss: 0.088\n",
      "(9, 1900), Average_loss: 0.087\n",
      "(9, 2000), Average_loss: 0.087\n",
      "(9, 2100), Average_loss: 0.086\n",
      "(9, 2200), Average_loss: 0.090\n",
      "(9, 2300), Average_loss: 0.088\n",
      "(9, 2400), Average_loss: 0.085\n",
      "(9, 2500), Average_loss: 0.087\n",
      "(9, 2600), Average_loss: 0.087\n",
      "(9, 2700), Average_loss: 0.088\n",
      "(9, 2800), Average_loss: 0.090\n",
      "(9, 2900), Average_loss: 0.088\n",
      "(9, 3000), Average_loss: 0.090\n",
      "(9, 3100), Average_loss: 0.085\n",
      "(9, 3200), Average_loss: 0.086\n",
      "(9, 3300), Average_loss: 0.089\n",
      "(9, 3400), Average_loss: 0.090\n",
      "(9, 3500), Average_loss: 0.088\n",
      "(9, 3600), Average_loss: 0.090\n",
      "(9, 3700), Average_loss: 0.091\n",
      "(10, 100), Average_loss: 0.133\n",
      "(10, 200), Average_loss: 0.089\n",
      "(10, 300), Average_loss: 0.088\n",
      "(10, 400), Average_loss: 0.087\n",
      "(10, 500), Average_loss: 0.085\n",
      "(10, 600), Average_loss: 0.086\n",
      "(10, 700), Average_loss: 0.088\n",
      "(10, 800), Average_loss: 0.086\n",
      "(10, 900), Average_loss: 0.088\n",
      "(10, 1000), Average_loss: 0.089\n",
      "(10, 1100), Average_loss: 0.091\n",
      "(10, 1200), Average_loss: 0.090\n",
      "(10, 1300), Average_loss: 0.087\n",
      "(10, 1400), Average_loss: 0.087\n",
      "(10, 1500), Average_loss: 0.087\n",
      "(10, 1600), Average_loss: 0.089\n",
      "(10, 1700), Average_loss: 0.090\n",
      "(10, 1800), Average_loss: 0.088\n",
      "(10, 1900), Average_loss: 0.089\n",
      "(10, 2000), Average_loss: 0.086\n",
      "(10, 2100), Average_loss: 0.089\n",
      "(10, 2200), Average_loss: 0.086\n",
      "(10, 2300), Average_loss: 0.087\n",
      "(10, 2400), Average_loss: 0.088\n",
      "(10, 2500), Average_loss: 0.089\n",
      "(10, 2600), Average_loss: 0.087\n",
      "(10, 2700), Average_loss: 0.086\n",
      "(10, 2800), Average_loss: 0.085\n",
      "(10, 2900), Average_loss: 0.086\n",
      "(10, 3000), Average_loss: 0.088\n",
      "(10, 3100), Average_loss: 0.086\n",
      "(10, 3200), Average_loss: 0.087\n",
      "(10, 3300), Average_loss: 0.087\n",
      "(10, 3400), Average_loss: 0.085\n",
      "(10, 3500), Average_loss: 0.086\n",
      "(10, 3600), Average_loss: 0.088\n",
      "(10, 3700), Average_loss: 0.085\n",
      "(11, 100), Average_loss: 0.130\n",
      "(11, 200), Average_loss: 0.087\n",
      "(11, 300), Average_loss: 0.088\n",
      "(11, 400), Average_loss: 0.086\n",
      "(11, 500), Average_loss: 0.086\n",
      "(11, 600), Average_loss: 0.088\n",
      "(11, 700), Average_loss: 0.087\n",
      "(11, 800), Average_loss: 0.089\n",
      "(11, 900), Average_loss: 0.084\n",
      "(11, 1000), Average_loss: 0.089\n",
      "(11, 1100), Average_loss: 0.087\n",
      "(11, 1200), Average_loss: 0.085\n",
      "(11, 1300), Average_loss: 0.084\n",
      "(11, 1400), Average_loss: 0.087\n",
      "(11, 1500), Average_loss: 0.084\n",
      "(11, 1600), Average_loss: 0.087\n",
      "(11, 1700), Average_loss: 0.087\n",
      "(11, 1800), Average_loss: 0.087\n",
      "(11, 1900), Average_loss: 0.087\n",
      "(11, 2000), Average_loss: 0.084\n",
      "(11, 2100), Average_loss: 0.083\n",
      "(11, 2200), Average_loss: 0.087\n",
      "(11, 2300), Average_loss: 0.086\n",
      "(11, 2400), Average_loss: 0.086\n",
      "(11, 2500), Average_loss: 0.088\n",
      "(11, 2600), Average_loss: 0.085\n",
      "(11, 2700), Average_loss: 0.087\n",
      "(11, 2800), Average_loss: 0.090\n",
      "(11, 2900), Average_loss: 0.087\n",
      "(11, 3000), Average_loss: 0.085\n",
      "(11, 3100), Average_loss: 0.084\n",
      "(11, 3200), Average_loss: 0.088\n",
      "(11, 3300), Average_loss: 0.087\n",
      "(11, 3400), Average_loss: 0.084\n",
      "(11, 3500), Average_loss: 0.085\n",
      "(11, 3600), Average_loss: 0.087\n",
      "(11, 3700), Average_loss: 0.086\n",
      "(12, 100), Average_loss: 0.130\n",
      "(12, 200), Average_loss: 0.085\n",
      "(12, 300), Average_loss: 0.086\n",
      "(12, 400), Average_loss: 0.085\n",
      "(12, 500), Average_loss: 0.087\n",
      "(12, 600), Average_loss: 0.089\n",
      "(12, 700), Average_loss: 0.087\n",
      "(12, 800), Average_loss: 0.089\n",
      "(12, 900), Average_loss: 0.085\n",
      "(12, 1000), Average_loss: 0.088\n",
      "(12, 1100), Average_loss: 0.085\n",
      "(12, 1200), Average_loss: 0.086\n",
      "(12, 1300), Average_loss: 0.086\n",
      "(12, 1400), Average_loss: 0.083\n",
      "(12, 1500), Average_loss: 0.085\n",
      "(12, 1600), Average_loss: 0.085\n",
      "(12, 1700), Average_loss: 0.089\n",
      "(12, 1800), Average_loss: 0.088\n",
      "(12, 1900), Average_loss: 0.084\n",
      "(12, 2000), Average_loss: 0.088\n",
      "(12, 2100), Average_loss: 0.086\n",
      "(12, 2200), Average_loss: 0.084\n",
      "(12, 2300), Average_loss: 0.087\n",
      "(12, 2400), Average_loss: 0.089\n",
      "(12, 2500), Average_loss: 0.087\n",
      "(12, 2600), Average_loss: 0.086\n",
      "(12, 2700), Average_loss: 0.084\n",
      "(12, 2800), Average_loss: 0.086\n",
      "(12, 2900), Average_loss: 0.083\n",
      "(12, 3000), Average_loss: 0.088\n",
      "(12, 3100), Average_loss: 0.085\n",
      "(12, 3200), Average_loss: 0.086\n",
      "(12, 3300), Average_loss: 0.085\n",
      "(12, 3400), Average_loss: 0.085\n",
      "(12, 3500), Average_loss: 0.087\n",
      "(12, 3600), Average_loss: 0.087\n",
      "(12, 3700), Average_loss: 0.088\n",
      "(13, 100), Average_loss: 0.131\n",
      "(13, 200), Average_loss: 0.085\n",
      "(13, 300), Average_loss: 0.086\n",
      "(13, 400), Average_loss: 0.086\n",
      "(13, 500), Average_loss: 0.085\n",
      "(13, 600), Average_loss: 0.087\n",
      "(13, 700), Average_loss: 0.085\n",
      "(13, 800), Average_loss: 0.085\n",
      "(13, 900), Average_loss: 0.088\n",
      "(13, 1000), Average_loss: 0.084\n",
      "(13, 1100), Average_loss: 0.085\n",
      "(13, 1200), Average_loss: 0.086\n",
      "(13, 1300), Average_loss: 0.086\n",
      "(13, 1400), Average_loss: 0.087\n",
      "(13, 1500), Average_loss: 0.085\n",
      "(13, 1600), Average_loss: 0.084\n",
      "(13, 1700), Average_loss: 0.085\n",
      "(13, 1800), Average_loss: 0.085\n",
      "(13, 1900), Average_loss: 0.084\n",
      "(13, 2000), Average_loss: 0.085\n",
      "(13, 2100), Average_loss: 0.086\n",
      "(13, 2200), Average_loss: 0.086\n",
      "(13, 2300), Average_loss: 0.088\n",
      "(13, 2400), Average_loss: 0.086\n",
      "(13, 2500), Average_loss: 0.082\n",
      "(13, 2600), Average_loss: 0.084\n",
      "(13, 2700), Average_loss: 0.086\n",
      "(13, 2800), Average_loss: 0.084\n",
      "(13, 2900), Average_loss: 0.087\n",
      "(13, 3000), Average_loss: 0.085\n",
      "(13, 3100), Average_loss: 0.087\n",
      "(13, 3200), Average_loss: 0.084\n",
      "(13, 3300), Average_loss: 0.085\n",
      "(13, 3400), Average_loss: 0.084\n",
      "(13, 3500), Average_loss: 0.084\n",
      "(13, 3600), Average_loss: 0.083\n",
      "(13, 3700), Average_loss: 0.084\n",
      "(14, 100), Average_loss: 0.128\n",
      "(14, 200), Average_loss: 0.083\n",
      "(14, 300), Average_loss: 0.087\n",
      "(14, 400), Average_loss: 0.087\n",
      "(14, 500), Average_loss: 0.085\n",
      "(14, 600), Average_loss: 0.087\n",
      "(14, 700), Average_loss: 0.086\n",
      "(14, 800), Average_loss: 0.083\n",
      "(14, 900), Average_loss: 0.085\n",
      "(14, 1000), Average_loss: 0.085\n",
      "(14, 1100), Average_loss: 0.086\n",
      "(14, 1200), Average_loss: 0.083\n",
      "(14, 1300), Average_loss: 0.087\n",
      "(14, 1400), Average_loss: 0.084\n",
      "(14, 1500), Average_loss: 0.084\n",
      "(14, 1600), Average_loss: 0.084\n",
      "(14, 1700), Average_loss: 0.084\n",
      "(14, 1800), Average_loss: 0.084\n",
      "(14, 1900), Average_loss: 0.085\n",
      "(14, 2000), Average_loss: 0.085\n",
      "(14, 2100), Average_loss: 0.087\n",
      "(14, 2200), Average_loss: 0.083\n",
      "(14, 2300), Average_loss: 0.084\n",
      "(14, 2400), Average_loss: 0.085\n",
      "(14, 2500), Average_loss: 0.082\n",
      "(14, 2600), Average_loss: 0.084\n",
      "(14, 2700), Average_loss: 0.086\n",
      "(14, 2800), Average_loss: 0.083\n",
      "(14, 2900), Average_loss: 0.083\n",
      "(14, 3000), Average_loss: 0.087\n",
      "(14, 3100), Average_loss: 0.083\n",
      "(14, 3200), Average_loss: 0.088\n",
      "(14, 3300), Average_loss: 0.084\n",
      "(14, 3400), Average_loss: 0.085\n",
      "(14, 3500), Average_loss: 0.084\n",
      "(14, 3600), Average_loss: 0.084\n",
      "(14, 3700), Average_loss: 0.083\n",
      "(15, 100), Average_loss: 0.127\n",
      "(15, 200), Average_loss: 0.084\n",
      "(15, 300), Average_loss: 0.086\n",
      "(15, 400), Average_loss: 0.086\n",
      "(15, 500), Average_loss: 0.088\n",
      "(15, 600), Average_loss: 0.085\n",
      "(15, 700), Average_loss: 0.083\n",
      "(15, 800), Average_loss: 0.084\n",
      "(15, 900), Average_loss: 0.086\n",
      "(15, 1000), Average_loss: 0.086\n",
      "(15, 1100), Average_loss: 0.083\n",
      "(15, 1200), Average_loss: 0.085\n",
      "(15, 1300), Average_loss: 0.086\n",
      "(15, 1400), Average_loss: 0.088\n",
      "(15, 1500), Average_loss: 0.083\n",
      "(15, 1600), Average_loss: 0.085\n",
      "(15, 1700), Average_loss: 0.083\n",
      "(15, 1800), Average_loss: 0.082\n",
      "(15, 1900), Average_loss: 0.084\n",
      "(15, 2000), Average_loss: 0.085\n",
      "(15, 2100), Average_loss: 0.085\n",
      "(15, 2200), Average_loss: 0.085\n",
      "(15, 2300), Average_loss: 0.082\n",
      "(15, 2400), Average_loss: 0.089\n",
      "(15, 2500), Average_loss: 0.084\n",
      "(15, 2600), Average_loss: 0.086\n",
      "(15, 2700), Average_loss: 0.082\n",
      "(15, 2800), Average_loss: 0.084\n",
      "(15, 2900), Average_loss: 0.083\n",
      "(15, 3000), Average_loss: 0.084\n",
      "(15, 3100), Average_loss: 0.083\n",
      "(15, 3200), Average_loss: 0.084\n",
      "(15, 3300), Average_loss: 0.082\n",
      "(15, 3400), Average_loss: 0.085\n",
      "(15, 3500), Average_loss: 0.084\n",
      "(15, 3600), Average_loss: 0.085\n",
      "(15, 3700), Average_loss: 0.084\n",
      "(16, 100), Average_loss: 0.124\n",
      "(16, 200), Average_loss: 0.083\n",
      "(16, 300), Average_loss: 0.083\n",
      "(16, 400), Average_loss: 0.084\n",
      "(16, 500), Average_loss: 0.082\n",
      "(16, 600), Average_loss: 0.086\n",
      "(16, 700), Average_loss: 0.086\n",
      "(16, 800), Average_loss: 0.081\n",
      "(16, 900), Average_loss: 0.085\n",
      "(16, 1000), Average_loss: 0.084\n",
      "(16, 1100), Average_loss: 0.083\n",
      "(16, 1200), Average_loss: 0.084\n",
      "(16, 1300), Average_loss: 0.083\n",
      "(16, 1400), Average_loss: 0.082\n",
      "(16, 1500), Average_loss: 0.086\n",
      "(16, 1600), Average_loss: 0.084\n",
      "(16, 1700), Average_loss: 0.084\n",
      "(16, 1800), Average_loss: 0.084\n",
      "(16, 1900), Average_loss: 0.084\n",
      "(16, 2000), Average_loss: 0.082\n",
      "(16, 2100), Average_loss: 0.084\n",
      "(16, 2200), Average_loss: 0.082\n",
      "(16, 2300), Average_loss: 0.082\n",
      "(16, 2400), Average_loss: 0.083\n",
      "(16, 2500), Average_loss: 0.083\n",
      "(16, 2600), Average_loss: 0.084\n",
      "(16, 2700), Average_loss: 0.083\n",
      "(16, 2800), Average_loss: 0.084\n",
      "(16, 2900), Average_loss: 0.088\n",
      "(16, 3000), Average_loss: 0.086\n",
      "(16, 3100), Average_loss: 0.087\n",
      "(16, 3200), Average_loss: 0.084\n",
      "(16, 3300), Average_loss: 0.085\n",
      "(16, 3400), Average_loss: 0.085\n",
      "(16, 3500), Average_loss: 0.081\n",
      "(16, 3600), Average_loss: 0.085\n",
      "(16, 3700), Average_loss: 0.084\n",
      "(17, 100), Average_loss: 0.126\n",
      "(17, 200), Average_loss: 0.083\n",
      "(17, 300), Average_loss: 0.083\n",
      "(17, 400), Average_loss: 0.086\n",
      "(17, 500), Average_loss: 0.085\n",
      "(17, 600), Average_loss: 0.084\n",
      "(17, 700), Average_loss: 0.084\n",
      "(17, 800), Average_loss: 0.084\n",
      "(17, 900), Average_loss: 0.083\n",
      "(17, 1000), Average_loss: 0.082\n",
      "(17, 1100), Average_loss: 0.082\n",
      "(17, 1200), Average_loss: 0.082\n",
      "(17, 1300), Average_loss: 0.084\n",
      "(17, 1400), Average_loss: 0.085\n",
      "(17, 1500), Average_loss: 0.085\n",
      "(17, 1600), Average_loss: 0.084\n",
      "(17, 1700), Average_loss: 0.084\n",
      "(17, 1800), Average_loss: 0.082\n",
      "(17, 1900), Average_loss: 0.083\n",
      "(17, 2000), Average_loss: 0.086\n",
      "(17, 2100), Average_loss: 0.085\n",
      "(17, 2200), Average_loss: 0.082\n",
      "(17, 2300), Average_loss: 0.081\n",
      "(17, 2400), Average_loss: 0.082\n",
      "(17, 2500), Average_loss: 0.081\n",
      "(17, 2600), Average_loss: 0.082\n",
      "(17, 2700), Average_loss: 0.083\n",
      "(17, 2800), Average_loss: 0.083\n",
      "(17, 2900), Average_loss: 0.086\n",
      "(17, 3000), Average_loss: 0.083\n",
      "(17, 3100), Average_loss: 0.083\n",
      "(17, 3200), Average_loss: 0.082\n",
      "(17, 3300), Average_loss: 0.082\n",
      "(17, 3400), Average_loss: 0.083\n",
      "(17, 3500), Average_loss: 0.084\n",
      "(17, 3600), Average_loss: 0.082\n",
      "(17, 3700), Average_loss: 0.084\n",
      "(18, 100), Average_loss: 0.125\n",
      "(18, 200), Average_loss: 0.083\n",
      "(18, 300), Average_loss: 0.083\n",
      "(18, 400), Average_loss: 0.083\n",
      "(18, 500), Average_loss: 0.082\n",
      "(18, 600), Average_loss: 0.081\n",
      "(18, 700), Average_loss: 0.082\n",
      "(18, 800), Average_loss: 0.084\n",
      "(18, 900), Average_loss: 0.081\n",
      "(18, 1000), Average_loss: 0.085\n",
      "(18, 1100), Average_loss: 0.085\n",
      "(18, 1200), Average_loss: 0.083\n",
      "(18, 1300), Average_loss: 0.081\n",
      "(18, 1400), Average_loss: 0.084\n",
      "(18, 1500), Average_loss: 0.083\n",
      "(18, 1600), Average_loss: 0.083\n",
      "(18, 1700), Average_loss: 0.084\n",
      "(18, 1800), Average_loss: 0.081\n",
      "(18, 1900), Average_loss: 0.081\n",
      "(18, 2000), Average_loss: 0.083\n",
      "(18, 2100), Average_loss: 0.083\n",
      "(18, 2200), Average_loss: 0.084\n",
      "(18, 2300), Average_loss: 0.083\n",
      "(18, 2400), Average_loss: 0.082\n",
      "(18, 2500), Average_loss: 0.083\n",
      "(18, 2600), Average_loss: 0.081\n",
      "(18, 2700), Average_loss: 0.086\n",
      "(18, 2800), Average_loss: 0.085\n",
      "(18, 2900), Average_loss: 0.085\n",
      "(18, 3000), Average_loss: 0.081\n",
      "(18, 3100), Average_loss: 0.085\n",
      "(18, 3200), Average_loss: 0.083\n",
      "(18, 3300), Average_loss: 0.083\n",
      "(18, 3400), Average_loss: 0.081\n",
      "(18, 3500), Average_loss: 0.082\n",
      "(18, 3600), Average_loss: 0.081\n",
      "(18, 3700), Average_loss: 0.084\n",
      "(19, 100), Average_loss: 0.121\n",
      "(19, 200), Average_loss: 0.081\n",
      "(19, 300), Average_loss: 0.085\n",
      "(19, 400), Average_loss: 0.085\n",
      "(19, 500), Average_loss: 0.084\n",
      "(19, 600), Average_loss: 0.082\n",
      "(19, 700), Average_loss: 0.083\n",
      "(19, 800), Average_loss: 0.083\n",
      "(19, 900), Average_loss: 0.082\n",
      "(19, 1000), Average_loss: 0.083\n",
      "(19, 1100), Average_loss: 0.082\n",
      "(19, 1200), Average_loss: 0.081\n",
      "(19, 1300), Average_loss: 0.080\n",
      "(19, 1400), Average_loss: 0.081\n",
      "(19, 1500), Average_loss: 0.081\n",
      "(19, 1600), Average_loss: 0.083\n",
      "(19, 1700), Average_loss: 0.082\n",
      "(19, 1800), Average_loss: 0.084\n",
      "(19, 1900), Average_loss: 0.081\n",
      "(19, 2000), Average_loss: 0.084\n",
      "(19, 2100), Average_loss: 0.081\n",
      "(19, 2200), Average_loss: 0.082\n",
      "(19, 2300), Average_loss: 0.083\n",
      "(19, 2400), Average_loss: 0.085\n",
      "(19, 2500), Average_loss: 0.083\n",
      "(19, 2600), Average_loss: 0.082\n",
      "(19, 2700), Average_loss: 0.083\n",
      "(19, 2800), Average_loss: 0.083\n",
      "(19, 2900), Average_loss: 0.082\n",
      "(19, 3000), Average_loss: 0.083\n",
      "(19, 3100), Average_loss: 0.082\n",
      "(19, 3200), Average_loss: 0.084\n",
      "(19, 3300), Average_loss: 0.082\n",
      "(19, 3400), Average_loss: 0.083\n",
      "(19, 3500), Average_loss: 0.082\n",
      "(19, 3600), Average_loss: 0.083\n",
      "(19, 3700), Average_loss: 0.081\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for e, data in enumerate(train_loader):\n",
    "        x0, _ = data\n",
    "        x0 = x0.to(device)\n",
    "        t = torch.randint(1, T+1, (batch_size,)).to(device)\n",
    "        eps = torch.randn(batch_size, 1, 28, 28).to(device)\n",
    "        # print(eps.shape)\n",
    "        # print(x0.shape)\n",
    "        xt = torch.sqrt(alpha_bar[t-1]) * x0 + torch.sqrt(1 - alpha_bar[t-1]) * eps\n",
    "        loss = criterion(eps, model(xt, t.view(batch_size, 1, 1, 1).expand(batch_size, 1, 28, 28)))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if e % 100 == 99:\n",
    "            print(f'{epoch, e+1}, Average_loss: {running_loss/100:.3f}')\n",
    "            running_loss = 0.0\n",
    "    if epoch % 10 == 9:\n",
    "        torch.save(model.state_dict(), f\"DDPM_{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54a74a6-6786-43c8-9b7b-569eb01df2f2",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "999697a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"DDPM_10.pth\",map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53f69f28-7031-48bc-96d4-85b61ba8d524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ad00217190>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    model.eval()\n",
    "    batch_size_sample = 1\n",
    "    xt = torch.randn(batch_size_sample, 1, 28, 28).to(device)\n",
    "\n",
    "    for t in torch.arange(T, 0, -1):\n",
    "        # print(t)\n",
    "        t = t.to(device)\n",
    "        z = torch.randn(batch_size_sample, 1, 28, 28).to(device) if t > 1 else torch.zeros(batch_size_sample, 1, 28, 28).to(device)\n",
    "        xt_new = 1 / torch.sqrt(alpha[t - 1]) * (xt - (1 - alpha[t - 1])/(torch.sqrt(1 - alpha_bar[t - 1])) * \n",
    "                                                    model(xt, t.view(batch_size_sample, 1, 1, 1).expand(batch_size_sample, 1, 28, 28))) + torch.sqrt(beta[t-1]) * z\n",
    "        xt = xt_new\n",
    "plt.imshow(xt[0][0].cpu().detach().numpy(), cmap=\"grey\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6bf278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from all the timesteps\n",
    "fig, axs = plt.subplots(1, 5, tight_layout=True, figsize=(40, 20))\n",
    "tens = np.arange(10,51,10)\n",
    "seed = 3546\n",
    "for ten in tens:\n",
    "    with torch.inference_mode():\n",
    "        # torch.manual_seed(seed)\n",
    "        model.load_state_dict(torch.load(f\"Data_cosnoise_normalized/DDPM_{ten}.pth\", map_location=device))\n",
    "        model.eval()\n",
    "        batch_size_sample = 1\n",
    "        xt = torch.randn(batch_size_sample, 1, 28, 28).to(device)\n",
    "\n",
    "        for t in torch.arange(T, 0, -1):\n",
    "            t = t.to(device)\n",
    "            # print(t)\n",
    "            z = torch.randn(batch_size_sample, 1, 28, 28).to(device) if t > 1 else torch.zeros(batch_size_sample, 1, 28, 28).to(device)\n",
    "            xt_new = 1 / torch.sqrt(alpha[t - 1]) * (xt - (1 - alpha[t - 1])/(torch.sqrt(1 - alpha_bar[t - 1])) * \n",
    "                                                        model(xt, t.view(batch_size_sample, 1, 1, 1).expand(batch_size_sample, 1, 28, 28))) + torch.sqrt(beta[t-1]) * z\n",
    "            xt = xt_new\n",
    "            axs[(ten//10)-1].imshow(xt[0][0].cpu().detach().numpy(), cmap=\"gray\")\n",
    "            axs[(ten//10)-1].axis(\"off\")\n",
    "            axs[(ten//10)-1].set_title(f\"Epoch={ten}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0293c70c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
