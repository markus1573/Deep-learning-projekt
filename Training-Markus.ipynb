{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "def4f85a-37b5-472e-8782-a5df13c5d601",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec1af4-1d61-46ef-90b9-b972129d09a5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3eeb867f-c38e-4e5c-ab89-a7e747ead472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017d87cd-410b-44ef-ac34-560022867208",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "#torch.backends.cudnn.enabled = False\n",
    "val_size = 5000\n",
    "test_size = 5000\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "pin_memory = False if device == torch.device('cpu') else True\n",
    "\n",
    "# Normalize input images to [-1, 1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "# Downloading MNIST again :) Training (60k) and test(5k) + val(5k) split\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('./mnist_data',\n",
    "                                            download=True,\n",
    "                                            train=True,\n",
    "                                            transform=transform),\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=num_workers,\n",
    "                                            pin_memory=pin_memory,\n",
    "                                            drop_last=True)\n",
    "\n",
    "test_dataset = datasets.MNIST('./mnist_data',\n",
    "                               download=True,\n",
    "                               train=False,\n",
    "                               transform=transform)\n",
    "\n",
    "val_dataset, test_dataset = random_split(test_dataset, [val_size, test_size])\n",
    "\n",
    "# Test set to compare with DDPM paper\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=num_workers,\n",
    "                                            pin_memory=pin_memory)\n",
    "\n",
    "# Validation set so we can keep track of approximated FID score while training\n",
    "validation_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=num_workers,\n",
    "                                            pin_memory=pin_memory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0ae1fca-cff3-4b53-86d1-65c59740a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cosine noise schedule\n",
    "def f(t, s=torch.tensor([0.008]), T=torch.tensor([1000])):\n",
    "    return min(torch.cos((t / T + s) / (1 + s) * (torch.pi / 2)).pow(2), 0.999)\n",
    "\n",
    "T = 1000\n",
    "ts = torch.arange(T)\n",
    "alpha_bar = torch.tensor([min(f(t)/f(torch.tensor([0])),0.999) for t in ts]) \n",
    "beta = torch.tensor([1 - alpha_bar[t]/(alpha_bar[t-1]) if t > 0 else torch.tensor([0]) for t in ts])\n",
    "alpha = 1 - beta\n",
    "alpha = alpha.view((1000, 1, 1, 1)).to(device)\n",
    "beta = beta.view((1000, 1, 1, 1)).to(device)\n",
    "alpha_bar = alpha_bar.view((1000, 1, 1, 1)).to(device)\n",
    "\n",
    "\n",
    "# # Linear noise schedule\n",
    "# T = 1000\n",
    "# beta_start, beta_end = 1e-4, 2e-2\n",
    "# beta = torch.linspace(beta_start, beta_end, T)  # Linear noise schedule\n",
    "# alpha = 1.0 - beta\n",
    "# alpha_bar = torch.cumprod(alpha, dim=0)  # Cumulative product for alpha_bar\n",
    "\n",
    "# # Reshape for broadcasting (if required for your model)\n",
    "# alpha = alpha.view((T, 1, 1, 1)).to(device)\n",
    "# beta = beta.view((T, 1, 1, 1)).to(device)\n",
    "# alpha_bar = alpha_bar.view((T, 1, 1, 1)).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42fe879-ee13-4434-9a0d-de015ddd4c2f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c671dbab-ffe9-4fe5-841a-80042e7593b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class SinusoidalEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(SinusoidalEmbedding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        half_dim = self.embedding_dim // 2\n",
    "        freqs = torch.exp(\n",
    "            -torch.arange(half_dim, dtype=torch.float32) * math.log(10000) / half_dim\n",
    "        ).to(t.device)\n",
    "        angles = t[:, None] * freqs[None, :]\n",
    "        return torch.cat([angles.sin(), angles.cos()], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f57e196",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearTimeEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(LinearTimeEmbedding, self).__init__()\n",
    "        self.projection = nn.Linear(1, embedding_dim)  # Project the scalar to the embedding dimension\n",
    "\n",
    "    def forward(self, t):\n",
    "        return self.projection(t.unsqueeze(-1))  # Add an extra dimension for the projection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9dd8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNET, self).__init__()\n",
    "        self.channels = [1,32, 64, 128, 256]\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(1, self.channels[1], kernel_size=3, padding=1),  # (batchsize, 32, 28, 28)\n",
    "                nn.GroupNorm(4, self.channels[1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2),  # (batchsize, 32, 14, 14)\n",
    "                nn.Conv2d(self.channels[1], self.channels[2], kernel_size=3, padding=1),  # (batchsize, 64, 14, 14)\n",
    "                nn.GroupNorm(4, self.channels[2]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2),  # (batchsize, 64, 7, 7)\n",
    "                nn.Conv2d(self.channels[2], self.channels[3], kernel_size=3, padding=1),  # (batchsize, 128, 7, 7)\n",
    "                nn.GroupNorm(8, self.channels[3]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.3)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2, padding=1),  # (batchsize, 128, 4, 4)\n",
    "                nn.Conv2d(self.channels[3], self.channels[4], kernel_size=3, padding=1),  # (batchsize, 256, 4, 4)\n",
    "                nn.GroupNorm(8, self.channels[4]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.3)\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        self.tconvs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(self.channels[4], self.channels[3], kernel_size=3, \n",
    "                                   stride=2, padding=1, output_padding=0),   # (batchsize, 128, 7, 7)\n",
    "                nn.GroupNorm(8, self.channels[3]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.3)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(self.channels[3]*2, self.channels[2], kernel_size=3,\n",
    "                                   stride=2, padding=1, output_padding=1),   # (batchsize, 64, 14, 14)\n",
    "                nn.GroupNorm(8, self.channels[2]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.3)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(self.channels[2]*2, self.channels[1], kernel_size=3, \n",
    "                                   stride=2, padding=1, output_padding=1),   # (batchsize, 32, 28, 28)\n",
    "                nn.GroupNorm(4, self.channels[1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(self.channels[1]*2, self.channels[1], kernel_size=3, padding=1),  # (batchsize, 32, 28, 28)\n",
    "                nn.GroupNorm(4, self.channels[1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout2d(0.2),\n",
    "                nn.Conv2d(self.channels[1], 1, kernel_size=1)  # (batchsize, 1, 28, 28)\n",
    "            )      \n",
    "        ])\n",
    "        \n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        signal = x\n",
    "        signals = []\n",
    "        t_emb_norm = F.normalize(t_emb, p=2, dim=-1)\n",
    "        # Pass through the encoding layers\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            lin_time = nn.Linear(128, self.channels[i],device=device) # 128 is the size of the time embedding.\n",
    "            t_emb_processed = lin_time(t_emb_norm).view(-1, self.channels[i], 1, 1)\n",
    "            signal= t_emb_processed+signal\n",
    "            signal = conv(signal)\n",
    "            if i < len(self.convs)-1:\n",
    "                signals.append(signal)\n",
    "\n",
    "        for i, tconv in enumerate(self.tconvs):\n",
    "            lin_time = nn.Linear(128, self.channels[-i-1],device=device)\n",
    "            t_emb_processed = lin_time(t_emb_norm).view(-1, self.channels[-i-1], 1, 1)\n",
    "            signal= signal+t_emb_processed\n",
    "            if i == 0:\n",
    "                signal = tconv(signal)\n",
    "            else:\n",
    "                signal = torch.cat((signal, signals[-i]), dim=-3)\n",
    "                signal = tconv(signal)\n",
    "\n",
    "        return signal\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f04a5df-da69-407e-9285-90de11992cef",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8dc7e14a-63cd-4432-8f07-43fd00841fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from UNET import UNET\n",
    "epochs = 200\n",
    "model = UNET().to(device)\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "running_loss = 0\n",
    "\n",
    "\n",
    "# Initialize the linear time embedding layer\n",
    "time_embedding_dim = 128  # You can set this to any suitable size\n",
    "time_embedding_layer = SinusoidalEmbedding(time_embedding_dim).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5031160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for e, data in enumerate(train_loader):\n",
    "        x0, _ = data\n",
    "        x0 = x0.to(device)\n",
    "        t = torch.randint(1, T+1, (batch_size,), device=device)\n",
    "        t_emb = time_embedding_layer(t)  # Shape: [batch_size, embedding_dim]\n",
    "        eps = torch.randn_like(x0).to(device)\n",
    "        x_t = torch.sqrt(alpha_bar[t-1]) * x0 + torch.sqrt(1 - alpha_bar[t-1]) * eps\n",
    "        predicted_eps = model(x_t, t_emb)  # Pass the precomputed embedding to the model\n",
    "        loss = criterion(predicted_eps, eps)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if e % 100 == 99:\n",
    "            print(f\"Epoch {epoch}, Batch {e+1}, Average Loss: {running_loss / 100:.4f}\")\n",
    "            running_loss = 0.0  \n",
    "\n",
    "    if epoch % 10 == 9:\n",
    "        torch.save(model.state_dict(), f\"DDPM_{epoch+1}.pth\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54a74a6-6786-43c8-9b7b-569eb01df2f2",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f24222b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x309bf1310>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_model = \"DDPM_60.pth\"\n",
    "\n",
    "if load_model:\n",
    "    model.load_state_dict(torch.load(load_model, map_location=device))\n",
    "with torch.inference_mode():\n",
    "    model.eval()\n",
    "    batch_size = 1\n",
    "    xt = torch.randn(batch_size, 1, 28, 28).to(device)\n",
    "\n",
    "    for t in torch.arange(T, 0, -1):\n",
    "        t = t.reshape(1)\n",
    "        t = t.to(device)\n",
    "        t_emb = time_embedding_layer(t.float())\n",
    "        z = torch.randn(batch_size, 1, 28, 28).to(device) if t > 1 else torch.zeros(batch_size, 1, 28, 28).to(device)\n",
    "        \n",
    "        xt_new = 1 / torch.sqrt(alpha[t - 1]) * (xt - (1 - alpha[t - 1])/(torch.sqrt(1 - alpha_bar[t - 1])) * \n",
    "                                                    model(xt, t_emb)) + torch.sqrt(beta[t-1]) * z\n",
    "        xt = xt_new\n",
    "\n",
    "\n",
    "plt.imshow(xt[0][0].cpu().detach().numpy(), cmap=\"grey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7d7c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from all the timesteps\n",
    "fig, axs = plt.subplots(1, 1, tight_layout=True, figsize=(40, 20))\n",
    "tens = np.arange(10,11,10)\n",
    "seed = 3546\n",
    "for ten in tens:\n",
    "    with torch.inference_mode():\n",
    "        # torch.manual_seed(seed)\n",
    "        model.load_state_dict(torch.load(f\"DDPM_{ten}.pth\", map_location=device))\n",
    "        model.eval()\n",
    "        batch_size = 1\n",
    "        xt = torch.randn(batch_size, 1, 28, 28).to(device)\n",
    "\n",
    "        for t in torch.arange(T, 0, -1):\n",
    "            t = t.reshape(1)\n",
    "            t = t.to(device)\n",
    "            t_emb = time_embedding_layer(t.float())\n",
    "            z = torch.randn(batch_size, 1, 28, 28).to(device) if t > 1 else torch.zeros(batch_size, 1, 28, 28).to(device)\n",
    "            \n",
    "            xt_new = 1 / torch.sqrt(alpha[t - 1]) * (xt - (1 - alpha[t - 1])/(torch.sqrt(1 - alpha_bar[t - 1])) * \n",
    "                                                        model(xt, t_emb)) + torch.sqrt(beta[t-1]) * z\n",
    "            xt = xt_new\n",
    "        axs[(ten//10)-1].imshow(xt[0][0].cpu().detach().numpy(), cmap=\"gray\")\n",
    "        axs[(ten//10)-1].axis(\"off\")\n",
    "        axs[(ten//10)-1].set_title(f\"Epoch={ten}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
