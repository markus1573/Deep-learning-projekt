{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "def4f85a-37b5-472e-8782-a5df13c5d601",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec1af4-1d61-46ef-90b9-b972129d09a5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eeb867f-c38e-4e5c-ab89-a7e747ead472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "017d87cd-410b-44ef-ac34-560022867208",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "#torch.backends.cudnn.enabled = False\n",
    "val_size = 5000\n",
    "test_size = 5000\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "pin_memory = False if device == torch.device('cpu') else True\n",
    "\n",
    "# Normalize input images to [-1, 1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "\n",
    "# Downloading MNIST again :) Training (60k) and test(5k) + val(5k) split\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('./mnist_data',\n",
    "                                            download=True,\n",
    "                                            train=True,\n",
    "                                            transform=transform),\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=num_workers,\n",
    "                                            pin_memory=pin_memory,\n",
    "                                            drop_last=True)\n",
    "\n",
    "test_dataset = datasets.MNIST('./mnist_data',\n",
    "                               download=True,\n",
    "                               train=False,\n",
    "                               transform=transform)\n",
    "\n",
    "val_dataset, test_dataset = random_split(test_dataset, [val_size, test_size])\n",
    "\n",
    "# Test set to compare with DDPM paper\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=num_workers,\n",
    "                                            pin_memory=pin_memory)\n",
    "\n",
    "# Validation set so we can keep track of approximated FID score while training\n",
    "validation_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=num_workers,\n",
    "                                            pin_memory=pin_memory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0ae1fca-cff3-4b53-86d1-65c59740a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000\n",
    "beta_start, beta_end = 1e-4, 2e-2\n",
    "beta = torch.linspace(beta_start, beta_end, T)  # Linear noise schedule\n",
    "alpha = 1.0 - beta\n",
    "alpha_bar = torch.cumprod(alpha, dim=0)  # Cumulative product for alpha_bar\n",
    "\n",
    "# Reshape for broadcasting (if required for your model)\n",
    "alpha = alpha.view((T, 1, 1, 1)).to(device)\n",
    "beta = beta.view((T, 1, 1, 1)).to(device)\n",
    "alpha_bar = alpha_bar.view((T, 1, 1, 1)).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42fe879-ee13-4434-9a0d-de015ddd4c2f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "939623f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SinusoidalEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(SinusoidalEmbedding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "        - t: Tensor of shape [batch_size] containing timesteps.\n",
    "\n",
    "        Output:\n",
    "        - embeddings: Tensor of shape [batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        half_dim = self.embedding_dim // 2\n",
    "        freqs = torch.exp(\n",
    "            -torch.arange(half_dim, dtype=torch.float32) * math.log(10000) / half_dim\n",
    "        )\n",
    "        angles = t[:, None] * freqs[None, :]\n",
    "        return torch.cat([angles.sin(), angles.cos()], dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c671dbab-ffe9-4fe5-841a-80042e7593b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SinusoidalEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(SinusoidalEmbedding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "        - t: Tensor of shape [batch_size] containing timesteps.\n",
    "\n",
    "        Output:\n",
    "        - embeddings: Tensor of shape [batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        half_dim = self.embedding_dim // 2\n",
    "        freqs = torch.exp(\n",
    "            -torch.arange(half_dim, dtype=torch.float32) * math.log(10000) / half_dim\n",
    "        ).to(t.device)\n",
    "        angles = t[:, None] * freqs[None, :]\n",
    "        return torch.cat([angles.sin(), angles.cos()], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f57e196",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearTimeEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(LinearTimeEmbedding, self).__init__()\n",
    "        self.projection = nn.Linear(1, embedding_dim)  # Project the scalar to the embedding dimension\n",
    "\n",
    "    def forward(self, t):\n",
    "        return self.projection(t.unsqueeze(-1))  # Add an extra dimension for the projection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c9dd8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNET, self).__init__()\n",
    "        channels = [32, 64, 128, 256]\n",
    "        \n",
    "        # Define convolutional layers for encoding\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(1, channels[0], kernel_size=3, padding=1),  # (batchsize, 32, 28, 28)\n",
    "                nn.ReLU(),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2),  # (batchsize, 32, 14, 14)\n",
    "                nn.Conv2d(channels[0], channels[1], kernel_size=3, padding=1),  # (batchsize, 64, 14, 14)\n",
    "                nn.ReLU(),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2),  # (batchsize, 64, 7, 7)\n",
    "                nn.Conv2d(channels[1], channels[2], kernel_size=3, padding=1),  # (batchsize, 128, 7, 7)\n",
    "                nn.ReLU(),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2, padding=1),  # (batchsize, 128, 4, 4)\n",
    "                nn.Conv2d(channels[2], channels[3], kernel_size=3, padding=1),  # (batchsize, 256, 4, 4)\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        # Define transpose convolutional layers for decoding\n",
    "        self.tconvs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(channels[3], channels[2], kernel_size=3, \n",
    "                                   stride=2, padding=1, output_padding=0),   # (batchsize, 128, 7, 7)\n",
    "                nn.ReLU()\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(channels[2]*2, channels[1], kernel_size=3,\n",
    "                                   stride=2, padding=1, output_padding=1),   # (batchsize, 64, 14, 14)\n",
    "                nn.ReLU()\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(channels[1]*2, channels[0], kernel_size=3, \n",
    "                                   stride=2, padding=1, output_padding=1),   # (batchsize, 32, 28, 28)\n",
    "                nn.ReLU()\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(channels[0]*2, channels[0], kernel_size=3, padding=1),  # (batchsize, 32, 28, 28)\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(channels[0], 1, kernel_size=1)  # (batchsize, 1, 28, 28)\n",
    "            )      \n",
    "        ])\n",
    "        \n",
    "        # Linear layer to process the time embedding\n",
    "        self.lin_time = nn.Linear(128, channels[3])\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        signal = x\n",
    "        signals = []\n",
    "\n",
    "        # Pass through the encoding layers\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            # print(f\"conv {i}\")\n",
    "            signal = conv(signal)\n",
    "            # print(signal.shape)\n",
    "            if i < len(self.convs)-1:\n",
    "                signals.append(signal)  # Store intermediate feature maps for skip connections\n",
    "\n",
    "        # Process time embedding and match dimensions\n",
    "        t_emb_processed = self.lin_time(t_emb).view(-1, 256, 1, 1)  # Map time embedding to match the feature map shape\n",
    "        \n",
    "        # Add time embedding to the bottleneck signal (this could be done at any point during the decoder)\n",
    "        signal = signal + t_emb_processed  # Broadcast and add to the signal in the bottleneck\n",
    "\n",
    "        # Pass through the decoding layers with skip connections and time-embedded signal\n",
    "        for i, tconv in enumerate(self.tconvs):\n",
    "            # print(f\"tconv {i}\")\n",
    "            # print(f\"signal shape: {signal.shape}\")\n",
    "            if i == 0:\n",
    "                signal = tconv(signal)\n",
    "            else:\n",
    "                # print(f\"signals[{-i}] shape: {signals[-i].shape}\")\n",
    "                signal = torch.cat((signal, signals[-i]), dim=-3)  # Skip connection\n",
    "                signal = tconv(signal)\n",
    "\n",
    "        return signal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f04a5df-da69-407e-9285-90de11992cef",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8dc7e14a-63cd-4432-8f07-43fd00841fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 100, Average Loss: 29434.9715\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 41\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m99\u001b[39m:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Average Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#from UNET import UNET\n",
    "epochs = 50\n",
    "model = UNET()\n",
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = torch.nn.MSELoss(reduction=\"sum\")\n",
    "running_loss = 0\n",
    "\n",
    "\n",
    "# Initialize the linear time embedding layer\n",
    "time_embedding_dim = 128  # You can set this to any suitable size\n",
    "time_embedding_layer = SinusoidalEmbedding(time_embedding_dim).to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for e, data in enumerate(train_loader):\n",
    "        x0, _ = data\n",
    "        x0 = x0.to(device)\n",
    "\n",
    "        # Sample timesteps and noise\n",
    "        t = torch.randint(0, T, (batch_size,), device=device).float()\n",
    "        \n",
    "        # Compute the time embedding\n",
    "        t_emb = time_embedding_layer(t)  # Shape: [batch_size, embedding_dim]\n",
    "        \n",
    "        eps = torch.randn_like(x0).to(device)\n",
    "\n",
    "        # Generate noisy inputs\n",
    "        x_t = torch.sqrt(alpha_bar[t.long()]) * x0 + torch.sqrt(1 - alpha_bar[t.long()]) * eps\n",
    "\n",
    "        # Forward pass\n",
    "        predicted_eps = model(x_t, t_emb)  # Pass the precomputed embedding to the model\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(predicted_eps, eps)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if e % 100 == 99:\n",
    "            print(f\"Epoch {epoch}, Batch {e+1}, Average Loss: {running_loss / 100:.4f}\")\n",
    "            running_loss = 0.0  \n",
    "\n",
    "    if epoch % 5 == 4:\n",
    "        torch.save(model.state_dict(), f\"DDPM_{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54a74a6-6786-43c8-9b7b-569eb01df2f2",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f24222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    model.eval()\n",
    "    batch_size = 1\n",
    "    xt = torch.randn(batch_size, 1, 28, 28).to(device)\n",
    "\n",
    "    for t in torch.arange(T, 0, -1):\n",
    "        t = t.reshape(1)\n",
    "        t = t.to(device)\n",
    "        t_emb = time_embedding_layer(t.float())\n",
    "        z = torch.randn(batch_size, 1, 28, 28).to(device) if t > 1 else torch.zeros(batch_size, 1, 28, 28).to(device)\n",
    "        \n",
    "        xt_new = 1 / torch.sqrt(alpha[t - 1]) * (xt - (1 - alpha[t - 1])/(torch.sqrt(1 - alpha_bar[t - 1])) * \n",
    "                                                    model(xt, t_emb)) + torch.sqrt(beta[t-1]) * z\n",
    "        xt = xt_new\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76f66843-b77f-4798-a78b-806310d3057b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17705a890>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xt[0][0].cpu().detach().numpy(), cmap=\"grey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bead4b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
