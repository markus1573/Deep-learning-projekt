{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "def4f85a-37b5-472e-8782-a5df13c5d601",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec1af4-1d61-46ef-90b9-b972129d09a5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eeb867f-c38e-4e5c-ab89-a7e747ead472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "017d87cd-410b-44ef-ac34-560022867208",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "#torch.backends.cudnn.enabled = False\n",
    "val_size = 5000\n",
    "test_size = 5000\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "pin_memory = False if device == torch.device('cpu') else True\n",
    "\n",
    "# Normalize input images to [-1, 1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "\n",
    "# Downloading MNIST again :) Training (60k) and test(5k) + val(5k) split\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('./mnist_data',\n",
    "                                            download=True,\n",
    "                                            train=True,\n",
    "                                            transform=transform),\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=num_workers,\n",
    "                                            pin_memory=pin_memory,\n",
    "                                            drop_last=True)\n",
    "\n",
    "test_dataset = datasets.MNIST('./mnist_data',\n",
    "                               download=True,\n",
    "                               train=False,\n",
    "                               transform=transform)\n",
    "\n",
    "val_dataset, test_dataset = random_split(test_dataset, [val_size, test_size])\n",
    "\n",
    "# Test set to compare with DDPM paper\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=num_workers,\n",
    "                                            pin_memory=pin_memory)\n",
    "\n",
    "# Validation set so we can keep track of approximated FID score while training\n",
    "validation_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=num_workers,\n",
    "                                            pin_memory=pin_memory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0ae1fca-cff3-4b53-86d1-65c59740a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000\n",
    "beta_start, beta_end = 1e-4, 2e-2\n",
    "beta = torch.linspace(beta_start, beta_end, T)  # Linear noise schedule\n",
    "alpha = 1.0 - beta\n",
    "alpha_bar = torch.cumprod(alpha, dim=0)  # Cumulative product for alpha_bar\n",
    "\n",
    "# Reshape for broadcasting (if required for your model)\n",
    "alpha = alpha.view((T, 1, 1, 1)).to(device)\n",
    "beta = beta.view((T, 1, 1, 1)).to(device)\n",
    "alpha_bar = alpha_bar.view((T, 1, 1, 1)).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42fe879-ee13-4434-9a0d-de015ddd4c2f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c671dbab-ffe9-4fe5-841a-80042e7593b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class SinusoidalEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(SinusoidalEmbedding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        half_dim = self.embedding_dim // 2\n",
    "        freqs = torch.exp(\n",
    "            -torch.arange(half_dim, dtype=torch.float32) * math.log(10000) / half_dim\n",
    "        ).to(t.device)\n",
    "        angles = t[:, None] * freqs[None, :]\n",
    "        return torch.cat([angles.sin(), angles.cos()], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f57e196",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearTimeEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(LinearTimeEmbedding, self).__init__()\n",
    "        self.projection = nn.Linear(1, embedding_dim)  # Project the scalar to the embedding dimension\n",
    "\n",
    "    def forward(self, t):\n",
    "        return self.projection(t.unsqueeze(-1))  # Add an extra dimension for the projection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c9dd8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNET, self).__init__()\n",
    "        self.channels = [1,32, 64, 128, 256]\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(1, self.channels[1], kernel_size=3, padding=1),  # (batchsize, 32, 28, 28)\n",
    "                nn.ReLU(),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2),  # (batchsize, 32, 14, 14)\n",
    "                nn.Conv2d(self.channels[1], self.channels[2], kernel_size=3, padding=1),  # (batchsize, 64, 14, 14)\n",
    "                nn.ReLU(),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2),  # (batchsize, 64, 7, 7)\n",
    "                nn.Conv2d(self.channels[2], self.channels[3], kernel_size=3, padding=1),  # (batchsize, 128, 7, 7)\n",
    "                nn.ReLU(),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.MaxPool2d(2, padding=1),  # (batchsize, 128, 4, 4)\n",
    "                nn.Conv2d(self.channels[3], self.channels[4], kernel_size=3, padding=1),  # (batchsize, 256, 4, 4)\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        self.tconvs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(self.channels[4], self.channels[3], kernel_size=3, \n",
    "                                   stride=2, padding=1, output_padding=0),   # (batchsize, 128, 7, 7)\n",
    "                nn.ReLU()\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(self.channels[3]*2, self.channels[2], kernel_size=3,\n",
    "                                   stride=2, padding=1, output_padding=1),   # (batchsize, 64, 14, 14)\n",
    "                nn.ReLU()\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(self.channels[2]*2, self.channels[1], kernel_size=3, \n",
    "                                   stride=2, padding=1, output_padding=1),   # (batchsize, 32, 28, 28)\n",
    "                nn.ReLU()\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(self.channels[1]*2, self.channels[1], kernel_size=3, padding=1),  # (batchsize, 32, 28, 28)\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(self.channels[1], 1, kernel_size=1)  # (batchsize, 1, 28, 28)\n",
    "            )      \n",
    "        ])\n",
    "        \n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        signal = x\n",
    "        signals = []\n",
    "\n",
    "        # Pass through the encoding layers\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            lin_time = nn.Linear(128, self.channels[i],device=device) # 128 is the size of the time embedding.\n",
    "            t_emb_processed = lin_time(t_emb).view(-1, self.channels[i], 1, 1)\n",
    "            signal= t_emb_processed+signal\n",
    "            signal = conv(signal)\n",
    "            if i < len(self.convs)-1:\n",
    "                signals.append(signal)\n",
    "\n",
    "        for i, tconv in enumerate(self.tconvs):\n",
    "            lin_time = nn.Linear(128, self.channels[-i-1],device=device)\n",
    "            t_emb_processed = lin_time(t_emb).view(-1, self.channels[-i-1], 1, 1)\n",
    "            signal= signal+t_emb_processed\n",
    "            if i == 0:\n",
    "                signal = tconv(signal)\n",
    "            else:\n",
    "                signal = torch.cat((signal, signals[-i]), dim=-3)\n",
    "                signal = tconv(signal)\n",
    "\n",
    "        return signal\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f04a5df-da69-407e-9285-90de11992cef",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dc7e14a-63cd-4432-8f07-43fd00841fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from UNET import UNET\n",
    "epochs = 200\n",
    "model = UNET().to(device)\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "running_loss = 0\n",
    "\n",
    "\n",
    "# Initialize the linear time embedding layer\n",
    "time_embedding_dim = 128  # You can set this to any suitable size\n",
    "time_embedding_layer = SinusoidalEmbedding(time_embedding_dim).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5031160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 100, Average Loss: 0.2470\n",
      "Epoch 0, Batch 200, Average Loss: 0.1487\n",
      "Epoch 0, Batch 300, Average Loss: 0.1342\n",
      "Epoch 0, Batch 400, Average Loss: 0.1155\n",
      "Epoch 0, Batch 500, Average Loss: 0.1090\n",
      "Epoch 0, Batch 600, Average Loss: 0.1001\n",
      "Epoch 0, Batch 700, Average Loss: 0.0942\n",
      "Epoch 0, Batch 800, Average Loss: 0.0914\n",
      "Epoch 0, Batch 900, Average Loss: 0.0912\n",
      "Epoch 0, Batch 1000, Average Loss: 0.0809\n",
      "Epoch 0, Batch 1100, Average Loss: 0.0797\n",
      "Epoch 0, Batch 1200, Average Loss: 0.0781\n",
      "Epoch 0, Batch 1300, Average Loss: 0.0756\n",
      "Epoch 0, Batch 1400, Average Loss: 0.0741\n",
      "Epoch 0, Batch 1500, Average Loss: 0.0732\n",
      "Epoch 0, Batch 1600, Average Loss: 0.0763\n",
      "Epoch 0, Batch 1700, Average Loss: 0.0713\n",
      "Epoch 0, Batch 1800, Average Loss: 0.0715\n",
      "Epoch 1, Batch 100, Average Loss: 0.1237\n",
      "Epoch 1, Batch 200, Average Loss: 0.0654\n",
      "Epoch 1, Batch 300, Average Loss: 0.0693\n",
      "Epoch 1, Batch 400, Average Loss: 0.0680\n",
      "Epoch 1, Batch 500, Average Loss: 0.0644\n",
      "Epoch 1, Batch 600, Average Loss: 0.0668\n",
      "Epoch 1, Batch 700, Average Loss: 0.0609\n",
      "Epoch 1, Batch 800, Average Loss: 0.0604\n",
      "Epoch 1, Batch 900, Average Loss: 0.0614\n",
      "Epoch 1, Batch 1000, Average Loss: 0.0598\n",
      "Epoch 1, Batch 1100, Average Loss: 0.0594\n",
      "Epoch 1, Batch 1200, Average Loss: 0.0622\n",
      "Epoch 1, Batch 1300, Average Loss: 0.0604\n",
      "Epoch 1, Batch 1400, Average Loss: 0.0624\n",
      "Epoch 1, Batch 1500, Average Loss: 0.0604\n",
      "Epoch 1, Batch 1600, Average Loss: 0.0651\n",
      "Epoch 1, Batch 1700, Average Loss: 0.0599\n",
      "Epoch 1, Batch 1800, Average Loss: 0.0569\n",
      "Epoch 2, Batch 100, Average Loss: 0.0996\n",
      "Epoch 2, Batch 200, Average Loss: 0.0589\n",
      "Epoch 2, Batch 300, Average Loss: 0.0571\n",
      "Epoch 2, Batch 400, Average Loss: 0.0592\n",
      "Epoch 2, Batch 500, Average Loss: 0.0604\n",
      "Epoch 2, Batch 600, Average Loss: 0.0578\n",
      "Epoch 2, Batch 700, Average Loss: 0.0541\n",
      "Epoch 2, Batch 800, Average Loss: 0.0568\n",
      "Epoch 2, Batch 900, Average Loss: 0.0565\n",
      "Epoch 2, Batch 1000, Average Loss: 0.0561\n",
      "Epoch 2, Batch 1100, Average Loss: 0.0628\n",
      "Epoch 2, Batch 1200, Average Loss: 0.0545\n",
      "Epoch 2, Batch 1300, Average Loss: 0.0535\n",
      "Epoch 2, Batch 1400, Average Loss: 0.0533\n",
      "Epoch 2, Batch 1500, Average Loss: 0.0521\n",
      "Epoch 2, Batch 1600, Average Loss: 0.0533\n",
      "Epoch 2, Batch 1700, Average Loss: 0.0538\n",
      "Epoch 2, Batch 1800, Average Loss: 0.0545\n",
      "Epoch 3, Batch 100, Average Loss: 0.0939\n",
      "Epoch 3, Batch 200, Average Loss: 0.0528\n",
      "Epoch 3, Batch 300, Average Loss: 0.0493\n",
      "Epoch 3, Batch 400, Average Loss: 0.0509\n",
      "Epoch 3, Batch 500, Average Loss: 0.0530\n",
      "Epoch 3, Batch 600, Average Loss: 0.0509\n",
      "Epoch 3, Batch 700, Average Loss: 0.0503\n",
      "Epoch 3, Batch 800, Average Loss: 0.0509\n",
      "Epoch 3, Batch 900, Average Loss: 0.0541\n",
      "Epoch 3, Batch 1000, Average Loss: 0.0486\n",
      "Epoch 3, Batch 1100, Average Loss: 0.0536\n",
      "Epoch 3, Batch 1200, Average Loss: 0.0538\n",
      "Epoch 3, Batch 1300, Average Loss: 0.0478\n",
      "Epoch 3, Batch 1400, Average Loss: 0.0505\n",
      "Epoch 3, Batch 1500, Average Loss: 0.0523\n",
      "Epoch 3, Batch 1600, Average Loss: 0.0520\n",
      "Epoch 3, Batch 1700, Average Loss: 0.0484\n",
      "Epoch 3, Batch 1800, Average Loss: 0.0533\n",
      "Epoch 4, Batch 100, Average Loss: 0.0895\n",
      "Epoch 4, Batch 200, Average Loss: 0.0486\n",
      "Epoch 4, Batch 300, Average Loss: 0.0496\n",
      "Epoch 4, Batch 400, Average Loss: 0.0514\n",
      "Epoch 4, Batch 500, Average Loss: 0.0502\n",
      "Epoch 4, Batch 600, Average Loss: 0.0499\n",
      "Epoch 4, Batch 700, Average Loss: 0.0488\n",
      "Epoch 4, Batch 800, Average Loss: 0.0515\n",
      "Epoch 4, Batch 900, Average Loss: 0.0527\n",
      "Epoch 4, Batch 1000, Average Loss: 0.0482\n",
      "Epoch 4, Batch 1100, Average Loss: 0.0480\n",
      "Epoch 4, Batch 1200, Average Loss: 0.0473\n",
      "Epoch 4, Batch 1300, Average Loss: 0.0475\n",
      "Epoch 4, Batch 1400, Average Loss: 0.0494\n",
      "Epoch 4, Batch 1500, Average Loss: 0.0492\n",
      "Epoch 4, Batch 1600, Average Loss: 0.0490\n",
      "Epoch 4, Batch 1700, Average Loss: 0.0497\n",
      "Epoch 4, Batch 1800, Average Loss: 0.0493\n",
      "Epoch 5, Batch 100, Average Loss: 0.0840\n",
      "Epoch 5, Batch 200, Average Loss: 0.0485\n",
      "Epoch 5, Batch 300, Average Loss: 0.0479\n",
      "Epoch 5, Batch 400, Average Loss: 0.0479\n",
      "Epoch 5, Batch 500, Average Loss: 0.0472\n",
      "Epoch 5, Batch 600, Average Loss: 0.0472\n",
      "Epoch 5, Batch 700, Average Loss: 0.0462\n",
      "Epoch 5, Batch 800, Average Loss: 0.0490\n",
      "Epoch 5, Batch 900, Average Loss: 0.0475\n",
      "Epoch 5, Batch 1000, Average Loss: 0.0481\n",
      "Epoch 5, Batch 1100, Average Loss: 0.0472\n",
      "Epoch 5, Batch 1200, Average Loss: 0.0451\n",
      "Epoch 5, Batch 1300, Average Loss: 0.0489\n",
      "Epoch 5, Batch 1400, Average Loss: 0.0474\n",
      "Epoch 5, Batch 1500, Average Loss: 0.0482\n",
      "Epoch 5, Batch 1600, Average Loss: 0.0450\n",
      "Epoch 5, Batch 1700, Average Loss: 0.0454\n",
      "Epoch 5, Batch 1800, Average Loss: 0.0459\n",
      "Epoch 6, Batch 100, Average Loss: 0.0815\n",
      "Epoch 6, Batch 200, Average Loss: 0.0467\n",
      "Epoch 6, Batch 300, Average Loss: 0.0473\n",
      "Epoch 6, Batch 400, Average Loss: 0.0450\n",
      "Epoch 6, Batch 500, Average Loss: 0.0457\n",
      "Epoch 6, Batch 600, Average Loss: 0.0451\n",
      "Epoch 6, Batch 700, Average Loss: 0.0473\n",
      "Epoch 6, Batch 800, Average Loss: 0.0449\n",
      "Epoch 6, Batch 900, Average Loss: 0.0499\n",
      "Epoch 6, Batch 1000, Average Loss: 0.0460\n",
      "Epoch 6, Batch 1100, Average Loss: 0.0455\n",
      "Epoch 6, Batch 1200, Average Loss: 0.0452\n",
      "Epoch 6, Batch 1300, Average Loss: 0.0487\n",
      "Epoch 6, Batch 1400, Average Loss: 0.0465\n",
      "Epoch 6, Batch 1500, Average Loss: 0.0487\n",
      "Epoch 6, Batch 1600, Average Loss: 0.0454\n",
      "Epoch 6, Batch 1700, Average Loss: 0.0455\n",
      "Epoch 6, Batch 1800, Average Loss: 0.0443\n",
      "Epoch 7, Batch 100, Average Loss: 0.0778\n",
      "Epoch 7, Batch 200, Average Loss: 0.0452\n",
      "Epoch 7, Batch 300, Average Loss: 0.0466\n",
      "Epoch 7, Batch 400, Average Loss: 0.0454\n",
      "Epoch 7, Batch 500, Average Loss: 0.0456\n",
      "Epoch 7, Batch 600, Average Loss: 0.0433\n",
      "Epoch 7, Batch 700, Average Loss: 0.0455\n",
      "Epoch 7, Batch 800, Average Loss: 0.0438\n",
      "Epoch 7, Batch 900, Average Loss: 0.0442\n",
      "Epoch 7, Batch 1000, Average Loss: 0.0454\n",
      "Epoch 7, Batch 1100, Average Loss: 0.0461\n",
      "Epoch 7, Batch 1200, Average Loss: 0.0446\n",
      "Epoch 7, Batch 1300, Average Loss: 0.0424\n",
      "Epoch 7, Batch 1400, Average Loss: 0.0454\n",
      "Epoch 7, Batch 1500, Average Loss: 0.0443\n",
      "Epoch 7, Batch 1600, Average Loss: 0.0452\n",
      "Epoch 7, Batch 1700, Average Loss: 0.0456\n",
      "Epoch 7, Batch 1800, Average Loss: 0.0442\n",
      "Epoch 8, Batch 100, Average Loss: 0.0790\n",
      "Epoch 8, Batch 200, Average Loss: 0.0461\n",
      "Epoch 8, Batch 300, Average Loss: 0.0503\n",
      "Epoch 8, Batch 400, Average Loss: 0.0465\n",
      "Epoch 8, Batch 500, Average Loss: 0.0427\n",
      "Epoch 8, Batch 600, Average Loss: 0.0438\n",
      "Epoch 8, Batch 700, Average Loss: 0.0453\n",
      "Epoch 8, Batch 800, Average Loss: 0.0432\n",
      "Epoch 8, Batch 900, Average Loss: 0.0492\n",
      "Epoch 8, Batch 1000, Average Loss: 0.0431\n",
      "Epoch 8, Batch 1100, Average Loss: 0.0435\n",
      "Epoch 8, Batch 1200, Average Loss: 0.0409\n",
      "Epoch 8, Batch 1300, Average Loss: 0.0449\n",
      "Epoch 8, Batch 1400, Average Loss: 0.0470\n",
      "Epoch 8, Batch 1500, Average Loss: 0.0432\n",
      "Epoch 8, Batch 1600, Average Loss: 0.0433\n",
      "Epoch 8, Batch 1700, Average Loss: 0.0426\n",
      "Epoch 8, Batch 1800, Average Loss: 0.0435\n",
      "Epoch 9, Batch 100, Average Loss: 0.0753\n",
      "Epoch 9, Batch 200, Average Loss: 0.0437\n",
      "Epoch 9, Batch 300, Average Loss: 0.0398\n",
      "Epoch 9, Batch 400, Average Loss: 0.0459\n",
      "Epoch 9, Batch 500, Average Loss: 0.0442\n",
      "Epoch 9, Batch 600, Average Loss: 0.0435\n",
      "Epoch 9, Batch 700, Average Loss: 0.0458\n",
      "Epoch 9, Batch 800, Average Loss: 0.0443\n",
      "Epoch 9, Batch 900, Average Loss: 0.0473\n",
      "Epoch 9, Batch 1000, Average Loss: 0.0435\n",
      "Epoch 9, Batch 1100, Average Loss: 0.0459\n",
      "Epoch 9, Batch 1200, Average Loss: 0.0422\n",
      "Epoch 9, Batch 1300, Average Loss: 0.0408\n",
      "Epoch 9, Batch 1400, Average Loss: 0.0436\n",
      "Epoch 9, Batch 1500, Average Loss: 0.0412\n",
      "Epoch 9, Batch 1600, Average Loss: 0.0427\n",
      "Epoch 9, Batch 1700, Average Loss: 0.0462\n",
      "Epoch 9, Batch 1800, Average Loss: 0.0451\n",
      "Epoch 10, Batch 100, Average Loss: 0.0714\n",
      "Epoch 10, Batch 200, Average Loss: 0.0421\n",
      "Epoch 10, Batch 300, Average Loss: 0.0414\n",
      "Epoch 10, Batch 400, Average Loss: 0.0434\n",
      "Epoch 10, Batch 500, Average Loss: 0.0445\n",
      "Epoch 10, Batch 600, Average Loss: 0.0442\n",
      "Epoch 10, Batch 700, Average Loss: 0.0442\n",
      "Epoch 10, Batch 800, Average Loss: 0.0430\n",
      "Epoch 10, Batch 900, Average Loss: 0.0415\n",
      "Epoch 10, Batch 1000, Average Loss: 0.0413\n",
      "Epoch 10, Batch 1100, Average Loss: 0.0434\n",
      "Epoch 10, Batch 1200, Average Loss: 0.0433\n",
      "Epoch 10, Batch 1300, Average Loss: 0.0442\n",
      "Epoch 10, Batch 1400, Average Loss: 0.0426\n",
      "Epoch 10, Batch 1500, Average Loss: 0.0412\n",
      "Epoch 10, Batch 1600, Average Loss: 0.0444\n",
      "Epoch 10, Batch 1700, Average Loss: 0.0415\n",
      "Epoch 10, Batch 1800, Average Loss: 0.0433\n",
      "Epoch 11, Batch 100, Average Loss: 0.0731\n",
      "Epoch 11, Batch 200, Average Loss: 0.0426\n",
      "Epoch 11, Batch 300, Average Loss: 0.0428\n",
      "Epoch 11, Batch 400, Average Loss: 0.0403\n",
      "Epoch 11, Batch 500, Average Loss: 0.0424\n",
      "Epoch 11, Batch 600, Average Loss: 0.0431\n",
      "Epoch 11, Batch 700, Average Loss: 0.0423\n",
      "Epoch 11, Batch 800, Average Loss: 0.0407\n",
      "Epoch 11, Batch 900, Average Loss: 0.0419\n",
      "Epoch 11, Batch 1000, Average Loss: 0.0412\n",
      "Epoch 11, Batch 1100, Average Loss: 0.0409\n",
      "Epoch 11, Batch 1200, Average Loss: 0.0423\n",
      "Epoch 11, Batch 1300, Average Loss: 0.0439\n",
      "Epoch 11, Batch 1400, Average Loss: 0.0439\n",
      "Epoch 11, Batch 1500, Average Loss: 0.0415\n",
      "Epoch 11, Batch 1600, Average Loss: 0.0418\n",
      "Epoch 11, Batch 1700, Average Loss: 0.0424\n",
      "Epoch 11, Batch 1800, Average Loss: 0.0438\n",
      "Epoch 12, Batch 100, Average Loss: 0.0720\n",
      "Epoch 12, Batch 200, Average Loss: 0.0427\n",
      "Epoch 12, Batch 300, Average Loss: 0.0403\n",
      "Epoch 12, Batch 400, Average Loss: 0.0406\n",
      "Epoch 12, Batch 500, Average Loss: 0.0432\n",
      "Epoch 12, Batch 600, Average Loss: 0.0426\n",
      "Epoch 12, Batch 700, Average Loss: 0.0411\n",
      "Epoch 12, Batch 800, Average Loss: 0.0411\n",
      "Epoch 12, Batch 900, Average Loss: 0.0411\n",
      "Epoch 12, Batch 1000, Average Loss: 0.0404\n",
      "Epoch 12, Batch 1100, Average Loss: 0.0400\n",
      "Epoch 12, Batch 1200, Average Loss: 0.0403\n",
      "Epoch 12, Batch 1300, Average Loss: 0.0414\n",
      "Epoch 12, Batch 1400, Average Loss: 0.0411\n",
      "Epoch 12, Batch 1500, Average Loss: 0.0445\n",
      "Epoch 12, Batch 1600, Average Loss: 0.0425\n",
      "Epoch 12, Batch 1700, Average Loss: 0.0442\n",
      "Epoch 12, Batch 1800, Average Loss: 0.0410\n",
      "Epoch 13, Batch 100, Average Loss: 0.0717\n",
      "Epoch 13, Batch 200, Average Loss: 0.0436\n",
      "Epoch 13, Batch 300, Average Loss: 0.0388\n",
      "Epoch 13, Batch 400, Average Loss: 0.0433\n",
      "Epoch 13, Batch 500, Average Loss: 0.0415\n",
      "Epoch 13, Batch 600, Average Loss: 0.0430\n",
      "Epoch 13, Batch 700, Average Loss: 0.0426\n",
      "Epoch 13, Batch 800, Average Loss: 0.0403\n",
      "Epoch 13, Batch 900, Average Loss: 0.0405\n",
      "Epoch 13, Batch 1000, Average Loss: 0.0413\n",
      "Epoch 13, Batch 1100, Average Loss: 0.0418\n",
      "Epoch 13, Batch 1200, Average Loss: 0.0398\n",
      "Epoch 13, Batch 1300, Average Loss: 0.0412\n",
      "Epoch 13, Batch 1400, Average Loss: 0.0395\n",
      "Epoch 13, Batch 1500, Average Loss: 0.0403\n",
      "Epoch 13, Batch 1600, Average Loss: 0.0432\n",
      "Epoch 13, Batch 1700, Average Loss: 0.0414\n",
      "Epoch 13, Batch 1800, Average Loss: 0.0404\n",
      "Epoch 14, Batch 100, Average Loss: 0.0707\n",
      "Epoch 14, Batch 200, Average Loss: 0.0444\n",
      "Epoch 14, Batch 300, Average Loss: 0.0416\n",
      "Epoch 14, Batch 400, Average Loss: 0.0401\n",
      "Epoch 14, Batch 500, Average Loss: 0.0394\n",
      "Epoch 14, Batch 600, Average Loss: 0.0436\n",
      "Epoch 14, Batch 700, Average Loss: 0.0414\n",
      "Epoch 14, Batch 800, Average Loss: 0.0401\n",
      "Epoch 14, Batch 900, Average Loss: 0.0406\n",
      "Epoch 14, Batch 1000, Average Loss: 0.0402\n",
      "Epoch 14, Batch 1100, Average Loss: 0.0431\n",
      "Epoch 14, Batch 1200, Average Loss: 0.0422\n",
      "Epoch 14, Batch 1300, Average Loss: 0.0413\n",
      "Epoch 14, Batch 1400, Average Loss: 0.0391\n",
      "Epoch 14, Batch 1500, Average Loss: 0.0401\n",
      "Epoch 14, Batch 1600, Average Loss: 0.0390\n",
      "Epoch 14, Batch 1700, Average Loss: 0.0400\n",
      "Epoch 14, Batch 1800, Average Loss: 0.0400\n",
      "Epoch 15, Batch 100, Average Loss: 0.0725\n",
      "Epoch 15, Batch 200, Average Loss: 0.0395\n",
      "Epoch 15, Batch 300, Average Loss: 0.0394\n",
      "Epoch 15, Batch 400, Average Loss: 0.0430\n",
      "Epoch 15, Batch 500, Average Loss: 0.0411\n",
      "Epoch 15, Batch 600, Average Loss: 0.0400\n",
      "Epoch 15, Batch 700, Average Loss: 0.0404\n",
      "Epoch 15, Batch 800, Average Loss: 0.0413\n",
      "Epoch 15, Batch 900, Average Loss: 0.0417\n",
      "Epoch 15, Batch 1000, Average Loss: 0.0398\n",
      "Epoch 15, Batch 1100, Average Loss: 0.0399\n",
      "Epoch 15, Batch 1200, Average Loss: 0.0402\n",
      "Epoch 15, Batch 1300, Average Loss: 0.0403\n",
      "Epoch 15, Batch 1400, Average Loss: 0.0405\n",
      "Epoch 15, Batch 1500, Average Loss: 0.0392\n",
      "Epoch 15, Batch 1600, Average Loss: 0.0404\n",
      "Epoch 15, Batch 1700, Average Loss: 0.0390\n",
      "Epoch 15, Batch 1800, Average Loss: 0.0396\n",
      "Epoch 16, Batch 100, Average Loss: 0.0704\n",
      "Epoch 16, Batch 200, Average Loss: 0.0403\n",
      "Epoch 16, Batch 300, Average Loss: 0.0411\n",
      "Epoch 16, Batch 400, Average Loss: 0.0428\n",
      "Epoch 16, Batch 500, Average Loss: 0.0400\n",
      "Epoch 16, Batch 600, Average Loss: 0.0426\n",
      "Epoch 16, Batch 700, Average Loss: 0.0406\n",
      "Epoch 16, Batch 800, Average Loss: 0.0417\n",
      "Epoch 16, Batch 900, Average Loss: 0.0407\n",
      "Epoch 16, Batch 1000, Average Loss: 0.0411\n",
      "Epoch 16, Batch 1100, Average Loss: 0.0399\n",
      "Epoch 16, Batch 1200, Average Loss: 0.0402\n",
      "Epoch 16, Batch 1300, Average Loss: 0.0422\n",
      "Epoch 16, Batch 1400, Average Loss: 0.0428\n",
      "Epoch 16, Batch 1500, Average Loss: 0.0375\n",
      "Epoch 16, Batch 1600, Average Loss: 0.0403\n",
      "Epoch 16, Batch 1700, Average Loss: 0.0392\n",
      "Epoch 16, Batch 1800, Average Loss: 0.0403\n",
      "Epoch 17, Batch 100, Average Loss: 0.0688\n",
      "Epoch 17, Batch 200, Average Loss: 0.0405\n",
      "Epoch 17, Batch 300, Average Loss: 0.0407\n",
      "Epoch 17, Batch 400, Average Loss: 0.0388\n",
      "Epoch 17, Batch 500, Average Loss: 0.0396\n",
      "Epoch 17, Batch 600, Average Loss: 0.0406\n",
      "Epoch 17, Batch 700, Average Loss: 0.0399\n",
      "Epoch 17, Batch 800, Average Loss: 0.0401\n",
      "Epoch 17, Batch 900, Average Loss: 0.0437\n",
      "Epoch 17, Batch 1000, Average Loss: 0.0377\n",
      "Epoch 17, Batch 1100, Average Loss: 0.0397\n",
      "Epoch 17, Batch 1200, Average Loss: 0.0398\n",
      "Epoch 17, Batch 1300, Average Loss: 0.0402\n",
      "Epoch 17, Batch 1400, Average Loss: 0.0399\n",
      "Epoch 17, Batch 1500, Average Loss: 0.0416\n",
      "Epoch 17, Batch 1600, Average Loss: 0.0395\n",
      "Epoch 17, Batch 1700, Average Loss: 0.0412\n",
      "Epoch 17, Batch 1800, Average Loss: 0.0401\n",
      "Epoch 18, Batch 100, Average Loss: 0.0697\n",
      "Epoch 18, Batch 200, Average Loss: 0.0381\n",
      "Epoch 18, Batch 300, Average Loss: 0.0412\n",
      "Epoch 18, Batch 400, Average Loss: 0.0388\n",
      "Epoch 18, Batch 500, Average Loss: 0.0406\n",
      "Epoch 18, Batch 600, Average Loss: 0.0393\n",
      "Epoch 18, Batch 700, Average Loss: 0.0378\n",
      "Epoch 18, Batch 800, Average Loss: 0.0392\n",
      "Epoch 18, Batch 900, Average Loss: 0.0398\n",
      "Epoch 18, Batch 1000, Average Loss: 0.0389\n",
      "Epoch 18, Batch 1100, Average Loss: 0.0398\n",
      "Epoch 18, Batch 1200, Average Loss: 0.0395\n",
      "Epoch 18, Batch 1300, Average Loss: 0.0422\n",
      "Epoch 18, Batch 1400, Average Loss: 0.0389\n",
      "Epoch 18, Batch 1500, Average Loss: 0.0406\n",
      "Epoch 18, Batch 1600, Average Loss: 0.0384\n",
      "Epoch 18, Batch 1700, Average Loss: 0.0417\n",
      "Epoch 18, Batch 1800, Average Loss: 0.0399\n",
      "Epoch 19, Batch 100, Average Loss: 0.0681\n",
      "Epoch 19, Batch 200, Average Loss: 0.0364\n",
      "Epoch 19, Batch 300, Average Loss: 0.0387\n",
      "Epoch 19, Batch 400, Average Loss: 0.0396\n",
      "Epoch 19, Batch 500, Average Loss: 0.0407\n",
      "Epoch 19, Batch 600, Average Loss: 0.0397\n",
      "Epoch 19, Batch 700, Average Loss: 0.0400\n",
      "Epoch 19, Batch 800, Average Loss: 0.0432\n",
      "Epoch 19, Batch 900, Average Loss: 0.0393\n",
      "Epoch 19, Batch 1000, Average Loss: 0.0391\n",
      "Epoch 19, Batch 1100, Average Loss: 0.0384\n",
      "Epoch 19, Batch 1200, Average Loss: 0.0416\n",
      "Epoch 19, Batch 1300, Average Loss: 0.0406\n",
      "Epoch 19, Batch 1400, Average Loss: 0.0389\n",
      "Epoch 19, Batch 1500, Average Loss: 0.0394\n",
      "Epoch 19, Batch 1600, Average Loss: 0.0388\n",
      "Epoch 19, Batch 1700, Average Loss: 0.0397\n",
      "Epoch 19, Batch 1800, Average Loss: 0.0406\n",
      "Epoch 20, Batch 100, Average Loss: 0.0697\n",
      "Epoch 20, Batch 200, Average Loss: 0.0436\n",
      "Epoch 20, Batch 300, Average Loss: 0.0417\n",
      "Epoch 20, Batch 400, Average Loss: 0.0375\n",
      "Epoch 20, Batch 500, Average Loss: 0.0383\n",
      "Epoch 20, Batch 600, Average Loss: 0.0394\n",
      "Epoch 20, Batch 700, Average Loss: 0.0393\n",
      "Epoch 20, Batch 800, Average Loss: 0.0391\n",
      "Epoch 20, Batch 900, Average Loss: 0.0385\n",
      "Epoch 20, Batch 1000, Average Loss: 0.0392\n",
      "Epoch 20, Batch 1100, Average Loss: 0.0380\n",
      "Epoch 20, Batch 1200, Average Loss: 0.0382\n",
      "Epoch 20, Batch 1300, Average Loss: 0.0392\n",
      "Epoch 20, Batch 1400, Average Loss: 0.0444\n",
      "Epoch 20, Batch 1500, Average Loss: 0.0382\n",
      "Epoch 20, Batch 1600, Average Loss: 0.0380\n",
      "Epoch 20, Batch 1700, Average Loss: 0.0402\n",
      "Epoch 20, Batch 1800, Average Loss: 0.0394\n",
      "Epoch 21, Batch 100, Average Loss: 0.0637\n",
      "Epoch 21, Batch 200, Average Loss: 0.0396\n",
      "Epoch 21, Batch 300, Average Loss: 0.0393\n",
      "Epoch 21, Batch 400, Average Loss: 0.0404\n",
      "Epoch 21, Batch 500, Average Loss: 0.0380\n",
      "Epoch 21, Batch 600, Average Loss: 0.0392\n",
      "Epoch 21, Batch 700, Average Loss: 0.0404\n",
      "Epoch 21, Batch 800, Average Loss: 0.0395\n",
      "Epoch 21, Batch 900, Average Loss: 0.0387\n",
      "Epoch 21, Batch 1000, Average Loss: 0.0401\n",
      "Epoch 21, Batch 1100, Average Loss: 0.0375\n",
      "Epoch 21, Batch 1200, Average Loss: 0.0391\n",
      "Epoch 21, Batch 1300, Average Loss: 0.0393\n",
      "Epoch 21, Batch 1400, Average Loss: 0.0366\n",
      "Epoch 21, Batch 1500, Average Loss: 0.0391\n",
      "Epoch 21, Batch 1600, Average Loss: 0.0369\n",
      "Epoch 21, Batch 1700, Average Loss: 0.0388\n",
      "Epoch 21, Batch 1800, Average Loss: 0.0389\n",
      "Epoch 22, Batch 100, Average Loss: 0.0675\n",
      "Epoch 22, Batch 200, Average Loss: 0.0404\n",
      "Epoch 22, Batch 300, Average Loss: 0.0365\n",
      "Epoch 22, Batch 400, Average Loss: 0.0392\n",
      "Epoch 22, Batch 500, Average Loss: 0.0396\n",
      "Epoch 22, Batch 600, Average Loss: 0.0416\n",
      "Epoch 22, Batch 700, Average Loss: 0.0373\n",
      "Epoch 22, Batch 800, Average Loss: 0.0373\n",
      "Epoch 22, Batch 900, Average Loss: 0.0380\n",
      "Epoch 22, Batch 1000, Average Loss: 0.0382\n",
      "Epoch 22, Batch 1100, Average Loss: 0.0385\n",
      "Epoch 22, Batch 1200, Average Loss: 0.0388\n",
      "Epoch 22, Batch 1300, Average Loss: 0.0373\n",
      "Epoch 22, Batch 1400, Average Loss: 0.0388\n",
      "Epoch 22, Batch 1500, Average Loss: 0.0392\n",
      "Epoch 22, Batch 1600, Average Loss: 0.0409\n",
      "Epoch 22, Batch 1700, Average Loss: 0.0382\n",
      "Epoch 22, Batch 1800, Average Loss: 0.0389\n",
      "Epoch 23, Batch 100, Average Loss: 0.0686\n",
      "Epoch 23, Batch 200, Average Loss: 0.0403\n",
      "Epoch 23, Batch 300, Average Loss: 0.0395\n",
      "Epoch 23, Batch 400, Average Loss: 0.0385\n",
      "Epoch 23, Batch 500, Average Loss: 0.0370\n",
      "Epoch 23, Batch 600, Average Loss: 0.0401\n",
      "Epoch 23, Batch 700, Average Loss: 0.0404\n",
      "Epoch 23, Batch 800, Average Loss: 0.0372\n",
      "Epoch 23, Batch 900, Average Loss: 0.0401\n",
      "Epoch 23, Batch 1000, Average Loss: 0.0386\n",
      "Epoch 23, Batch 1100, Average Loss: 0.0401\n",
      "Epoch 23, Batch 1200, Average Loss: 0.0400\n",
      "Epoch 23, Batch 1300, Average Loss: 0.0377\n",
      "Epoch 23, Batch 1400, Average Loss: 0.0391\n",
      "Epoch 23, Batch 1500, Average Loss: 0.0370\n",
      "Epoch 23, Batch 1600, Average Loss: 0.0387\n",
      "Epoch 23, Batch 1700, Average Loss: 0.0375\n",
      "Epoch 23, Batch 1800, Average Loss: 0.0362\n",
      "Epoch 24, Batch 100, Average Loss: 0.0733\n",
      "Epoch 24, Batch 200, Average Loss: 0.0400\n",
      "Epoch 24, Batch 300, Average Loss: 0.0385\n",
      "Epoch 24, Batch 400, Average Loss: 0.0390\n",
      "Epoch 24, Batch 500, Average Loss: 0.0408\n",
      "Epoch 24, Batch 600, Average Loss: 0.0368\n",
      "Epoch 24, Batch 700, Average Loss: 0.0396\n",
      "Epoch 24, Batch 800, Average Loss: 0.0386\n",
      "Epoch 24, Batch 900, Average Loss: 0.0398\n",
      "Epoch 24, Batch 1000, Average Loss: 0.0374\n",
      "Epoch 24, Batch 1100, Average Loss: 0.0394\n",
      "Epoch 24, Batch 1200, Average Loss: 0.0372\n",
      "Epoch 24, Batch 1300, Average Loss: 0.0363\n",
      "Epoch 24, Batch 1400, Average Loss: 0.0400\n",
      "Epoch 24, Batch 1500, Average Loss: 0.0377\n",
      "Epoch 24, Batch 1600, Average Loss: 0.0384\n",
      "Epoch 24, Batch 1700, Average Loss: 0.0391\n",
      "Epoch 24, Batch 1800, Average Loss: 0.0421\n",
      "Epoch 25, Batch 100, Average Loss: 0.0664\n",
      "Epoch 25, Batch 200, Average Loss: 0.0379\n",
      "Epoch 25, Batch 300, Average Loss: 0.0385\n",
      "Epoch 25, Batch 400, Average Loss: 0.0372\n",
      "Epoch 25, Batch 500, Average Loss: 0.0379\n",
      "Epoch 25, Batch 600, Average Loss: 0.0376\n",
      "Epoch 25, Batch 700, Average Loss: 0.0386\n",
      "Epoch 25, Batch 800, Average Loss: 0.0360\n",
      "Epoch 25, Batch 900, Average Loss: 0.0384\n",
      "Epoch 25, Batch 1000, Average Loss: 0.0384\n",
      "Epoch 25, Batch 1100, Average Loss: 0.0382\n",
      "Epoch 25, Batch 1200, Average Loss: 0.0373\n",
      "Epoch 25, Batch 1300, Average Loss: 0.0379\n",
      "Epoch 25, Batch 1400, Average Loss: 0.0368\n",
      "Epoch 25, Batch 1500, Average Loss: 0.0387\n",
      "Epoch 25, Batch 1600, Average Loss: 0.0374\n",
      "Epoch 25, Batch 1700, Average Loss: 0.0376\n",
      "Epoch 25, Batch 1800, Average Loss: 0.0398\n",
      "Epoch 26, Batch 100, Average Loss: 0.0669\n",
      "Epoch 26, Batch 200, Average Loss: 0.0381\n",
      "Epoch 26, Batch 300, Average Loss: 0.0390\n",
      "Epoch 26, Batch 400, Average Loss: 0.0383\n",
      "Epoch 26, Batch 500, Average Loss: 0.0423\n",
      "Epoch 26, Batch 600, Average Loss: 0.0395\n",
      "Epoch 26, Batch 700, Average Loss: 0.0416\n",
      "Epoch 26, Batch 800, Average Loss: 0.0376\n",
      "Epoch 26, Batch 900, Average Loss: 0.0374\n",
      "Epoch 26, Batch 1000, Average Loss: 0.0371\n",
      "Epoch 26, Batch 1100, Average Loss: 0.0373\n",
      "Epoch 26, Batch 1200, Average Loss: 0.0369\n",
      "Epoch 26, Batch 1300, Average Loss: 0.0380\n",
      "Epoch 26, Batch 1400, Average Loss: 0.0378\n",
      "Epoch 26, Batch 1500, Average Loss: 0.0376\n",
      "Epoch 26, Batch 1600, Average Loss: 0.0404\n",
      "Epoch 26, Batch 1700, Average Loss: 0.0375\n",
      "Epoch 26, Batch 1800, Average Loss: 0.0402\n",
      "Epoch 27, Batch 100, Average Loss: 0.0668\n",
      "Epoch 27, Batch 200, Average Loss: 0.0385\n",
      "Epoch 27, Batch 300, Average Loss: 0.0381\n",
      "Epoch 27, Batch 400, Average Loss: 0.0392\n",
      "Epoch 27, Batch 500, Average Loss: 0.0395\n",
      "Epoch 27, Batch 600, Average Loss: 0.0391\n",
      "Epoch 27, Batch 700, Average Loss: 0.0391\n",
      "Epoch 27, Batch 800, Average Loss: 0.0381\n",
      "Epoch 27, Batch 900, Average Loss: 0.0364\n",
      "Epoch 27, Batch 1000, Average Loss: 0.0393\n",
      "Epoch 27, Batch 1100, Average Loss: 0.0386\n",
      "Epoch 27, Batch 1200, Average Loss: 0.0392\n",
      "Epoch 27, Batch 1300, Average Loss: 0.0378\n",
      "Epoch 27, Batch 1400, Average Loss: 0.0389\n",
      "Epoch 27, Batch 1500, Average Loss: 0.0397\n",
      "Epoch 27, Batch 1600, Average Loss: 0.0372\n",
      "Epoch 27, Batch 1700, Average Loss: 0.0385\n",
      "Epoch 27, Batch 1800, Average Loss: 0.0393\n",
      "Epoch 28, Batch 100, Average Loss: 0.0675\n",
      "Epoch 28, Batch 200, Average Loss: 0.0371\n",
      "Epoch 28, Batch 300, Average Loss: 0.0380\n",
      "Epoch 28, Batch 400, Average Loss: 0.0398\n",
      "Epoch 28, Batch 500, Average Loss: 0.0370\n",
      "Epoch 28, Batch 600, Average Loss: 0.0380\n",
      "Epoch 28, Batch 700, Average Loss: 0.0363\n",
      "Epoch 28, Batch 800, Average Loss: 0.0421\n",
      "Epoch 28, Batch 900, Average Loss: 0.0356\n",
      "Epoch 28, Batch 1000, Average Loss: 0.0404\n",
      "Epoch 28, Batch 1100, Average Loss: 0.0372\n",
      "Epoch 28, Batch 1200, Average Loss: 0.0370\n",
      "Epoch 28, Batch 1300, Average Loss: 0.0384\n",
      "Epoch 28, Batch 1400, Average Loss: 0.0386\n",
      "Epoch 28, Batch 1500, Average Loss: 0.0376\n",
      "Epoch 28, Batch 1600, Average Loss: 0.0376\n",
      "Epoch 28, Batch 1700, Average Loss: 0.0393\n",
      "Epoch 28, Batch 1800, Average Loss: 0.0371\n",
      "Epoch 29, Batch 100, Average Loss: 0.0664\n",
      "Epoch 29, Batch 200, Average Loss: 0.0363\n",
      "Epoch 29, Batch 300, Average Loss: 0.0370\n",
      "Epoch 29, Batch 400, Average Loss: 0.0369\n",
      "Epoch 29, Batch 500, Average Loss: 0.0396\n",
      "Epoch 29, Batch 600, Average Loss: 0.0396\n",
      "Epoch 29, Batch 700, Average Loss: 0.0387\n",
      "Epoch 29, Batch 800, Average Loss: 0.0376\n",
      "Epoch 29, Batch 900, Average Loss: 0.0377\n",
      "Epoch 29, Batch 1000, Average Loss: 0.0387\n",
      "Epoch 29, Batch 1100, Average Loss: 0.0378\n",
      "Epoch 29, Batch 1200, Average Loss: 0.0376\n",
      "Epoch 29, Batch 1300, Average Loss: 0.0375\n",
      "Epoch 29, Batch 1400, Average Loss: 0.0372\n",
      "Epoch 29, Batch 1500, Average Loss: 0.0373\n",
      "Epoch 29, Batch 1600, Average Loss: 0.0366\n",
      "Epoch 29, Batch 1700, Average Loss: 0.0391\n",
      "Epoch 29, Batch 1800, Average Loss: 0.0370\n",
      "Epoch 30, Batch 100, Average Loss: 0.0658\n",
      "Epoch 30, Batch 200, Average Loss: 0.0381\n",
      "Epoch 30, Batch 300, Average Loss: 0.0377\n",
      "Epoch 30, Batch 400, Average Loss: 0.0370\n",
      "Epoch 30, Batch 500, Average Loss: 0.0378\n",
      "Epoch 30, Batch 600, Average Loss: 0.0386\n",
      "Epoch 30, Batch 700, Average Loss: 0.0377\n",
      "Epoch 30, Batch 800, Average Loss: 0.0382\n",
      "Epoch 30, Batch 900, Average Loss: 0.0362\n",
      "Epoch 30, Batch 1000, Average Loss: 0.0378\n",
      "Epoch 30, Batch 1100, Average Loss: 0.0388\n",
      "Epoch 30, Batch 1200, Average Loss: 0.0378\n",
      "Epoch 30, Batch 1300, Average Loss: 0.0397\n",
      "Epoch 30, Batch 1400, Average Loss: 0.0387\n",
      "Epoch 30, Batch 1500, Average Loss: 0.0386\n",
      "Epoch 30, Batch 1600, Average Loss: 0.0361\n",
      "Epoch 30, Batch 1700, Average Loss: 0.0381\n",
      "Epoch 30, Batch 1800, Average Loss: 0.0378\n",
      "Epoch 31, Batch 100, Average Loss: 0.0648\n",
      "Epoch 31, Batch 200, Average Loss: 0.0365\n",
      "Epoch 31, Batch 300, Average Loss: 0.0373\n",
      "Epoch 31, Batch 400, Average Loss: 0.0394\n",
      "Epoch 31, Batch 500, Average Loss: 0.0401\n",
      "Epoch 31, Batch 600, Average Loss: 0.0379\n",
      "Epoch 31, Batch 700, Average Loss: 0.0358\n",
      "Epoch 31, Batch 800, Average Loss: 0.0391\n",
      "Epoch 31, Batch 900, Average Loss: 0.0361\n",
      "Epoch 31, Batch 1000, Average Loss: 0.0356\n",
      "Epoch 31, Batch 1100, Average Loss: 0.0383\n",
      "Epoch 31, Batch 1200, Average Loss: 0.0390\n",
      "Epoch 31, Batch 1300, Average Loss: 0.0360\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m eps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(x0)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m x_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(alpha_bar[t\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m*\u001b[39m x0 \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha_bar[t\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m*\u001b[39m eps\n\u001b[1;32m---> 10\u001b[0m predicted_eps \u001b[38;5;241m=\u001b[39m model(x_t, t_emb)  \u001b[38;5;66;03m# Pass the precomputed embedding to the model\u001b[39;00m\n\u001b[0;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predicted_eps, eps)\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 61\u001b[0m, in \u001b[0;36mUNET.forward\u001b[1;34m(self, x, t_emb)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs):\n\u001b[0;32m     60\u001b[0m     lin_time \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels[i],device\u001b[38;5;241m=\u001b[39mdevice),nn\u001b[38;5;241m.\u001b[39mReLU()) \u001b[38;5;66;03m# 128 is the size of the time embedding.\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m     t_emb_processed \u001b[38;5;241m=\u001b[39m lin_time(t_emb)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels[i], \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     62\u001b[0m     signal\u001b[38;5;241m=\u001b[39m t_emb_processed\u001b[38;5;241m+\u001b[39msignal\n\u001b[0;32m     63\u001b[0m     signal \u001b[38;5;241m=\u001b[39m conv(signal)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for e, data in enumerate(train_loader):\n",
    "        x0, _ = data\n",
    "        x0 = x0.to(device)\n",
    "        t = torch.randint(1, T+1, (batch_size,), device=device)\n",
    "        t_emb = time_embedding_layer(t)  # Shape: [batch_size, embedding_dim]\n",
    "        eps = torch.randn_like(x0).to(device)\n",
    "        x_t = torch.sqrt(alpha_bar[t-1]) * x0 + torch.sqrt(1 - alpha_bar[t-1]) * eps\n",
    "        predicted_eps = model(x_t, t_emb)  # Pass the precomputed embedding to the model\n",
    "        loss = criterion(predicted_eps, eps)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if e % 100 == 99:\n",
    "            print(f\"Epoch {epoch}, Batch {e+1}, Average Loss: {running_loss / 100:.4f}\")\n",
    "            running_loss = 0.0  \n",
    "\n",
    "    if epoch % 10 == 9:\n",
    "        torch.save(model.state_dict(), f\"DDPM_{epoch+1}.pth\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54a74a6-6786-43c8-9b7b-569eb01df2f2",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f24222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = None\n",
    "\n",
    "if load_model:\n",
    "    model.load_state_dict(torch.load(load_model, map_location=device))\n",
    "with torch.inference_mode():\n",
    "    model.eval()\n",
    "    batch_size = 1\n",
    "    xt = torch.randn(batch_size, 1, 28, 28).to(device)\n",
    "\n",
    "    for t in torch.arange(T, 0, -1):\n",
    "        t = t.reshape(1)\n",
    "        t = t.to(device)\n",
    "        t_emb = time_embedding_layer(t.float())\n",
    "        z = torch.randn(batch_size, 1, 28, 28).to(device) if t > 1 else torch.zeros(batch_size, 1, 28, 28).to(device)\n",
    "        \n",
    "        xt_new = 1 / torch.sqrt(alpha[t - 1]) * (xt - (1 - alpha[t - 1])/(torch.sqrt(1 - alpha_bar[t - 1])) * \n",
    "                                                    model(xt, t_emb)) + torch.sqrt(beta[t-1]) * z\n",
    "        xt = xt_new\n",
    "\n",
    "\n",
    "plt.imshow(xt[0][0].cpu().detach().numpy(), cmap=\"grey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b7d7c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 4000x2000 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sample from all the timesteps\n",
    "fig, axs = plt.subplots(1, 3, tight_layout=True, figsize=(40, 20))\n",
    "tens = np.arange(10,31,10)\n",
    "seed = 3546\n",
    "for ten in tens:\n",
    "    with torch.inference_mode():\n",
    "        # torch.manual_seed(seed)\n",
    "        model.load_state_dict(torch.load(f\"DDPM_{ten}.pth\", map_location=device))\n",
    "        model.eval()\n",
    "        batch_size = 1\n",
    "        xt = torch.randn(batch_size, 1, 28, 28).to(device)\n",
    "\n",
    "        for t in torch.arange(T, 0, -1):\n",
    "            t = t.reshape(1)\n",
    "            t = t.to(device)\n",
    "            t_emb = time_embedding_layer(t.float())\n",
    "            z = torch.randn(batch_size, 1, 28, 28).to(device) if t > 1 else torch.zeros(batch_size, 1, 28, 28).to(device)\n",
    "            \n",
    "            xt_new = 1 / torch.sqrt(alpha[t - 1]) * (xt - (1 - alpha[t - 1])/(torch.sqrt(1 - alpha_bar[t - 1])) * \n",
    "                                                        model(xt, t_emb)) + torch.sqrt(beta[t-1]) * z\n",
    "            xt = xt_new\n",
    "        axs[(ten//10)-1].imshow(xt[0][0].cpu().detach().numpy(), cmap=\"gray\")\n",
    "        axs[(ten//10)-1].axis(\"off\")\n",
    "        axs[(ten//10)-1].set_title(f\"Epoch={ten}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f94672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9abca9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
